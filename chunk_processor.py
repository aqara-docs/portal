import streamlit as st

def split_rag_data_into_chunks(mysql_data=None, website_data=None, files_data=None, model_name=None, max_chunk_size=800000):
    """RAG ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ë°˜í™˜"""
    chunks = []
    
    # ëª¨ë¸ë³„ ì²­í¬ í¬ê¸° ì„¤ì •
    if model_name and model_name.startswith('claude'):
        max_chunk_size = 6000000  # 6MB
    elif model_name and model_name.startswith('o1'):
        max_chunk_size = 600000   # 600KB
    elif model_name and (model_name.startswith('gpt-4') or model_name.startswith('gpt-3')):
        max_chunk_size = 2500000  # 2.5MB
    else:
        max_chunk_size = 800000   # 800KB
    
    # ì†ŒìŠ¤ë³„ë¡œ ë°ì´í„° ìˆ˜ì§‘
    data_sources = []
    
    # MySQL ë°ì´í„° ì¤€ë¹„
    if mysql_data:
        for table_name, df in mysql_data.items():
            table_info = {
                'type': 'mysql',
                'name': table_name,
                'data': df,
                'size_estimate': len(str(df.head(3))) + len(', '.join(df.columns)) + 200
            }
            data_sources.append(table_info)
    
    # ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ì¤€ë¹„
    if website_data:
        for i, page_data in enumerate(website_data):
            page_info = {
                'type': 'website',
                'name': f"í˜ì´ì§€_{i+1}",
                'data': page_data,
                'size_estimate': len(page_data['content'][:500]) + len(page_data['title']) + 100
            }
            data_sources.append(page_info)
    
    # íŒŒì¼ ë°ì´í„° ì¤€ë¹„
    if files_data:
        for file_data in files_data:
            file_info = {
                'type': 'files',
                'name': file_data['name'],
                'data': file_data,
                'size_estimate': len(file_data['content'][:500]) + 100
            }
            data_sources.append(file_info)
    
    # í¬ê¸° ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì‘ì€ ê²ƒë¶€í„°)
    data_sources.sort(key=lambda x: x['size_estimate'])
    
    # ì²­í¬ë¡œ ë¶„í• 
    current_chunk = []
    current_size = 0
    
    for source in data_sources:
        if current_size + source['size_estimate'] > max_chunk_size and current_chunk:
            # í˜„ì¬ ì²­í¬ê°€ ê½‰ ì°¼ìœ¼ë¯€ë¡œ ìƒˆ ì²­í¬ ì‹œì‘
            chunks.append(current_chunk)
            current_chunk = [source]
            current_size = source['size_estimate']
        else:
            current_chunk.append(source)
            current_size += source['size_estimate']
    
    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def create_chunk_context(chunk_data, model_name=None):
    """íŠ¹ì • ì²­í¬ì˜ RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    context_parts = []
    
    # ëª¨ë¸ë³„ ì„¤ì •
    if model_name and model_name.startswith('claude'):
        MYSQL_SAMPLE_ROWS = 1
        CONTENT_PREVIEW_SIZE = 200
    elif model_name and model_name.startswith('o1'):
        MYSQL_SAMPLE_ROWS = 2
        CONTENT_PREVIEW_SIZE = 300
    else:
        MYSQL_SAMPLE_ROWS = 3
        CONTENT_PREVIEW_SIZE = 500
    
    # ì†ŒìŠ¤ë³„ë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    mysql_tables = []
    website_pages = []
    file_names = []
    
    for source in chunk_data:
        if source['type'] == 'mysql':
            df = source['data']
            table_context = f"\n[{source['name']}] í…Œì´ë¸”:\n"
            table_context += f"- í–‰ ìˆ˜: {len(df):,}ê°œ\n"
            table_context += f"- ì»¬ëŸ¼: {', '.join(df.columns.tolist())}\n"
            
            if len(df) > 0:
                table_context += "- ìƒ˜í”Œ ë°ì´í„°:\n"
                sample_data = df.head(MYSQL_SAMPLE_ROWS).to_string(index=False, max_cols=5)
                if len(sample_data) > 500:
                    sample_data = sample_data[:500] + "..."
                table_context += sample_data + "\n"
            
            context_parts.append(table_context)
            mysql_tables.append(source['name'])
            
        elif source['type'] == 'website':
            page_data = source['data']
            page_context = f"\n[{source['name']}] {page_data['title']}\n"
            page_context += f"URL: {page_data['url']}\n"
            content_preview = page_data['content'][:CONTENT_PREVIEW_SIZE] + "..."
            page_context += f"ë‚´ìš©: {content_preview}\n"
            
            context_parts.append(page_context)
            website_pages.append(source['name'])
            
        elif source['type'] == 'files':
            file_data = source['data']
            file_context = f"\n[ë¬¸ì„œ] {file_data['name']}\n"
            file_context += f"í¬ê¸°: {file_data['size']:,}ì\n"
            content_preview = file_data['content'][:CONTENT_PREVIEW_SIZE] + "..."
            file_context += f"ë‚´ìš©: {content_preview}\n"
            
            context_parts.append(file_context)
            file_names.append(file_data['name'])
    
    # í—¤ë” ì¶”ê°€
    if mysql_tables:
        context_parts.insert(0, f"=== MySQL í…Œì´ë¸” ({len(mysql_tables)}ê°œ) ===")
    if website_pages:
        context_parts.insert(-len(website_pages) if website_pages else 0, f"=== ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ ({len(website_pages)}ê°œ) ===")
    if file_names:
        context_parts.append(f"=== ë¬¸ì„œ íŒŒì¼ ({len(file_names)}ê°œ) ===")
    
    context_text = "\n".join(context_parts)
    
    # ì†ŒìŠ¤ ì •ë³´ ìƒì„±
    rag_sources = []
    if mysql_tables:
        rag_sources.append({
            'type': 'mysql',
            'name': f'MySQL í…Œì´ë¸” ({len(mysql_tables)}ê°œ)',
            'details': ', '.join(mysql_tables[:3]) + ('...' if len(mysql_tables) > 3 else ''),
            'tables': mysql_tables
        })
    if website_pages:
        rag_sources.append({
            'type': 'website',
            'name': f'ì›¹ì‚¬ì´íŠ¸ ({len(website_pages)}ê°œ)',
            'details': ', '.join(website_pages[:3]) + ('...' if len(website_pages) > 3 else ''),
            'pages': website_pages
        })
    if file_names:
        rag_sources.append({
            'type': 'files',
            'name': f'ë¬¸ì„œ ({len(file_names)}ê°œ)',
            'details': ', '.join(file_names[:3]) + ('...' if len(file_names) > 3 else ''),
            'files': file_names
        })
    
    return context_text, rag_sources

def process_chunked_rag(user_query, mysql_data=None, website_data=None, files_data=None, model_name=None, get_ai_response_func=None):
    """ì²­í¬ ê¸°ë°˜ RAG ì²˜ë¦¬"""
    
    # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    chunks = split_rag_data_into_chunks(mysql_data, website_data, files_data, model_name)
    
    if len(chunks) <= 1:
        # ì²­í¬ê°€ 1ê°œ ì´í•˜ë©´ ì¼ë°˜ ì²˜ë¦¬
        return None
    
    st.info(f"ğŸ”„ **ì²­í¬ ë¶„í•  ëª¨ë“œ**: ë°ì´í„°ê°€ í¬ê¸° ì œí•œì„ ì´ˆê³¼í•˜ì—¬ {len(chunks)}ê°œ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    
    chunk_responses = []
    all_rag_sources = []
    
    # ê° ì²­í¬ë³„ ì²˜ë¦¬
    for i, chunk in enumerate(chunks):
        st.write(f"**ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...**")
        
        # ì²­í¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        chunk_context, chunk_sources = create_chunk_context(chunk, model_name)
        all_rag_sources.extend(chunk_sources)
        
        # ì²­í¬ë³„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        chunk_prompt = f"""
ë‹¤ìŒì€ ì „ì²´ ë°ì´í„° ì¤‘ ì¼ë¶€ì…ë‹ˆë‹¤ (ì²­í¬ {i+1}/{len(chunks)}):

{chunk_context}

ì§ˆë¬¸: {user_query}

ì´ ì²­í¬ì˜ ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì²­í¬ì˜ ê²°ê³¼ì™€ ë‚˜ì¤‘ì— ì¢…í•©ë  ì˜ˆì •ì…ë‹ˆë‹¤.
"""
        
        # AI ì‘ë‹µ ìƒì„±
        response = get_ai_response_func(
            prompt=chunk_prompt,
            model_name=model_name,
            system_prompt="ë‹¹ì‹ ì€ RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì²­í¬ ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë¶€ë¶„ì  ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.",
            enable_thinking=False  # ì²­í¬ ì²˜ë¦¬ ì‹œ thinking ë¹„í™œì„±í™”
        )
        
        chunk_responses.append({
            'chunk_id': i+1,
            'content': response['content'],
            'sources': chunk_sources
        })
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress = (i + 1) / len(chunks)
        st.progress(progress)
    
    # ìµœì¢… ì¢…í•© ë‹µë³€ ìƒì„±
    st.write("**ğŸ”„ ì²­í¬ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ì¤‘...**")
    
    synthesis_prompt = f"""
ë‹¤ìŒì€ ë™ì¼í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ì²­í¬ë¡œë¶€í„° ì–»ì€ ë¶€ë¶„ì  ë‹µë³€ë“¤ì…ë‹ˆë‹¤:

ì§ˆë¬¸: {user_query}

ì²­í¬ë³„ ë‹µë³€ë“¤:
"""
    
    for i, chunk_resp in enumerate(chunk_responses):
        synthesis_prompt += f"\n--- ì²­í¬ {chunk_resp['chunk_id']} ë‹µë³€ ---\n{chunk_resp['content']}\n"
    
    synthesis_prompt += """
ìœ„ì˜ ëª¨ë“  ì²­í¬ ë‹µë³€ë“¤ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•˜ê³  ì¼ê´€ëœ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ì¤‘ë³µëœ ì •ë³´ëŠ” í†µí•©í•˜ê¸°
2. ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•˜ê¸°  
3. ì „ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ì™€ ê²°ë¡  ì œì‹œí•˜ê¸°
4. ì²­í¬ë³„ ì„¸ë¶€ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì •ë¦¬í•˜ê¸°
"""
    
    final_response = get_ai_response_func(
        prompt=synthesis_prompt,
        model_name=model_name,
        system_prompt="ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¶€ë¶„ì  ë‹µë³€ì„ ì¢…í•©í•˜ì—¬ ì™„ì „í•œ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        enable_thinking=st.session_state.get('enable_sidebar_reasoning', False)
    )
    
    return {
        'content': final_response['content'],
        'thinking': final_response.get('thinking', ''),
        'has_thinking': final_response.get('has_thinking', False),
        'chunk_count': len(chunks),
        'chunk_responses': chunk_responses,
        'rag_sources_used': all_rag_sources
    } 