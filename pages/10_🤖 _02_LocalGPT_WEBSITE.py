import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import Document

from bs4 import BeautifulSoup
import requests
import nltk
import os
import time
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin, urlparse
import logging

# NLTK punkt package download
nltk.download('punkt')
nltk.data.path.append('/path/to/your/nltk_data')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Set up page config
st.set_page_config(
    page_title="Website GPT", 
    page_icon="ğŸ¤–"
)

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í¬ê¸° ìˆœ)
AVAILABLE_MODELS = [
    {"name": "llama3.2:latest", "size": "2.0 GB"},
    {"name": "llama2:latest", "size": "3.8 GB"},
    {"name": "mistral:latest", "size": "4.1 GB"},
    {"name": "llama3.1:latest", "size": "4.9 GB"},
    {"name": "llama3.1:8b", "size": "4.9 GB"},
    {"name": "gemma:latest", "size": "5.0 GB"},
    {"name": "gemma2:latest", "size": "5.4 GB"},
    {"name": "deepseek-r1:14b", "size": "9.0 GB"},
    {"name": "phi4:latest", "size": "9.1 GB"},
    {"name": "deepseek-r1:32b", "size": "19 GB"},
    {"name": "llama3.3:latest", "size": "42 GB"}
]

# ëª¨ë¸ ì„ íƒ UI
st.sidebar.title("ëª¨ë¸ ì„¤ì •")
selected_model = st.sidebar.selectbox(
    "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
    options=[model["name"] for model in AVAILABLE_MODELS],
    format_func=lambda x: f"{x} ({next(m['size'] for m in AVAILABLE_MODELS if m['name'] == x)})",
    index=2  # mistral:latestë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
)

# Temperature ì„¤ì •
temperature = st.sidebar.slider(
    "Temperature:", 
    min_value=0.0, 
    max_value=2.0, 
    value=0.1, 
    step=0.1,
    help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
)

# í¬ë¡¤ë§ ì„¤ì •
st.sidebar.title("í¬ë¡¤ë§ ì„¤ì •")
max_pages = st.sidebar.number_input(
    "ìµœëŒ€ í˜ì´ì§€ ìˆ˜:", 
    min_value=1, 
    max_value=100, 
    value=10,
    help="í¬ë¡¤ë§í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
)

respect_robots = st.sidebar.checkbox(
    "robots.txt ì¤€ìˆ˜", 
    value=True,
    help="ì›¹ì‚¬ì´íŠ¸ì˜ robots.txt íŒŒì¼ì„ í™•ì¸í•˜ê³  í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤."
)

delay_between_requests = st.sidebar.slider(
    "ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„(ì´ˆ):", 
    min_value=0.0, 
    max_value=5.0, 
    value=1.0, 
    step=0.5,
    help="ì„œë²„ ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•œ ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„ì…ë‹ˆë‹¤."
)

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)


# Ollama ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def get_llm_website(model_name, temp):
    return ChatOllama(
        model=model_name,
        temperature=temp,
        streaming=True,
        callbacks=[ChatCallbackHandler()],
    )

def check_robots_txt(url, user_agent='*'):
    """robots.txt íŒŒì¼ì„ í™•ì¸í•˜ì—¬ í¬ë¡¤ë§ í—ˆìš© ì—¬ë¶€ë¥¼ íŒë‹¨"""
    try:
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        
        rp = RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        
        can_fetch = rp.can_fetch(user_agent, url)
        logger.info(f"robots.txt í™•ì¸ - URL: {url}, í—ˆìš©: {can_fetch}")
        return can_fetch
    except Exception as e:
        logger.warning(f"robots.txt í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        return True  # í™•ì¸í•  ìˆ˜ ì—†ëŠ” ê²½ìš° í—ˆìš©ìœ¼ë¡œ ì²˜ë¦¬

def is_valid_url(url, base_domain):
    """URLì´ ìœ íš¨í•˜ê³  ê°™ì€ ë„ë©”ì¸ì¸ì§€ í™•ì¸"""
    try:
        parsed_url = urlparse(url)
        base_parsed = urlparse(base_domain)
        
        # ìœ íš¨í•œ ìŠ¤í‚¤ë§ˆì¸ì§€ í™•ì¸
        if parsed_url.scheme not in ['http', 'https']:
            logger.info(f"ìœ íš¨í•˜ì§€ ì•Šì€ ìŠ¤í‚¤ë§ˆ: {url}")
            return False
        
        # ì²« ë²ˆì§¸ URLì¸ ê²½ìš° ë¬´ì¡°ê±´ í—ˆìš©
        if url == base_domain:
            return True
            
        # ê°™ì€ ë„ë©”ì¸ì¸ì§€ í™•ì¸ (ì„œë¸Œë„ë©”ì¸ë„ í—ˆìš©)
        if not (parsed_url.netloc == base_parsed.netloc or 
                parsed_url.netloc.endswith('.' + base_parsed.netloc)):
            logger.info(f"ë‹¤ë¥¸ ë„ë©”ì¸: {url} (ê¸°ì¤€: {base_domain})")
            return False
            
        # íŒŒì¼ í™•ì¥ì í•„í„°ë§ (ì´ë¯¸ì§€, PDF ë“± ì œì™¸)
        excluded_extensions = ['.pdf', '.jpg', '.jpeg', '.png', '.gif', '.css', '.js', '.ico', '.xml', '.zip', '.exe']
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            logger.info(f"ì œì™¸ëœ íŒŒì¼ í™•ì¥ì: {url}")
            return False
            
        return True
    except Exception as e:
        logger.error(f"URL ìœ íš¨ì„± ê²€ì‚¬ ì˜¤ë¥˜: {e}")
        return False

def test_url_accessibility(url):
    """URL ì ‘ê·¼ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸"""
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        return True, response.status_code, response.headers.get('content-type', 'unknown')
    except Exception as e:
        return False, str(e), None

# Function to scrape website pages and extract text
def scrape_website(url, max_pages=10, respect_robots=True, delay=1.0):
    """ê°œì„ ëœ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜"""
    
    # URL ì •ê·œí™”
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    st.info(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: {url}")
    
    # URL ì ‘ê·¼ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸
    accessible, status, content_type = test_url_accessibility(url)
    if not accessible:
        st.error(f"âŒ URLì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {status}")
        return []
    
    st.success(f"âœ… URL ì ‘ê·¼ ì„±ê³µ (ìƒíƒœì½”ë“œ: {status}, ì½˜í…ì¸  íƒ€ì…: {content_type})")
    
    visited_urls = set()
    base_domain = url
    texts = []
    urls_to_visit = [url]
    
    # ì„¸ì…˜ ì„¤ì •
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    debug_info = st.empty()
    
    page_count = 0
    failed_urls = []
    
    while urls_to_visit and page_count < max_pages:
        current_url = urls_to_visit.pop(0)
        
        if current_url in visited_urls:
            continue
            
        # URL ìœ íš¨ì„± ê²€ì‚¬
        if not is_valid_url(current_url, base_domain):
            logger.info(f"ìœ íš¨í•˜ì§€ ì•Šì€ URL ê±´ë„ˆëœ€: {current_url}")
            continue
            
        # robots.txt í™•ì¸
        if respect_robots and not check_robots_txt(current_url):
            logger.info(f"robots.txtì— ì˜í•´ í¬ë¡¤ë§ì´ ê¸ˆì§€ëœ URL: {current_url}")
            failed_urls.append((current_url, "robots.txt ì°¨ë‹¨"))
            continue
            
        try:
            status_text.text(f"ğŸ”„ í¬ë¡¤ë§ ì¤‘: {current_url}")
            debug_info.text(f"ğŸ“Š ì§„í–‰ìƒí™©: {page_count}/{max_pages} í˜ì´ì§€, {len(texts)}ê°œ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ë¨")
            
            # ìš”ì²­ ë³´ë‚´ê¸°
            response = session.get(current_url, timeout=10)
            response.raise_for_status()
            
            visited_urls.add(current_url)
            page_count += 1
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('content-type', '').lower()
            if 'text/html' not in content_type:
                logger.info(f"HTMLì´ ì•„ë‹Œ ì½˜í…ì¸  ê±´ë„ˆëœ€: {current_url} ({content_type})")
                continue
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.content, "html.parser")
            
            # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
            for tag in soup(["script", "style", "nav", "header", "footer", "aside", "iframe", "noscript"]):
                tag.decompose()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
            page_text = soup.get_text(separator="\n", strip=True)
            
            # ë¹ˆ ì¤„ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬
            lines = [line.strip() for line in page_text.split('\n') if line.strip()]
            cleaned_text = '\n'.join(lines)
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ë° í’ˆì§ˆ í™•ì¸
            if len(cleaned_text) > 200:  # ìµœì†Œ ê¸¸ì´ë¥¼ 200ìë¡œ ì¡°ì •
                # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸ (ë¬¸ìì™€ ê³µë°±ì˜ ë¹„ìœ¨)
                text_chars = sum(1 for c in cleaned_text if c.isalnum() or c.isspace())
                if text_chars / len(cleaned_text) > 0.8:  # 80% ì´ìƒì´ ì¼ë°˜ í…ìŠ¤íŠ¸
                    page_title = soup.title.string.strip() if soup.title and soup.title.string else 'No Title'
                    texts.append({
                        'content': cleaned_text,
                        'url': current_url,
                        'title': page_title,
                        'length': len(cleaned_text)
                    })
                    logger.info(f"í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì„±ê³µ: {current_url} ({len(cleaned_text)}ì)")
                else:
                    logger.info(f"í…ìŠ¤íŠ¸ í’ˆì§ˆ ë¶ˆëŸ‰ìœ¼ë¡œ ê±´ë„ˆëœ€: {current_url}")
            else:
                logger.info(f"í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶€ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€: {current_url} ({len(cleaned_text)}ì)")
            
            # ìƒˆë¡œìš´ ë§í¬ ì°¾ê¸° (ìµœëŒ€ í˜ì´ì§€ ìˆ˜ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°ì—ë§Œ)
            if page_count < max_pages:
                new_links_found = 0
                for link in soup.find_all("a", href=True):
                    absolute_link = urljoin(current_url, link['href'])
                    # URL ì •ë¦¬ (ì•µì»¤ ì œê±°)
                    absolute_link = absolute_link.split('#')[0]
                    
                    if (absolute_link not in visited_urls and 
                        absolute_link not in urls_to_visit and 
                        is_valid_url(absolute_link, base_domain)):
                        urls_to_visit.append(absolute_link)
                        new_links_found += 1
                        if new_links_found >= 10:  # í•œ í˜ì´ì§€ì—ì„œ ìµœëŒ€ 10ê°œ ë§í¬ë§Œ
                            break
                
                logger.info(f"ìƒˆë¡œìš´ ë§í¬ {new_links_found}ê°œ ë°œê²¬: {current_url}")
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress = min(page_count / max_pages, 1.0)
            progress_bar.progress(progress)
            
            # ì§€ì—°ì‹œê°„
            if delay > 0:
                time.sleep(delay)
                
        except requests.RequestException as e:
            error_msg = f"ìš”ì²­ ì˜¤ë¥˜ - {current_url}: {str(e)}"
            logger.error(error_msg)
            failed_urls.append((current_url, str(e)))
            continue
        except Exception as e:
            error_msg = f"í¬ë¡¤ë§ ì˜¤ë¥˜ - {current_url}: {str(e)}"
            logger.error(error_msg)
            failed_urls.append((current_url, str(e)))
            continue
    
    progress_bar.empty()
    status_text.empty()
    debug_info.empty()
    
    # ê²°ê³¼ ìš”ì•½
    if texts:
        total_text_length = sum(t['length'] for t in texts)
        st.success(f"âœ… ì´ {len(texts)}ê°œì˜ í˜ì´ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ í¬ë¡¤ë§í–ˆìŠµë‹ˆë‹¤. (ì´ {total_text_length:,}ì)")
        
        # í†µê³„ ì €ì¥
        st.session_state.last_crawl_stats = {
            'pages': len(texts),
            'total_chars': total_text_length,
            'failed_urls': len(failed_urls),
            'success_rate': len(texts) / (len(texts) + len(failed_urls)) * 100 if (len(texts) + len(failed_urls)) > 0 else 0
        }
        
        # ìƒì„¸ ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“‹ í¬ë¡¤ë§ ìƒì„¸ ì •ë³´"):
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ì„±ê³µí•œ í˜ì´ì§€", len(texts))
                st.metric("ì‹¤íŒ¨í•œ í˜ì´ì§€", len(failed_urls))
            with col2:
                success_rate = st.session_state.last_crawl_stats['success_rate']
                st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
                st.metric("í‰ê·  í˜ì´ì§€ ê¸¸ì´", f"{total_text_length // len(texts):,}ì")
            
            st.write("**ìˆ˜ì§‘ëœ í˜ì´ì§€:**")
            for i, text_info in enumerate(texts[:5]):  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                st.write(f"{i+1}. **{text_info['title']}** ({text_info['length']:,}ì)")
                st.write(f"   URL: {text_info['url']}")
            
            if len(texts) > 5:
                st.write(f"... ì™¸ {len(texts)-5}ê°œ í˜ì´ì§€")
    else:
        st.error(f"âŒ í¬ë¡¤ë§ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # í†µê³„ ì €ì¥ (ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
        st.session_state.last_crawl_stats = {
            'pages': 0,
            'total_chars': 0,
            'failed_urls': len(failed_urls),
            'success_rate': 0
        }
        
        # ì‹¤íŒ¨ ì›ì¸ ë¶„ì„
        if failed_urls:
            with st.expander("âŒ ì‹¤íŒ¨í•œ URLë“¤"):
                for url, reason in failed_urls[:10]:  # ìƒìœ„ 10ê°œë§Œ í‘œì‹œ
                    st.write(f"- {url}: {reason}")
        
        # ë””ë²„ê¹… ì œì•ˆ
        st.info("""
        ğŸ’¡ **ë¬¸ì œ í•´ê²° ë°©ë²•:**
        1. robots.txt ì¤€ìˆ˜ë¥¼ ë¹„í™œì„±í™”í•´ë³´ì„¸ìš”
        2. ë‹¤ë¥¸ URLì„ ì‹œë„í•´ë³´ì„¸ìš”
        3. ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”
        4. ì§€ì—°ì‹œê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”
        """)
    
    return texts

# Function to embed the website's scraped text
@st.cache_data(show_spinner="Embedding website content...")
def embed_website(url, model_name, max_pages=10, respect_robots=True, delay=1.0):
    # ë…ë¦½ì ì¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
    safe_url = url.replace('/', '_').replace(':', '_').replace('?', '_').replace('&', '_')
    cache_dir = LocalFileStore(f"./website_embeddings/{safe_url}")
    
    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • ê°œì„ 
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=800,  # ì²­í¬ ì‚¬ì´ì¦ˆ ì¦ê°€
        chunk_overlap=200,  # ì˜¤ë²„ë© ì¦ê°€
    )

    # ì›¹ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸ ìŠ¤í¬ë˜í•‘
    web_data = scrape_website(url, max_pages, respect_robots, delay)
    
    if not web_data:
        st.error("í¬ë¡¤ë§ëœ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None
    
    # Document ê°ì²´ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
    docs = []
    for data in web_data:
        docs.append(Document(
            page_content=data['content'], 
            metadata={
                "source": data['url'],
                "title": data['title'],
                "domain": urlparse(url).netloc
            }
        ))

    # ë¬¸ì„œ ë¶„í• 
    split_docs = []
    for doc in docs:
        split_texts = splitter.split_text(doc.page_content)
        for i, split_text in enumerate(split_texts):
            split_docs.append(Document(
                page_content=split_text, 
                metadata={
                    **doc.metadata,
                    "chunk_id": i
                }
            ))

    if not split_docs:
        st.error("ë¶„í• ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
    try:
        embeddings = OllamaEmbeddings(model=model_name)
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

        vectorstore = FAISS.from_documents(split_docs, cached_embeddings)
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
        )

        return retriever
    except Exception as e:
        st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# Function to save chat messages
def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def save_message(message, role):
    if "website_messages" not in st.session_state:
        st.session_state["website_messages"] = []
    st.session_state["website_messages"].append({"message": message, "role": role})

# Function to display chat history
def paint_history():
    if "website_messages" in st.session_state:
        for message in st.session_state["website_messages"]:
            send_message(
                message["message"],
                message["role"],
                save=False,
            )

# Function to format retrieved documents
def format_docs(docs):
    formatted_docs = []
    for i, doc in enumerate(docs):
        content = doc.page_content
        source = doc.metadata.get('source', 'Unknown')
        title = doc.metadata.get('title', 'No Title')
        
        formatted_docs.append(f"[ë¬¸ì„œ {i+1}]\nì œëª©: {title}\nì¶œì²˜: {source}\në‚´ìš©: {content}")
    
    return "\n\n".join(formatted_docs)

# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_template(
    """ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì˜ ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. 
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.
    ë‹µë³€í•  ë•ŒëŠ” ê´€ë ¨ëœ ì¶œì²˜(URL)ë„ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
    {context}
    
    ì§ˆë¬¸: {question}
    
    ë‹µë³€:"""
)

st.title("Website Scraping GPT")
st.markdown(f"ğŸ¤– í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{selected_model}**")

st.markdown(
    """
    Welcome! 
    Enter a website URL to scrape and use the content for AI-powered question answering.
    
    âš ï¸ **ì£¼ì˜ì‚¬í•­:**
    - ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ê³¼ robots.txtë¥¼ ì¤€ìˆ˜í•©ë‹ˆë‹¤
    - ê³¼ë„í•œ í¬ë¡¤ë§ìœ¼ë¡œ ì¸í•œ ì„œë²„ ë¶€í•˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì§€ì—°ì‹œê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤
    - ê°œì¸ì •ë³´ë‚˜ ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸ ëŠ” ì£¼ì˜í•´ì„œ ì‚¬ìš©í•˜ì„¸ìš”
    """
)

# User inputs URL in the sidebar
with st.sidebar:
    st.title("ì›¹ì‚¬ì´íŠ¸ ì…ë ¥")
    website_url = st.text_input("Enter website URL", placeholder="https://example.com")
    
    # URL í…ŒìŠ¤íŠ¸ ê¸°ëŠ¥
    if website_url:
        if st.button("ğŸ” URL ì ‘ê·¼ í…ŒìŠ¤íŠ¸"):
            # URL ì •ê·œí™”
            test_url = website_url
            if not test_url.startswith(('http://', 'https://')):
                test_url = 'https://' + test_url
            
            with st.spinner("URL í…ŒìŠ¤íŠ¸ ì¤‘..."):
                accessible, status, content_type = test_url_accessibility(test_url)
                
                if accessible:
                    st.success(f"âœ… ì ‘ê·¼ ì„±ê³µ!")
                    st.write(f"**ìƒíƒœì½”ë“œ:** {status}")
                    st.write(f"**ì½˜í…ì¸  íƒ€ì…:** {content_type}")
                    
                    # robots.txt í™•ì¸
                    if respect_robots:
                        robots_allowed = check_robots_txt(test_url)
                        if robots_allowed:
                            st.success("âœ… robots.txt í—ˆìš©")
                        else:
                            st.warning("âš ï¸ robots.txtì—ì„œ í¬ë¡¤ë§ ì°¨ë‹¨ë¨")
                    
                    # ê°„ë‹¨í•œ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸
                    try:
                        response = requests.get(test_url, timeout=10)
                        soup = BeautifulSoup(response.content, "html.parser")
                        
                        # í˜ì´ì§€ ì •ë³´
                        title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
                        links = len(soup.find_all("a", href=True))
                        text_length = len(soup.get_text())
                        
                        st.write(f"**í˜ì´ì§€ ì œëª©:** {title}")
                        st.write(f"**ë§í¬ ìˆ˜:** {links}ê°œ")
                        st.write(f"**í…ìŠ¤íŠ¸ ê¸¸ì´:** {text_length:,}ì")
                        
                    except Exception as e:
                        st.error(f"ìƒì„¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
                else:
                    st.error(f"âŒ ì ‘ê·¼ ì‹¤íŒ¨: {status}")
                    
                    # ì¼ë°˜ì ì¸ í•´ê²° ë°©ë²• ì œì•ˆ
                    st.info("""
                    ğŸ’¡ **í•´ê²° ë°©ë²•:**
                    - URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
                    - 'http://' ë˜ëŠ” 'https://' í¬í•¨ ì—¬ë¶€ í™•ì¸
                    - ì›¹ì‚¬ì´íŠ¸ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    - ë°©í™”ë²½ì´ë‚˜ ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸
                    """)

if website_url:
    try:
        retriever = embed_website(
            website_url, 
            selected_model, 
            max_pages, 
            respect_robots, 
            delay_between_requests
        )
        
        if retriever:
            send_message("ğŸ‰ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤! ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”.", "ai", save=False)
            paint_history()
            
            message = st.chat_input("ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”...")
            if message:
                send_message(message, "human")
                
                # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ë¡œ LLM ì´ˆê¸°í™”
                llm = get_llm_website(selected_model, temperature)
                
                chain = (
                    {
                        "context": retriever | RunnableLambda(format_docs),
                        "question": RunnablePassthrough(),
                    }
                    | prompt
                    | llm
                )
                with st.chat_message("ai"):
                    try:
                        chain.invoke(message)
                    except Exception as e:
                        st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        st.info("ë‹¤ì‹œ ì‹œë„í•´ë³´ê±°ë‚˜ ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”.")
        else:
            st.error("âŒ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ì¶”ê°€ ë„ì›€ë§
            with st.expander("ğŸ”§ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ"):
                st.markdown("""
                ### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•:
                
                1. **URL ì ‘ê·¼ ë¶ˆê°€**
                   - URLì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
                   - ì›¹ì‚¬ì´íŠ¸ê°€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                   - VPNì´ë‚˜ í”„ë¡ì‹œ ì„¤ì • í™•ì¸
                
                2. **robots.txt ì°¨ë‹¨**
                   - ì‚¬ì´ë“œë°”ì—ì„œ "robots.txt ì¤€ìˆ˜" ì˜µì…˜ í•´ì œ
                   - ë‹¤ë¥¸ ì›¹ì‚¬ì´íŠ¸ë¡œ ì‹œë„
                
                3. **í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨**
                   - JavaScriptê°€ ë§ì´ ì‚¬ìš©ëœ SPA ì‚¬ì´íŠ¸ëŠ” í¬ë¡¤ë§ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŒ
                   - ì •ì  HTML ì½˜í…ì¸ ê°€ ë§ì€ ì‚¬ì´íŠ¸ë¥¼ ì‹œë„
                
                4. **ì†ë„ ì œí•œ**
                   - ìš”ì²­ ê°„ ì§€ì—°ì‹œê°„ì„ ëŠ˜ë ¤ë³´ì„¸ìš”
                   - ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”
                
                ### ì¶”ì²œ í…ŒìŠ¤íŠ¸ ì‚¬ì´íŠ¸:
                - https://example.com (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)
                - https://httpbin.org (HTTP í…ŒìŠ¤íŠ¸)
                - ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë‚˜ ë¸”ë¡œê·¸ (í…ìŠ¤íŠ¸ ì½˜í…ì¸  í’ë¶€)
                """)
            
    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        logger.error(f"ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜: {e}")
        
        # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ› ë””ë²„ê¹… ì •ë³´"):
            st.code(str(e))
            st.write("**ì˜¤ë¥˜ íƒ€ì…:**", type(e).__name__)
            
            # ì‹œìŠ¤í…œ ì •ë³´
            st.write("**ì‹œìŠ¤í…œ ì •ë³´:**")
            st.write(f"- Python ë²„ì „: {os.sys.version}")
            st.write(f"- í˜„ì¬ ë””ë ‰í† ë¦¬: {os.getcwd()}")

if st.sidebar.button("ğŸ’¬ ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”"):
    st.session_state["website_messages"] = []
    st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "website_messages" not in st.session_state:
    st.session_state["website_messages"] = []
    
# ì¶”ê°€ ì •ë³´ ì‚¬ì´ë“œë°”
with st.sidebar:
    st.markdown("---")
    st.markdown("### ğŸ“Š í¬ë¡¤ë§ í†µê³„")
    if "last_crawl_stats" in st.session_state:
        stats = st.session_state.last_crawl_stats
        st.metric("ë§ˆì§€ë§‰ í¬ë¡¤ë§ í˜ì´ì§€", stats.get('pages', 0))
        st.metric("ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ê¸¸ì´", f"{stats.get('total_chars', 0):,}ì")
    
    st.markdown("### â„¹ï¸ ë„ì›€ë§")
    with st.expander("ì‚¬ìš©ë²•"):
        st.markdown("""
        1. ì›¹ì‚¬ì´íŠ¸ URL ì…ë ¥
        2. í¬ë¡¤ë§ ì„¤ì • ì¡°ì •
        3. URL í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)
        4. í¬ë¡¤ë§ ì‹¤í–‰
        5. AIì—ê²Œ ì§ˆë¬¸í•˜ê¸°
        """)
    
    with st.expander("ì£¼ì˜ì‚¬í•­"):
        st.markdown("""
        - ì €ì‘ê¶Œ ë³´í˜¸ ì½˜í…ì¸  ì£¼ì˜
        - ê³¼ë„í•œ í¬ë¡¤ë§ ê¸ˆì§€
        - robots.txt ì¤€ìˆ˜ ê¶Œì¥
        - ê°œì¸ì •ë³´ í¬í•¨ ì‚¬ì´íŠ¸ ì£¼ì˜
        """)