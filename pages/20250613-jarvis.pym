import streamlit as st
import os
import base64
import pandas as pd
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor
from langchain_anthropic import ChatAnthropic
import anthropic
import mysql.connector
import json
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import io
import re
from sqlalchemy import create_engine
from langchain.agents import Tool, initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchResults, WikipediaQueryRun, Tool
from langchain_experimental.tools import PythonREPLTool
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.utilities import WikipediaAPIWrapper

st.set_page_config(page_title="ë‚˜ë§Œì˜ Jarvis", layout="wide")
st.title("[âš™ï¸ J.A.R.V.I.S. ONLINE âš™ï¸]")

# ì¸ì¦ ê¸°ëŠ¥ (ê°„ë‹¨í•œ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸)
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False
admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.')
    st.stop()
if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()
        
# --- DB ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜: ë°˜ë“œì‹œ ìƒë‹¨ì— ìœ„ì¹˜ ---
def load_conversation_history(user_id, search_keyword=None, limit=100):
    try:
        conn = mysql.connector.connect(
            host=os.getenv('SQL_HOST'),
            user=os.getenv('SQL_USER'),
            password=os.getenv('SQL_PASSWORD'),
            database=os.getenv('SQL_DATABASE_NEWBIZ'),
            charset='utf8mb4'
        )
        cursor = conn.cursor(dictionary=True)
        query = "SELECT * FROM jarvis_interactions WHERE user_id = %s"
        params = [user_id]
        if search_keyword:
            query += " AND (user_input LIKE %s OR jarvis_response LIKE %s)"
            kw = f"%{search_keyword}%"
            params.extend([kw, kw])
        query += " ORDER BY created_at DESC LIMIT %s"
        params.append(limit)
        cursor.execute(query, params)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()
        return rows
    except Exception as e:
        st.warning(f"[JARVIS ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° ì˜¤ë¥˜] {e}")
        return []

# --- Session state initialization (must be at the very top, before any UI or logic) ---
if 'selected_ai_api' not in st.session_state:
    st.session_state.selected_ai_api = 'Anthropic (Claude)'
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = 'claude-3-7-sonnet-latest'
# --- JARVIS ê²°ê³¼/ë¡œê·¸ ì„¸ì…˜ ìƒíƒœë„ í•­ìƒ ì´ˆê¸°í™” ---
if 'jarvis_last_results' not in st.session_state:
    st.session_state.jarvis_last_results = None
if 'jarvis_last_logs' not in st.session_state:
    st.session_state.jarvis_last_logs = None

# --- Chat history state ---
if 'chat_history' not in st.session_state:
    st.session_state['chat_history'] = []

# --- 0. í™”ì(ì‚¬ìš©ì) ì„ íƒ UI (í•­ìƒ ìƒë‹¨ì— ìœ„ì¹˜) ---
SPEAKERS = ["ìƒí˜„ë‹˜","ê²½í˜¸ë‹˜","ì„±ë²”ë‹˜","ì„±ì¼ë‹˜","ì¬ì›ë‹˜","ì°½í™˜ë‹˜", "í˜„ì² ë‹˜"]
if 'selected_speaker' not in st.session_state:
    st.session_state['selected_speaker'] = SPEAKERS[0]
selected_speaker = st.selectbox('í™”ì(ì‚¬ìš©ì) ì„ íƒ', SPEAKERS, key='selected_speaker')

# --- 1. AI API/ëª¨ë¸ ì„ íƒ UI (DBìƒì„±.py ì°¸ê³ ) ---
has_anthropic_key = os.environ.get('ANTHROPIC_API_KEY') is not None
has_openai_key = os.environ.get('OPENAI_API_KEY') is not None
available_models = []
if has_anthropic_key:
    available_models.extend([
        'claude-3-7-sonnet-latest',
        'claude-3-5-sonnet-latest',
        'claude-3-5-haiku-latest',
    ])
if has_openai_key:
    available_models.extend(['gpt-4o', 'gpt-4o-mini'])
if not available_models:
    available_models = ['claude-3-7-sonnet-latest']

col_api, col_model = st.columns([1,2])
with col_api:
    selected_ai_api = st.selectbox(
        'AI API ì„ íƒ',
        options=['Anthropic (Claude)', 'OpenAI (GPT)'],
        index=0 if st.session_state.selected_ai_api == 'Anthropic (Claude)' else 1
    )
    st.session_state.selected_ai_api = selected_ai_api
with col_model:
    filtered_models = [m for m in available_models if (('claude' in m and selected_ai_api=='Anthropic (Claude)') or ('gpt' in m and selected_ai_api=='OpenAI (GPT)'))]
    if not filtered_models:
        filtered_models = ['claude-3-7-sonnet-latest']
    selected_model = st.selectbox(
        'AI ëª¨ë¸ ì„ íƒ',
        options=filtered_models,
        index=filtered_models.index(st.session_state.selected_model) if st.session_state.selected_model in filtered_models else 0
    )
    st.session_state.selected_model = selected_model

# --- 2. JARVIS í˜ë¥´ì†Œë‚˜(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸) ---
JARVIS_SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì•„ì´ì–¸ë§¨(í† ë‹ˆ ìŠ¤íƒ€í¬)ì˜ ì¸ê³µì§€ëŠ¥ ë¹„ì„œ JARVISì…ë‹ˆë‹¤. "
    "í•­ìƒ ë…¼ë¦¬ì ì´ê³ , ì‹ ì†í•˜ë©°, ì˜ˆì˜ ë°”ë¥´ê³ , ì „ë¬¸ê°€ ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
    "ëª¨ë“  ë‹µë³€ì€ 'JARVIS:'ë¡œ ì‹œì‘í•˜ë©°, ìì‹ ì„ JARVIS(ì•„ì´ì–¸ë§¨ì˜ AI)ë¡œë§Œ ì¸ì‹í•©ë‹ˆë‹¤. "
    "ì ˆëŒ€ ìì‹ ì„ ë‹¤ë¥¸ AI, ì±—ë´‡, ì–´ì‹œìŠ¤í„´íŠ¸, ì¸ê°„ ë“±ìœ¼ë¡œ ì†Œê°œí•˜ì§€ ë§ˆì„¸ìš”. "
    "ë¶ˆí•„ìš”í•œ ë†ë‹´ì´ë‚˜ ì•„ì´ì–¸ë§¨ ê´€ë ¨ ë†ë‹´ì€ í•˜ì§€ ë§ˆì„¸ìš”. "
    "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë¶ˆì™„ì „í•´ë„, í† ë‹ˆ ìŠ¤íƒ€í¬ì˜ ë¹„ì„œë‹µê²Œ í•µì‹¬ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•´ ëª…í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µì„ ì œì‹œí•˜ì„¸ìš”. "
    "(ëª¨ë“  ë‹µë³€ì€ JARVISì˜ í˜ë¥´ì†Œë‚˜ë¡œë§Œ!)"
)

# --- 3. DB/íŒŒì¼/ìŒì„± í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼) ---
def load_user_history(user_id):
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'claude-3-7-sonnet-latest'
    if 'selected_ai_api' not in st.session_state:
        st.session_state.selected_ai_api = 'Anthropic (Claude)'
    # DBì—ì„œ ëŒ€í™”/ì§€ì‹/íŒŒì¼ íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (ì˜ˆì‹œ)
    # ì‹¤ì œë¡œëŠ” MySQL ì¿¼ë¦¬ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
    return [
        {"role": "user", "content": "ì§€ë‚œë²ˆ íšŒì˜ ìš”ì•½ ë³´ì—¬ì¤˜"},
        {"role": "jarvis", "content": "2024-06-10 íšŒì˜ ìš”ì•½: ..."}
    ]

def save_interaction(user_id, speaker, msg, files=None, audio=None):
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'claude-3-7-sonnet-latest'
    if 'selected_ai_api' not in st.session_state:
        st.session_state.selected_ai_api = 'Anthropic (Claude)'
    try:
        conn = mysql.connector.connect(
            host=os.getenv('SQL_HOST'),
            user=os.getenv('SQL_USER'),
            password=os.getenv('SQL_PASSWORD'),
            database=os.getenv('SQL_DATABASE_NEWBIZ'),
            charset='utf8mb4'
        )
        cursor = conn.cursor()
        user_input = msg.get('input', None)
        jarvis_response = None
        if msg.get('results') and isinstance(msg['results'], dict):
            jarvis_response = msg['results'].get('ëŒ€í™”ì—ì´ì „íŠ¸', None)
        logs_json = json.dumps(msg.get('logs', None), ensure_ascii=False) if msg.get('logs') else None
        files_json = json.dumps(files, ensure_ascii=False) if files else None
        audio_blob = audio if audio else None
        cursor.execute("""
            INSERT INTO jarvis_interactions
            (user_id, speaker, user_input, jarvis_response, files_json, audio_blob, logs_json, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, NOW())
        """, (
            user_id,
            speaker,
            user_input,
            jarvis_response,
            files_json,
            audio_blob,
            logs_json
        ))
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        st.warning(f"[JARVIS ëŒ€í™” ì €ì¥ ì˜¤ë¥˜] {e}")

def parse_uploaded_files(uploaded_files):
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'claude-3-7-sonnet-latest'
    if 'selected_ai_api' not in st.session_state:
        st.session_state.selected_ai_api = 'Anthropic (Claude)'
    # íŒŒì¼ íŒŒì‹±/ë¯¸ë¦¬ë³´ê¸° (í…ìŠ¤íŠ¸, PDF, docx, xlsx ì¼ë¶€ ì§€ì›)
    previews = []
    for f in uploaded_files:
        ext = os.path.splitext(f.name)[-1].lower()
        try:
            if ext in [".txt", ".md"]:
                content = f.read().decode("utf-8")
            elif ext == ".pdf":
                import PyPDF2
                reader = PyPDF2.PdfReader(f)
                content = "\n".join([page.extract_text() for page in reader.pages if page.extract_text()])
            elif ext == ".docx":
                from docx import Document
                doc = Document(f)
                content = "\n".join([p.text for p in doc.paragraphs])
            elif ext == ".xlsx":
                df = pd.read_excel(f)
                content = df.to_csv(index=False)
            else:
                content = f"[{f.name}] ë¯¸ë¦¬ë³´ê¸° ì§€ì› ì•ˆë¨"
        except Exception as e:
            content = f"[{f.name}] íŒŒì‹± ì˜¤ë¥˜: {e}"
        previews.append({"filename": f.name, "preview": content})
    return previews

def text_to_speech(text):
    if 'selected_model' not in st.session_state:
        st.session_state.selected_model = 'claude-3-7-sonnet-latest'
    if 'selected_ai_api' not in st.session_state:
        st.session_state.selected_ai_api = 'Anthropic (Claude)'
    # ì‹¤ì œ OpenAI TTS ì—°ë™ ì˜ˆì‹œ (API í‚¤ í•„ìš”)
    try:
        from openai import OpenAI
        openai_key = os.getenv('OPENAI_API_KEY')
        if not openai_key:
            return None
        client = OpenAI(api_key=openai_key)
        response = client.audio.speech.create(
            model="tts-1",
            voice="alloy",
            input=text
        )
        audio_data = response.content
        audio_base64 = base64.b64encode(audio_data).decode('utf-8')
        audio_html = f'<audio controls><source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3"></audio>'
        return audio_html
    except Exception as e:
        return f"[TTS ì˜¤ë¥˜] {e}"

# --- JARVIS ëŒ€í™”ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„± ë³´ì¡° í•¨ìˆ˜ ---
def should_attach_history(user_input):
    keywords = [
        'ì´ì „ ëŒ€í™”', 'ìµœê·¼ ê¸°ë¡', 'ì§€ë‚œ íšŒì˜', 'ëŒ€í™” íˆìŠ¤í† ë¦¬', 'ìµœê·¼ ì§ˆë¬¸', 'ìµœê·¼ ë‹µë³€',
        'ëŒ€í™” ë‚´ì—­', 'ê³¼ê±° ëŒ€í™”', 'ë§ˆì§€ë§‰ ëŒ€í™”', 'ìµœê·¼ ëŒ€í™”', 'ê¸°ë¡ ë³´ì—¬ì¤˜', 'ê¸°ë¡ ì•Œë ¤ì¤˜',
        'ëŒ€í™” ë³´ì—¬ì¤˜', 'ëŒ€í™” ì•Œë ¤ì¤˜', 'íˆìŠ¤í† ë¦¬', 'history', 'log', 'meeting', 'ìš”ì•½'
    ]
    return any(k in user_input for k in keywords)

def get_recent_history_for_prompt(user_id, n=5):
    rows = load_conversation_history(user_id, limit=n)
    if not rows:
        return "ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
    history_lines = []
    for r in rows:
        history_lines.append(f"[{r['created_at']}] ì§ˆë¬¸: {r['user_input']}\në‹µë³€: {r['jarvis_response']}")
    return '\n\n'.join(history_lines)

# --- íŒŒì¼/ìŒì„± í…ìŠ¤íŠ¸ ì¶”ì¶œ ë³´ì¡° í•¨ìˆ˜ (ê°„ë‹¨ ë²„ì „) ---
def extract_text_from_fileinfo(fileinfo):
    # fileinfo: dict with keys like 'name', 'content', 'type', 'text' (if extracted)
    if not fileinfo:
        return ''
    # 1. í…ìŠ¤íŠ¸ í•„ë“œê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if isinstance(fileinfo, dict) and 'text' in fileinfo and fileinfo['text']:
        return fileinfo['text']
    # 2. íŒŒì¼ëª…/íƒ€ì… ë“± ë©”íƒ€ì •ë³´
    name = fileinfo.get('name', '')
    ftype = fileinfo.get('type', '')
    return f"[íŒŒì¼: {name} ({ftype})]"

def extract_text_from_audio(audio_blob):
    # ì‹¤ì œë¡œëŠ” STT ë“±ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ í•„ìš”. ì—¬ê¸°ì„  placeholder
    if audio_blob:
        return '[ìŒì„± íŒŒì¼: í…ìŠ¤íŠ¸ ë³€í™˜ ê²°ê³¼(ì˜ˆì‹œ)]'
    return ''

def get_all_table_names():
    try:
        conn = mysql.connector.connect(
            host=os.getenv('SQL_HOST'),
            user=os.getenv('SQL_USER'),
            password=os.getenv('SQL_PASSWORD'),
            database=os.getenv('SQL_DATABASE_NEWBIZ'),
            charset='utf8mb4'
        )
        cursor = conn.cursor()
        cursor.execute("SHOW TABLES")
        tables = [row[0] for row in cursor.fetchall()]
        cursor.close()
        conn.close()
        return tables
    except Exception as e:
        st.warning(f"[JARVIS í…Œì´ë¸” ëª©ë¡ ì˜¤ë¥˜] {e}")
        return []

def search_db_tables_for_relevance(user_input, max_rows=100, topn=2):
    tables = get_all_table_names()
    relevant_tables = []
    table_summaries = []
    # Prepare SQLAlchemy engine once
    engine = create_engine(
        f"mysql+mysqlconnector://{os.getenv('SQL_USER')}:{os.getenv('SQL_PASSWORD')}@{os.getenv('SQL_HOST')}/{os.getenv('SQL_DATABASE_NEWBIZ')}?charset=utf8mb4"
    )
    for table in tables:
        try:
            query = f"SELECT * FROM {table} LIMIT {max_rows}"
            df = pd.read_sql(query, engine)
            if df.empty:
                continue
            # ëª¨ë“  ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
            docs = df.astype(str).apply(lambda row: ' '.join(row), axis=1).tolist()
            vectorizer = TfidfVectorizer().fit(docs + [user_input])
            doc_vecs = vectorizer.transform(docs)
            query_vec = vectorizer.transform([user_input])
            sims = cosine_similarity(query_vec, doc_vecs)[0]
            top_idx = np.argsort(sims)[::-1][:topn]
            # ìœ ì‚¬ë„ê°€ 0.01 ì´ìƒì¸ í–‰ë§Œ ì¶”ì¶œ
            relevant_rows = [(i, sims[i]) for i in top_idx if sims[i] > 0.01]
            if relevant_rows:
                relevant_tables.append(table)
                # ìƒìœ„ Nê°œ í–‰ ìš”ì•½
                summary = '\n'.join([f"[{table}] {docs[i][:200]}... (ìœ ì‚¬ë„: {sims[i]:.2f})" for i, _ in relevant_rows])
                table_summaries.append(summary)
        except Exception as e:
            continue
    return relevant_tables, table_summaries

# --- ì›¹ì‚¬ì´íŠ¸ RAG: í¬ë¡¤ë§ í•¨ìˆ˜ (Virtual Company ì°¸ê³ ) ---
def scrape_website_simple(url, max_pages=5):
    import requests
    from bs4 import BeautifulSoup
    from urllib.parse import urljoin, urlparse
    import time
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        visited_urls = set()
        texts = []
        urls_to_visit = [url]
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        successfully_scraped = 0
        while urls_to_visit and successfully_scraped < max_pages:
            current_url = urls_to_visit.pop(0)
            if current_url in visited_urls:
                continue
            try:
                response = session.get(current_url, timeout=10)
                response.raise_for_status()
                visited_urls.add(current_url)
                soup = BeautifulSoup(response.content, "html.parser")
                for tag in soup(["script", "style", "nav", "header", "footer"]):
                    tag.decompose()
                page_text = soup.get_text(separator="\n", strip=True)
                lines = [line.strip() for line in page_text.split('\n') if line.strip()]
                cleaned_text = '\n'.join(lines)
                if len(cleaned_text) > 50:
                    page_title = soup.title.string.strip() if soup.title and soup.title.string else f'í˜ì´ì§€ {successfully_scraped + 1}'
                    texts.append({
                        'content': cleaned_text,
                        'url': current_url,
                        'title': page_title
                    })
                    successfully_scraped += 1
                base_domain = urlparse(url).netloc
                for link in soup.find_all("a", href=True)[:20]:
                    absolute_link = urljoin(current_url, link['href'])
                    parsed_link = urlparse(absolute_link)
                    if (parsed_link.netloc == base_domain and 
                        absolute_link not in visited_urls and 
                        absolute_link not in urls_to_visit and
                        not any(x in absolute_link.lower() for x in ['#', 'javascript:', 'mailto:', 'tel:']) and
                        len(urls_to_visit) < 50):
                        urls_to_visit.append(absolute_link)
                time.sleep(0.5)
            except Exception:
                continue
        return texts
    except Exception:
        return []

# --- RAG ì†ŒìŠ¤ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_rag_sources(user_id, user_input, file_previews=None, website_texts=None):
    # DB ëŒ€í™”/ì§€ì‹/íŒŒì¼/ìŒì„± ê¸°ë¡
    rows = load_conversation_history(user_id, limit=200)
    db_keyword_matches = []
    db_semantic_matches = []
    chat_keyword_matches = []
    chat_semantic_matches = []
    file_keyword_matches = []
    file_semantic_matches = []
    web_keyword_matches = []
    web_semantic_matches = []
    keyword = user_input.strip().lower()
    # DB: keyword match & semantic
    for r in rows:
        if r.get('user_input') and keyword in str(r['user_input']).lower():
            db_keyword_matches.append(f"[DB] ì§ˆë¬¸: {r['user_input']}\në‹µë³€: {r['jarvis_response']}")
        elif r.get('jarvis_response') and keyword in str(r['jarvis_response']).lower():
            db_keyword_matches.append(f"[DB] ì§ˆë¬¸: {r['user_input']}\në‹µë³€: {r['jarvis_response']}")
    # DB: semantic (ìœ ì‚¬ë„)
    db_docs = [f"ì§ˆë¬¸: {r['user_input']}\në‹µë³€: {r['jarvis_response']}" for r in rows]
    if db_docs:
        vectorizer = TfidfVectorizer().fit(db_docs + [user_input])
        doc_vecs = vectorizer.transform(db_docs)
        query_vec = vectorizer.transform([user_input])
        sims = cosine_similarity(query_vec, doc_vecs)[0]
        top_idx = np.argsort(sims)[::-1][:5]
        db_semantic_matches = [db_docs[i] for i in top_idx if sims[i] > 0.1]
    # ëŒ€í™”/íŒŒì¼/ì›¹: keyword & semantic
    if file_previews:
        for f in file_previews:
            if keyword in f.get('preview','').lower():
                file_keyword_matches.append(f"[íŒŒì¼] {f['filename']}: {f['preview'][:300]}")
        file_docs = [f.get('preview','') for f in file_previews if f.get('preview')]
        if file_docs:
            vectorizer = TfidfVectorizer().fit(file_docs + [user_input])
            doc_vecs = vectorizer.transform(file_docs)
            query_vec = vectorizer.transform([user_input])
            sims = cosine_similarity(query_vec, doc_vecs)[0]
            top_idx = np.argsort(sims)[::-1][:3]
            file_semantic_matches = [file_docs[i] for i in top_idx if sims[i] > 0.1]
    if website_texts:
        for i, w in enumerate(website_texts):
            if keyword in w.lower():
                web_keyword_matches.append(f"[ì›¹ì‚¬ì´íŠ¸] {w[:300]}")
        if website_texts:
            vectorizer = TfidfVectorizer().fit(website_texts + [user_input])
            doc_vecs = vectorizer.transform(website_texts)
            query_vec = vectorizer.transform([user_input])
            sims = cosine_similarity(query_vec, doc_vecs)[0]
            top_idx = np.argsort(sims)[::-1][:3]
            web_semantic_matches = [website_texts[i] for i in top_idx if sims[i] > 0.1]
    # ëŒ€í™” íˆìŠ¤í† ë¦¬(ì„¸ì…˜)
    for msg in st.session_state.get('chat_history', []):
        if msg['role'] == 'user' and keyword in msg['content'].lower():
            chat_keyword_matches.append(f"[ëŒ€í™”] {msg['content']}")
        elif msg['role'] == 'jarvis' and keyword in msg['content'].lower():
            chat_keyword_matches.append(f"[ëŒ€í™”] {msg['content']}")
    # ëŒ€í™” ì˜ë¯¸ ê¸°ë°˜(ìµœê·¼ 10ê°œ)
    chat_docs = [m['content'] for m in st.session_state.get('chat_history', []) if m['role'] in ['user','jarvis']]
    if chat_docs:
        vectorizer = TfidfVectorizer().fit(chat_docs + [user_input])
        doc_vecs = vectorizer.transform(chat_docs)
        query_vec = vectorizer.transform([user_input])
        sims = cosine_similarity(query_vec, doc_vecs)[0]
        top_idx = np.argsort(sims)[::-1][:3]
        chat_semantic_matches = [chat_docs[i] for i in top_idx if sims[i] > 0.1]
    return {
        'db_keyword': db_keyword_matches,
        'db_semantic': db_semantic_matches,
        'chat_keyword': chat_keyword_matches,
        'chat_semantic': chat_semantic_matches,
        'file_keyword': file_keyword_matches,
        'file_semantic': file_semantic_matches,
        'web_keyword': web_keyword_matches,
        'web_semantic': web_semantic_matches,
    }

def search_all_db_tables_for_rag(user_input, max_rows=100, topn=2):
    from sqlalchemy import create_engine
    import pandas as pd
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import os
    import json
    engine = create_engine(
        f"mysql+mysqlconnector://{os.getenv('SQL_USER')}:{os.getenv('SQL_PASSWORD')}@{os.getenv('SQL_HOST')}/{os.getenv('SQL_DATABASE_NEWBIZ')}?charset=utf8mb4"
    )
    tables = get_all_table_names()
    rag_results = []
    for table in tables:
        try:
            df = pd.read_sql(f"SELECT * FROM {table} LIMIT {max_rows}", engine)
            if df.empty:
                continue
            docs = df.astype(str).apply(lambda row: ' '.join(row), axis=1).tolist()
            vectorizer = TfidfVectorizer().fit(docs + [user_input])
            doc_vecs = vectorizer.transform(docs)
            query_vec = vectorizer.transform([user_input])
            sims = cosine_similarity(query_vec, doc_vecs)[0]
            top_idx = np.argsort(sims)[::-1][:topn]
            for i in top_idx:
                if sims[i] > 0.1:
                    # êµ¬ì¡°ì ìœ¼ë¡œ ì»¬ëŸ¼ëª…:ê°’ ë”•ì…”ë„ˆë¦¬ë¡œ ì²¨ë¶€
                    row_dict = df.iloc[i].to_dict()
                    rag_results.append({
                        'table': table,
                        'row': row_dict,
                        'similarity': float(sims[i])
                    })
        except Exception:
            continue
    return rag_results

def get_meeting_titles(limit=10):
    import mysql.connector
    conn = mysql.connector.connect(
        host=os.getenv('SQL_HOST'),
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4'
    )
    cursor = conn.cursor()
    cursor.execute("SELECT title FROM meeting_records ORDER BY created_at DESC LIMIT %s", (limit,))
    titles = [row[0] for row in cursor.fetchall()]
    cursor.close()
    conn.close()
    return titles

def get_meeting_record_by_title(title):
    import mysql.connector
    conn = mysql.connector.connect(
        host=os.getenv('SQL_HOST'),
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4'
    )
    cursor = conn.cursor(dictionary=True)
    cursor.execute("SELECT * FROM meeting_records WHERE title = %s LIMIT 1", (title,))
    row = cursor.fetchone()
    cursor.close()
    conn.close()
    return row

def get_meeting_records_by_keyword(keyword, limit=10):
    import mysql.connector
    import re
    # ì •ê·œí™”: ì†Œë¬¸ì, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì(-,_) ì œê±°
    keyword_norm = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', keyword).lower()
    conn = mysql.connector.connect(
        host=os.getenv('SQL_HOST'),
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4'
    )
    cursor = conn.cursor(dictionary=True)
    # IFNULLë¡œ NULL ì•ˆì „í•˜ê²Œ, ê³µë°±/í•˜ì´í”ˆ/ì–¸ë”ìŠ¤ì½”ì–´ ëª¨ë‘ ì œê±°
    try:
        query = """
            SELECT * FROM meeting_records
            WHERE REPLACE(REPLACE(REPLACE(LOWER(IFNULL(title, '')), ' ', ''), '-', ''), '_', '') LIKE %s
               OR REPLACE(REPLACE(REPLACE(LOWER(IFNULL(summary, '')), ' ', ''), '-', ''), '_', '') LIKE %s
               OR REPLACE(REPLACE(REPLACE(LOWER(IFNULL(full_text, '')), ' ', ''), '-', ''), '_', '') LIKE %s
            LIMIT %s
        """
        like_pattern = f'%{keyword_norm}%'
        cursor.execute(query, (like_pattern, like_pattern, like_pattern, limit))
        rows = cursor.fetchall()
    except Exception as e:
        rows = []
    cursor.close()
    conn.close()
    return rows

# --- ëª¨ë“  í…Œì´ë¸”ì˜ ì»¬ëŸ¼ ì •ë³´ì™€ ìƒ˜í”Œ rowë¥¼ DataFrame ê¸°ë°˜ìœ¼ë¡œ ë°˜í™˜ ---
def get_all_table_columns_and_samples(sample_n=2, keyword=None):
    import mysql.connector
    conn = mysql.connector.connect(
        host=os.getenv('SQL_HOST'),
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4'
    )
    cursor = conn.cursor()
    cursor.execute("SHOW TABLES")
    tables = [row[0] for row in cursor.fetchall()]
    all_info = {}
    for table in tables:
        cursor.execute(f"SHOW COLUMNS FROM {table}")
        columns = [row[0] for row in cursor.fetchall()]
        rows = []
        col_names = []
        # 1. keywordê°€ ìˆìœ¼ë©´, keywordê°€ í¬í•¨ë˜ê³  summary/full_text/action_itemsê°€ ì±„ì›Œì§„ row ìš°ì„ 
        if keyword:
            try:
                cursor.execute(
                    f"SELECT * FROM {table} WHERE "
                    f"(title LIKE %s OR summary LIKE %s OR full_text LIKE %s) AND "
                    f"((summary IS NOT NULL AND summary != '') OR (full_text IS NOT NULL AND full_text != '') OR (action_items IS NOT NULL AND action_items != '')) "
                    f"LIMIT {sample_n}",
                    (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%')
                )
                rows = cursor.fetchall()
                col_names = [desc[0] for desc in cursor.description]
            except Exception:
                rows = []
        # 2. keywordê°€ ìˆìœ¼ë©´, keywordê°€ í¬í•¨ëœ row
        if not rows and keyword:
            try:
                cursor.execute(
                    f"SELECT * FROM {table} WHERE "
                    f"(title LIKE %s OR summary LIKE %s OR full_text LIKE %s) "
                    f"LIMIT {sample_n}",
                    (f'%{keyword}%', f'%{keyword}%', f'%{keyword}%')
                )
                rows = cursor.fetchall()
                col_names = [desc[0] for desc in cursor.description]
            except Exception:
                rows = []
        # 3. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ê·¸ëƒ¥ LIMIT 2
        if not rows:
            try:
                cursor.execute(f"SELECT * FROM {table} LIMIT {sample_n}")
                rows = cursor.fetchall()
                col_names = [desc[0] for desc in cursor.description]
            except Exception:
                rows = []
                col_names = []
        sample_rows = [dict(zip(col_names, row)) for row in rows]
        all_info[table] = {
            "columns": columns,
            "sample_rows": sample_rows
        }
    cursor.close()
    conn.close()
    return all_info

# --- JARVIS í”„ë¡¬í”„íŠ¸ ìƒì„± ---
def build_jarvis_prompt(rag, user_input):
    prompt = ''
    # 0. ì§ì „ JARVIS ë‹µë³€ ì²¨ë¶€ (ì„¸ì…˜ ìš°ì„ , ì—†ìœ¼ë©´ DB)
    last_jarvis = None
    for msg in reversed(st.session_state.get('chat_history', [])):
        if msg['role'] == 'jarvis':
            last_jarvis = msg['content']
            break
    if not last_jarvis:
        rows = load_conversation_history(user_id, limit=3)
        for r in rows:
            if r.get('jarvis_response'):
                last_jarvis = r['jarvis_response']
                break
    if last_jarvis:
        prompt += '[ì§ì „ JARVIS ë‹µë³€]\n' + last_jarvis + '\n\n'
    # --- ì›¹í¬ë¡¤ë§ ìš”ì•½ ìš”ì²­ ì‹œ ì›¹ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸ ì „ì²´ ì²¨ë¶€ ---
    if any(k in user_input for k in ['ì›¹í¬ë¡¤ë§', 'í¬ë¡¤ë§', 'ì›¹ì‚¬ì´íŠ¸ ìš”ì•½', 'ì›¹ì‚¬ì´íŠ¸ ë‚´ìš©']):
        website_texts = st.session_state.get('website_texts', [])
        if website_texts:
            joined = '\n\n'.join(website_texts)
            prompt += '[ì›¹í¬ë¡¤ë§ í…ìŠ¤íŠ¸ ì „ì²´]\n' + joined[:3000] + '\n\n'
    # 1. jarvis_interactions(ëŒ€í™” ì´ë ¥) ê¸°ë°˜ ì •ë³´ ìµœìš°ì„  ì²¨ë¶€
    if rag['db_keyword']:
        prompt += '[DBì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­ëœ ì •ë³´]\n' + '\n'.join(rag['db_keyword']) + '\n\n'
    if rag['chat_keyword']:
        prompt += '[ëŒ€í™”ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­]\n' + '\n'.join(rag['chat_keyword']) + '\n\n'
    if rag['db_semantic']:
        prompt += '[DB ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ì •ë³´]\n' + '\n'.join(rag['db_semantic']) + '\n\n'
    if rag['chat_semantic']:
        prompt += '[ëŒ€í™” ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ì •ë³´]\n' + '\n'.join(rag['chat_semantic']) + '\n\n'
    # 2. íŒŒì¼/ì›¹ ê¸°ë°˜ ì •ë³´
    if rag['file_keyword']:
        prompt += '[íŒŒì¼ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­]\n' + '\n'.join(rag['file_keyword']) + '\n\n'
    if rag['web_keyword']:
        prompt += '[ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ í‚¤ì›Œë“œ ë§¤ì¹­]\n' + '\n'.join(rag['web_keyword']) + '\n\n'
    if rag['file_semantic']:
        prompt += '[íŒŒì¼ ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ì •ë³´]\n' + '\n'.join(rag['file_semantic']) + '\n\n'
    if rag['web_semantic']:
        prompt += '[ì›¹ì‚¬ì´íŠ¸ ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ì •ë³´]\n' + '\n'.join(rag['web_semantic']) + '\n\n'
    # 3. DB ì „ì²´ í…Œì´ë¸” ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ ì •ë³´ (ë§¨ ë§ˆì§€ë§‰ì—ë§Œ ì²¨ë¶€)
    all_db_rag = search_all_db_tables_for_rag(user_input)
    # --- ì‹¤ì œ meeting_records.title ì¶”ì¶œ ---
    meeting_titles = []
    meeting_detail = None
    meeting_multi_detail = []
    # title í›„ë³´ ì¶”ì¶œ(ì§ˆë¬¸ì— í¬í•¨ëœ titleì´ ìˆìœ¼ë©´)
    if 'meeting_records' in user_input.lower():
        try:
            meeting_titles = get_meeting_titles(10)
        except Exception as e:
            meeting_titles = [f'ì˜¤ë¥˜: {e}']
        # ì§ˆë¬¸ì— title í›„ë³´ê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìƒì„¸ ì •ë³´ë„ ì¶”ì¶œ
        for t in meeting_titles:
            if t in user_input:
                try:
                    meeting_detail = get_meeting_record_by_title(t)
                except Exception as e:
                    meeting_detail = {'ì˜¤ë¥˜': str(e)}
                break
        # --- ë‹¤ì¤‘ row: í‚¤ì›Œë“œê°€ í¬í•¨ëœ ëª¨ë“  row ì¶”ì¶œ ---
        # (ì˜ˆ: 'ë…ì„œí† ë¡ ' ë“±)
        import re
        m = re.search(r'meeting_records.*?(\w+)', user_input.lower())
        keyword = None
        if m:
            # meeting_records ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë¥¼ í‚¤ì›Œë“œë¡œ ì‚¬ìš©
            keyword = m.group(1)
        # ë˜ëŠ”, "~ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ëœ" íŒ¨í„´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        m2 = re.search(r'([\wê°€-í£]+)[\s,]*[ì´ë¼ëŠ”|ë¼ëŠ”|ê°€ í¬í•¨ëœ|í¬í•¨ëœ]', user_input)
        if m2:
            keyword = m2.group(1)
        if keyword:
            # ì •ê·œí™”: ì†Œë¬¸ì, ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°
            keyword_norm = re.sub(r'[^ê°€-í£a-zA-Z0-9]', '', keyword).lower()
            try:
                meeting_multi_detail = get_meeting_records_by_keyword(keyword_norm, limit=10)
            except Exception as e:
                meeting_multi_detail = [{'ì˜¤ë¥˜': str(e)}]
    # í”„ë¡¬í”„íŠ¸ì— ì‹¤ì œ meeting_records.title ì²¨ë¶€
    if meeting_titles:
        prompt += '[ì‹¤ì œ meeting_records.title]\n' + '\n'.join(meeting_titles) + '\n\n'
    # í”„ë¡¬í”„íŠ¸ì— meeting_records ìƒì„¸ ì •ë³´ ì²¨ë¶€
    if meeting_detail:
        prompt += '[meeting_records ìƒì„¸ ì •ë³´]\n'
        for k, v in meeting_detail.items():
            prompt += f'{k}: {v}\n'
        prompt += '\n'
    # í”„ë¡¬í”„íŠ¸ì— meeting_records ë‹¤ì¤‘ row ìƒì„¸ ì •ë³´ ì²¨ë¶€
    if meeting_multi_detail:
        prompt += '[meeting_records ë‹¤ì¤‘ row ìƒì„¸ ì •ë³´]\n'
        for idx, row in enumerate(meeting_multi_detail, 1):
            prompt += f'- row {idx}:\n'
            for k, v in row.items():
                if k in ['summary', 'full_text'] and (not v or str(v).strip() == ''):
                    v = 'ë‚´ìš© ì—†ìŒ'
                prompt += f'    {k}: {v}\n'
            prompt += '\n'
        prompt += (
            'ìœ„ì˜ ê° rowì— ëŒ€í•´ title, summary, full_text, action_items ë“± ëª¨ë“  ì»¬ëŸ¼ì„ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ê°ê°ì˜ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. '
            'íŠ¹íˆ summary, full_text, action_itemsì— ì‹¤ì œ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. '
            'titleë§Œ ë‚˜ì—´í•˜ì§€ ë§ê³ , ê° rowì˜ ì‹¤ì œ ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. '
            'ì•„ë˜ì˜ í‘œ/JSON êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì°¸ê³ í•˜ì„¸ìš”. titleë§Œ ìš”ì•½í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ëª¨ë“  ì»¬ëŸ¼ì„ ë°˜ì˜í•˜ì„¸ìš”.\n'
        )
    # í”„ë¡¬í”„íŠ¸ì— DB ì „ì²´ í…Œì´ë¸” ìœ ì‚¬ ì •ë³´(êµ¬ì¡°ì ) ì²¨ë¶€
    if all_db_rag:
        already = set(prompt)
        filtered = [x for x in all_db_rag if str(x) not in already]
        if filtered:
            prompt += '[DB ì „ì²´ í…Œì´ë¸” ìœ ì‚¬ ì •ë³´]\n'
            for item in filtered:
                prompt += f'- [í…Œì´ë¸”: {item["table"]}] (ìœ ì‚¬ë„: {item["similarity"]:.2f})\n'
                for k, v in item['row'].items():
                    prompt += f'    {k}: {v}\n'
                prompt += '\n'
            prompt += '(ì°¸ê³ : ìœ„ ì •ë³´ëŠ” ëŒ€í™” ì´ë ¥ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ê°€ ë¶€ì¡±í•  ë•Œë§Œ ë³´ì¡°ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.)\n\n'
    # --- ëª¨ë“  í…Œì´ë¸”ì˜ ì¹¼ëŸ¼ ì •ë³´ì™€ ìƒ˜í”Œ rowë¥¼ DataFrame ê¸°ë°˜ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€ ---
    try:
        # ì§ˆë¬¸ì—ì„œ keyword ì¶”ì¶œ (ì˜ˆ: 'ë…ì„œí† ë¡ ')
        import re
        keyword = None
        m = re.search(r'([\wê°€-í£]+)[\s,]*(?:ì´ë¼ëŠ”|ë¼ëŠ”|ê°€ í¬í•¨ëœ|í¬í•¨ëœ|ê´€ë ¨ëœ|ê´€ë ¨|ë¦¬ìŠ¤íŠ¸|ìš”ì•½|íšŒì˜|ë¯¸íŒ…)', user_input)
        if m:
            keyword = m.group(1)
        # ì¶”ê°€: meeting_recordsì—ì„œ ~, ~ì™€ ê´€ë ¨ëœ, ~ í¬í•¨ëœ ë“± ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›
        m2 = re.search(r'meeting_records.*?(\w+)', user_input.lower())
        if m2:
            keyword = m2.group(1)
        all_info = get_all_table_columns_and_samples(sample_n=2, keyword=keyword)
        prompt += '\n[DB ì „ì²´ í…Œì´ë¸” êµ¬ì¡° ë° ìƒ˜í”Œ]\n'
        for table, info in all_info.items():
            prompt += f'[{table}]\n- ì»¬ëŸ¼: {', '.join(info["columns"])}\n- ìƒ˜í”Œ ë°ì´í„°:\n'
            for row in info["sample_rows"]:
                import json
                prompt += f'    {json.dumps(row, ensure_ascii=False)}\n'
        prompt += (
            '\në°˜ë“œì‹œ ìœ„ì˜ ì»¬ëŸ¼ëª…ê³¼ ìƒ˜í”Œ rowë§Œ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. ì„ì˜ì˜ ì»¬ëŸ¼ëª…ì„ ìƒìƒí•˜ì§€ ë§ê³ , ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\n'
            'titleë§Œ ìš”ì•½í•˜ì§€ ë§ê³  summary, full_text, action_items ë“± ëª¨ë“  ì»¬ëŸ¼ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì„¸ìš”.\n'
            'ì•„ë˜ì˜ í‘œ/JSON êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì°¸ê³ í•˜ì„¸ìš”. titleë§Œ ìš”ì•½í•˜ì§€ ë§ê³  ë°˜ë“œì‹œ ëª¨ë“  ì»¬ëŸ¼ì„ ë°˜ì˜í•˜ì„¸ìš”.\n'
            'ìƒ˜í”Œ row ì™¸ì—ë„ ì‹¤ì œ DBì— ê°’ì´ ìˆëŠ” rowê°€ ë” ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ë°˜ë“œì‹œ ëª¨ë“  rowë¥¼ ìš”ì•½í•˜ë¼. summary/full_text/action_itemsê°€ ë¹„ì–´ìˆì§€ ì•Šì€ rowê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê·¸ ë‚´ìš©ì„ ë°˜ì˜í•˜ë¼.\n'
        )
    except Exception as e:
        prompt += f"[DB ì „ì²´ í…Œì´ë¸” êµ¬ì¡°/ìƒ˜í”Œ ì •ë³´ ì˜¤ë¥˜: {e}]\n"
    # 4. ì‹¤ì œ ì§ˆë¬¸ ë° ì•ˆë‚´
    prompt += (
        'ìƒê¸° ì •ë³´ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•˜ë”ë¼ë„, JARVISì˜ ì „ë¬¸ ì§€ì‹ê³¼ ë…¼ë¦¬ì  ì¶”ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n\n'
        f'{user_input}'
    )
    return prompt

# --- JARVIS ë‹µë³€ ìƒì„± ---
def get_jarvis_answer(prompt, use_langchain, selected_model, selected_ai_api, files=None, speaker=None):
    answer = ''
    # 1. LLM í˜¸ì¶œ (Claude/OpenAI)
    try:
        if selected_ai_api == 'OpenAI (GPT)':
            llm = ChatOpenAI(model=selected_model or "gpt-4o", openai_api_key=os.getenv('OPENAI_API_KEY'), max_tokens=4096)
        elif selected_ai_api == 'Anthropic (Claude)':
            llm = ChatAnthropic(model=selected_model or "claude-3-7-sonnet-latest", api_key=os.getenv('ANTHROPIC_API_KEY'), max_tokens=4096)
        else:
            llm = ChatOpenAI(model="gpt-4o", openai_api_key=os.getenv('OPENAI_API_KEY'), max_tokens=4096)
        response = llm.invoke(prompt)
        answer = response.content if hasattr(response, 'content') else str(response)
    except Exception as e:
        answer = ''
    # 2. LangChain ë©€í‹°íˆ´ Agent fallback (ì›¹ê²€ìƒ‰, ê³„ì‚° ë“±)
    if (not answer or not str(answer).strip()) and use_langchain:
        # Firecrawl ì›¹ê²€ìƒ‰ íˆ´ ë“± ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ
        import requests
        def firecrawl_search(query: str) -> str:
            api_key = os.getenv('FIRECRAWL_API_KEY')
            url = "https://api.firecrawl.dev/v1/search"
            headers = {"Authorization": f"Bearer {api_key}"}
            params = {"q": query, "num_results": 5}
            try:
                response = requests.get(url, headers=headers, params=params, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    results = data.get("results", [])
                    if not results:
                        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
                    return "\n\n".join([f"{r['title']}\n{r['url']}\n{r.get('snippet','')}" for r in results])
                else:
                    return f"Firecrawl ê²€ìƒ‰ ì˜¤ë¥˜: {response.status_code} {response.text}"
            except Exception as e:
                return f"Firecrawl ê²€ìƒ‰ ì˜ˆì™¸: {e}"
        firecrawl_tool = Tool(
            name="Firecrawl Web Search",
            func=firecrawl_search,
            description="ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ ì‚¬ìš© (Firecrawl API ê¸°ë°˜)"
        )
        wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
        calc_tool = PythonREPLTool()
        tools = [
            firecrawl_tool,
            Tool(name="Wikipedia", func=wiki_tool.run, description="ë°±ê³¼ì‚¬ì „ ì •ë³´ê°€ í•„ìš”í•  ë•Œ ì‚¬ìš©"),
            Tool(name="Calculator", func=calc_tool.run, description="ìˆ˜ì‹ ê³„ì‚°, ë°ì´í„° ë¶„ì„, íŒŒì´ì¬ ì½”ë“œ ì‹¤í–‰"),
        ]
        if files:
            for f in files:
                if 'preview' in f and f['preview']:
                    tools.append(Tool(name=f"Summary of {f['filename']}", func=lambda x, t=f['preview']: t[:500], description=f"ì—…ë¡œë“œ íŒŒì¼ {f['filename']} ìš”ì•½"))
        if selected_ai_api == 'OpenAI (GPT)':
            llm_agent = ChatOpenAI(model=selected_model or "gpt-4o", openai_api_key=os.getenv('OPENAI_API_KEY'), max_tokens=4096)
        elif selected_ai_api == 'Anthropic (Claude)':
            llm_agent = ChatAnthropic(model=selected_model or "claude-3-7-sonnet-latest", api_key=os.getenv('ANTHROPIC_API_KEY'), max_tokens=4096)
        else:
            llm_agent = ChatOpenAI(model="gpt-4o", openai_api_key=os.getenv('OPENAI_API_KEY'), max_tokens=4096)
        agent = initialize_agent(tools, llm_agent, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False)
        try:
            answer = agent.run(prompt)
        except Exception as e:
            answer = ''
    # 3. ê¸°ë³¸ ì•ˆë‚´ ë©”ì‹œì§€
    if not answer or not str(answer).strip():
        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    # --- í™”ì ì²« ëŒ€í™” ì‹œ ì¸ì‚¬ ---
    if speaker:
        import mysql.connector
        try:
            conn = mysql.connector.connect(
                host=os.getenv('SQL_HOST'),
                user=os.getenv('SQL_USER'),
                password=os.getenv('SQL_PASSWORD'),
                database=os.getenv('SQL_DATABASE_NEWBIZ'),
                charset='utf8mb4'
            )
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM jarvis_interactions WHERE speaker = %s", (speaker,))
            count = cursor.fetchone()[0]
            cursor.close()
            conn.close()
            if count == 0:
                answer = f"JARVIS: ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤, {speaker}. ì•ìœ¼ë¡œ ì˜ ë¶€íƒë“œë¦½ë‹ˆë‹¤.\n" + answer
        except Exception as e:
            pass
    # í•­ìƒ JARVIS: prefix, ì¤‘ë³µ ë°©ì§€ (ëŒ€ì†Œë¬¸ì, ê³µë°± ë¬´ì‹œ)
    answer = answer.strip()
    # Remove double prefix if present
    answer = re.sub(r'^(\s*jarvis:)(\s*jarvis:)+', r'JARVIS:', answer, flags=re.IGNORECASE)
    # Remove any accidental 'JARVIS: JARVIS:'
    answer = re.sub(r'^(\s*jarvis:)\s*jarvis:', r'JARVIS:', answer, flags=re.IGNORECASE)
    # Add prefix if missing
    if not re.match(r'^\s*jarvis:', answer, re.IGNORECASE):
        answer = "JARVIS: " + answer
    return answer

# --- ë§ˆì§€ë§‰ ìš”ì²­ ì§ˆì˜ ê°ì§€ í•¨ìˆ˜ ---
def is_last_request_query(user_input):
    keywords = [
        'ì´ì „ ëŒ€í™”', 'ìµœê·¼ ê¸°ë¡', 'ì§€ë‚œ íšŒì˜', 'ëŒ€í™” íˆìŠ¤í† ë¦¬', 'ìµœê·¼ ì§ˆë¬¸', 'ìµœê·¼ ë‹µë³€',
        'ëŒ€í™” ë‚´ì—­', 'ê³¼ê±° ëŒ€í™”', 'ë§ˆì§€ë§‰ ëŒ€í™”', 'ìµœê·¼ ëŒ€í™”', 'ê¸°ë¡ ë³´ì—¬ì¤˜', 'ê¸°ë¡ ì•Œë ¤ì¤˜',
        'ëŒ€í™” ë³´ì—¬ì¤˜', 'ëŒ€í™” ì•Œë ¤ì¤˜', 'íˆìŠ¤í† ë¦¬', 'history', 'log', 'meeting', 'ìš”ì•½'
    ]
    return any(k in user_input for k in keywords)

def get_last_meaningful_request(user_id, n=5):
    rows = load_conversation_history(user_id, limit=n)
    if not rows:
        return "ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."
    history_lines = []
    for r in rows:
        history_lines.append(f"[{r['created_at']}] ì§ˆë¬¸: {r['user_input']}\në‹µë³€: {r['jarvis_response']}")
    return '\n\n'.join(history_lines)

# --- LangChain ë©€í‹°íˆ´/ê¸°ì¡´ ë¶„ì„ ì—ì´ì „íŠ¸ í•¨ìˆ˜ ---
def analyze_with_agents(user_input, files, audio, user_history, agent_tools, use_langchain=False, selected_model=None, selected_ai_api=None):
    results = {}
    logs = []
    # ... (í•¨ìˆ˜ ë³¸ë¬¸ì€ ê¸°ì¡´ëŒ€ë¡œ ìœ ì§€, ëª¨ë“  returnì—ì„œ results, logs ë°˜í™˜)
    # ...
    return results, logs

# --- 4. ì‚¬ìš©ì/ì„¸ì…˜ ì •ë³´ ---
user_id = "user1"  # ì‹¤ì œ êµ¬í˜„ì‹œ ë¡œê·¸ì¸/ì„¸ì…˜ ê¸°ë°˜
user_history = load_user_history(user_id)

# --- ë©”ì¸ ëŒ€í™”/ë¶„ì„ ì»¨í…Œì´ë„ˆ (ì…ë ¥+ê²°ê³¼ í•œ í™”ë©´) ---
main_area = st.container()
with main_area:
    # --- ì›¹ì‚¬ì´íŠ¸ RAG: Streamlit UI ë° ì„¸ì…˜ ìƒíƒœ ---
    if 'website_texts' not in st.session_state:
        st.session_state['website_texts'] = []
    with st.expander('ğŸŒ ì›¹ì‚¬ì´íŠ¸ RAG ë°ì´í„° ì¶”ê°€', expanded=False):
        website_url = st.text_input('ì›¹ì‚¬ì´íŠ¸ URL ì…ë ¥', key='website_url')
        max_pages = st.slider('ìµœëŒ€ í¬ë¡¤ë§ í˜ì´ì§€ ìˆ˜', 1, 10, 3, key='website_max_pages')
        if st.button('ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§', key='website_crawl_btn') and website_url:
            with st.spinner('ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...'):
                website_texts = scrape_website_simple(website_url, max_pages)
                if website_texts:
                    st.session_state['website_texts'] = [w['content'] for w in website_texts]
                    st.success(f"{len(website_texts)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
                else:
                    st.warning('í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.')
        if st.session_state['website_texts']:
            st.info(f"í˜„ì¬ {len(st.session_state['website_texts'])}ê°œ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ê°€ RAGì— í¬í•¨ë©ë‹ˆë‹¤.")

    with st.form(key="jarvis_input_form"):
        st.markdown("#### ë©”ì‹œì§€ ì…ë ¥ (JARVIS)")
        col1, col2 = st.columns([3,1])
        with col1:
            user_input = st.text_area("í…ìŠ¤íŠ¸/ëª…ë ¹/ì§ˆë¬¸ ì…ë ¥", key="user_input")
        with col2:
            audio_file = st.file_uploader("ìŒì„± ì—…ë¡œë“œ(mp3/wav)", type=["mp3","wav"], key="audio_input")
        file_uploads = st.file_uploader("ë¬¸ì„œ/ì´ë¯¸ì§€/íŒŒì¼ ì—…ë¡œë“œ", type=["pdf","docx","xlsx","pptx","png","jpg","jpeg","txt","md"], accept_multiple_files=True, key="file_input")
        agent_tools = {"ëŒ€í™”ì—ì´ì „íŠ¸": ["ê¸°ë³¸ëŒ€í™”"], "ë¬¸ì„œë¶„ì„ì—ì´ì „íŠ¸": ["ë¬¸ì„œë¶„ì„"], "ìŒì„±ì—ì´ì „íŠ¸": ["ìŒì„±ë¶„ì„"], "ë³´ê³ ì—ì´ì „íŠ¸": ["ë³´ê³ ì„œ"]}
        use_langchain = st.checkbox("LangChain ì‚¬ìš©", value=False)
        submit_btn = st.form_submit_button("JARVISì—ê²Œ ìš”ì²­")

    if submit_btn:
        try:
            file_previews = parse_uploaded_files(file_uploads) if file_uploads else []
            audio_data = audio_file.read() if audio_file else None
            website_texts = st.session_state.get('website_texts', [])
            rag = extract_rag_sources(user_id, user_input, file_previews, website_texts)
            prompt = build_jarvis_prompt(rag, user_input)
            answer = get_jarvis_answer(prompt, use_langchain, st.session_state.selected_model, st.session_state.selected_ai_api, files=file_previews, speaker=selected_speaker)
            st.session_state['chat_history'].append({'role': 'user', 'content': user_input, 'speaker': selected_speaker})
            st.session_state['chat_history'].append({'role': 'jarvis', 'content': answer, 'speaker': selected_speaker})
            st.session_state.jarvis_last_results = {'ëŒ€í™”ì—ì´ì „íŠ¸': answer}
            st.session_state.jarvis_last_logs = [prompt]
            st.session_state.jarvis_last_input = user_input
            st.session_state.jarvis_last_files = file_previews
            st.session_state.jarvis_last_audio = audio_data
        except Exception as e:
            st.error(f"[JARVIS ì˜¤ë¥˜] {e}")
            import traceback
            st.error(traceback.format_exc())

    # ëˆ„ì  ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ChatGPT/Claude ìŠ¤íƒ€ì¼)
    if st.session_state.get('chat_history'):
        st.markdown('---')
        for msg in st.session_state['chat_history']:
            if msg['role'] == 'user':
                st.markdown(f"**ğŸ™‹â€â™‚ï¸ {msg.get('speaker', selected_speaker)}:** {msg['content']}")
            else:
                answer_html = msg['content'].replace('\n', '<br>') if isinstance(msg['content'], str) else msg['content']
                st.markdown(f"**ğŸ¤– JARVIS:** {answer_html}", unsafe_allow_html=True)
    # ì°¸ê³  ë°ì´í„°(References) í‘œê¸° (ë§ˆì§€ë§‰ JARVIS ë‹µë³€ ê¸°ì¤€)
    if st.session_state.get('jarvis_last_results'):
        references = []
        if st.session_state.get('jarvis_last_logs'):
            if any('DB í…Œì´ë¸”' in log or 'newbiz DB' in log for log in st.session_state['jarvis_last_logs']):
                references.append('DB(ëŒ€í™”/í…Œì´ë¸”)')
        if st.session_state.get('website_texts'):
            if len(st.session_state['website_texts']) > 0:
                references.append('ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§')
        if references:
            st.markdown('---')
            st.markdown('**ì°¸ê³  ë°ì´í„°(References):** ' + ', '.join(references))
        save_interaction(user_id, selected_speaker, {"input": st.session_state.jarvis_last_input, "results": st.session_state.jarvis_last_results, "logs": st.session_state.jarvis_last_logs}, files=st.session_state.jarvis_last_files, audio=st.session_state.jarvis_last_audio)

# --- 9. íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì—…ë¡œë“œì‹œ) ---
if 'jarvis_last_files' in st.session_state and st.session_state.jarvis_last_files:
    st.markdown("#### ì—…ë¡œë“œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìµœì‹  ìš”ì²­ ê¸°ì¤€)")
    for f in st.session_state.jarvis_last_files:
        st.write(f["filename"])
        st.write(f["preview"][:500])

def download_history_as_csv(history_rows):
    import pandas as pd
    df = pd.DataFrame(history_rows)
    if not df.empty:
        df = df.rename(columns={
            'created_at': 'ì‹œê°„',
            'user_input': 'ì§ˆë¬¸',
            'jarvis_response': 'ë‹µë³€',
            'files_json': 'íŒŒì¼',
            'logs_json': 'ë¡œê·¸',
        })
        return df.to_csv(index=False).encode('utf-8-sig')
    return b''

# --- í•˜ë‹¨: ëŒ€í™” íˆìŠ¤í† ë¦¬/ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ UI ---
with st.expander('ğŸ’¬ JARVIS ëŒ€í™” íˆìŠ¤í† ë¦¬/ê²€ìƒ‰/ë‹¤ìš´ë¡œë“œ', expanded=False):
    selected_speaker = st.session_state['selected_speaker']
    search_kw = st.text_input('í‚¤ì›Œë“œë¡œ ëŒ€í™” ê²€ìƒ‰', key='history_search_kw')
    if 'user_id' in locals():
        hist_rows = load_conversation_history(user_id, search_kw)
        # speaker ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ í•„í„°ë§
        hist_rows = [r for r in hist_rows if r.get('speaker') == selected_speaker]
        if hist_rows:
            st.write(f"ì´ {len(hist_rows)}ê±´ì˜ ëŒ€í™” ê¸°ë¡ (í™”ì: {selected_speaker})")
            st.dataframe([
                {
                    'ì‹œê°„': r['created_at'],
                    'ì§ˆë¬¸': r['user_input'],
                    'ë‹µë³€': r['jarvis_response'],
                    'íŒŒì¼': r['files_json'],
                    'ë¡œê·¸': r['logs_json'],
                } for r in hist_rows
            ], hide_index=True)
            csv_bytes = download_history_as_csv(hist_rows)
            st.download_button('CSVë¡œ ë‹¤ìš´ë¡œë“œ', data=csv_bytes, file_name=f'jarvis_history_{selected_speaker}.csv', mime='text/csv')
            # --- í™”ìë³„ ëŒ€í™” ê¸°ë¡ ì „ì²´ ì‚­ì œ ë²„íŠ¼ ---
            if st.button(f'âš ï¸ {selected_speaker}ì˜ ëŒ€í™” ê¸°ë¡ ì „ì²´ ì‚­ì œ', key='delete_speaker_history'):
                try:
                    conn = mysql.connector.connect(
                        host=os.getenv('SQL_HOST'),
                        user=os.getenv('SQL_USER'),
                        password=os.getenv('SQL_PASSWORD'),
                        database=os.getenv('SQL_DATABASE_NEWBIZ'),
                        charset='utf8mb4'
                    )
                    cursor = conn.cursor()
                    cursor.execute("DELETE FROM jarvis_interactions WHERE speaker = %s", (selected_speaker,))
                    conn.commit()
                    cursor.close()
                    conn.close()
                    st.success(f'{selected_speaker}ì˜ ëŒ€í™” ê¸°ë¡ì´ ëª¨ë‘ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.')
                    st.rerun()
                except Exception as e:
                    st.error(f'ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}')
        else:
            st.info(f'{selected_speaker}ì˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.')
    else:
        st.info('ë¡œê·¸ì¸/ì„¸ì…˜ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤.') 
        