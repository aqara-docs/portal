import streamlit as st
import os
import mysql.connector
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import time
import io
import base64
from datetime import datetime
from bs4 import BeautifulSoup
import requests
import tempfile
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# AI/LLM
from openai import OpenAI
import anthropic

# íŒŒì¼ íŒŒì‹±
import PyPDF2
import docx
from pptx import Presentation
import openpyxl

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# DB ì—°ê²°
DB_CONFIG = {
    'user': os.getenv('SQL_USER'),
    'password': os.getenv('SQL_PASSWORD'),
    'host': os.getenv('SQL_HOST'),
    'database': os.getenv('SQL_DATABASE_NEWBIZ'),
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_general_ci'
}

def connect_to_db():
    return mysql.connector.connect(**DB_CONFIG)

# --- DB í…Œì´ë¸” ìƒì„±/í™•ì¥ ---
def ensure_tables():
    conn = connect_to_db()
    cursor = conn.cursor()
    # ì›¹ì‚¬ì´íŠ¸ ë¶„ì„
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS business_website_analyses (
            analysis_id INT AUTO_INCREMENT PRIMARY KEY,
            url VARCHAR(500),
            analysis_date DATETIME DEFAULT CURRENT_TIMESTAMP,
            title VARCHAR(500),
            meta_description TEXT,
            status_code INT,
            response_time DECIMAL(10,3),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    # íŒŒì¼ ì—…ë¡œë“œ
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS business_uploaded_files (
            file_id INT AUTO_INCREMENT PRIMARY KEY,
            analysis_id INT,
            filename VARCHAR(255),
            file_type VARCHAR(20),
            file_content LONGTEXT,
            file_binary_data LONGBLOB,
            file_size INT,
            uploaded_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (analysis_id) REFERENCES business_website_analyses(analysis_id) ON DELETE CASCADE
        )
    ''')
    # AI ë¶„ì„ ê²°ê³¼
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS business_analysis_reports (
            report_id INT AUTO_INCREMENT PRIMARY KEY,
            analysis_id INT,
            report_type VARCHAR(50),
            report_content LONGTEXT,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (analysis_id) REFERENCES business_website_analyses(analysis_id) ON DELETE CASCADE
        )
    ''')
    conn.commit()
    cursor.close()
    conn.close()

# --- íŒŒì¼ íŒŒì‹±/ì €ì¥/ë¯¸ë¦¬ë³´ê¸° ---
def parse_uploaded_file(uploaded_file):
    try:
        file_extension = uploaded_file.name.split('.')[-1].lower()
        content = ""
        uploaded_file.seek(0)
        binary_data = uploaded_file.read()
        uploaded_file.seek(0)
        if file_extension == 'pdf':
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                content += page.extract_text() + "\n"
        elif file_extension == 'docx':
            doc = docx.Document(uploaded_file)
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
        elif file_extension == 'pptx':
            prs = Presentation(uploaded_file)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"
        elif file_extension == 'xlsx':
            workbook = openpyxl.load_workbook(uploaded_file, data_only=True)
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                content += f"=== {sheet_name} ===\n"
                for row in sheet.iter_rows(values_only=True):
                    row_text = '\t'.join([str(cell) if cell is not None else '' for cell in row])
                    if row_text.strip():
                        content += row_text + "\n"
                content += "\n"
        elif file_extension in ['txt', 'md']:
            uploaded_file.seek(0)
            content = uploaded_file.read().decode('utf-8')
        elif file_extension in ['jpg', 'jpeg', 'png', 'gif']:
            content = f"[{file_extension.upper()} ì´ë¯¸ì§€ íŒŒì¼ - {uploaded_file.name}]"
        else:
            try:
                uploaded_file.seek(0)
                content = uploaded_file.read().decode('utf-8', errors='ignore')
                if not content.strip():
                    content = f"[{file_extension.upper()} íŒŒì¼ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€]"
            except:
                content = f"[{file_extension.upper()} íŒŒì¼ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€]"
        return {
            'filename': uploaded_file.name,
            'file_type': file_extension,
            'content': content,
            'binary_data': binary_data,
            'size': len(binary_data)
        }
    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return None

def save_uploaded_file(analysis_id, file_data):
    try:
        conn = connect_to_db()
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO business_uploaded_files
            (analysis_id, filename, file_type, file_content, file_binary_data, file_size)
            VALUES (%s, %s, %s, %s, %s, %s)
        ''', (
            analysis_id,
            file_data['filename'],
            file_data['file_type'],
            file_data['content'],
            file_data['binary_data'],
            file_data['size']
        ))
        conn.commit()
        cursor.close()
        conn.close()
        return True
    except Exception as e:
        st.error(f"íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return False

def get_uploaded_files(analysis_id):
    try:
        conn = connect_to_db()
        cursor = conn.cursor(dictionary=True)
        cursor.execute('''
            SELECT * FROM business_uploaded_files WHERE analysis_id = %s ORDER BY uploaded_at DESC
        ''', (analysis_id,))
        files = cursor.fetchall()
        cursor.close()
        conn.close()
        return files
    except Exception as e:
        st.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def get_file_binary(file_id):
    try:
        conn = connect_to_db()
        cursor = conn.cursor(dictionary=True)
        cursor.execute('''
            SELECT filename, file_type, file_binary_data, file_size FROM business_uploaded_files WHERE file_id = %s
        ''', (file_id,))
        result = cursor.fetchone()
        cursor.close()
        conn.close()
        return result
    except Exception as e:
        st.error(f"íŒŒì¼ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

def get_file_mime_type(file_type):
    mime_types = {
        'pdf': 'application/pdf',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'txt': 'text/plain',
        'md': 'text/markdown',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif'
    }
    return mime_types.get(file_type.lower(), 'application/octet-stream')

def display_file_preview(file, file_type, filename):
    try:
        import base64
        import tempfile
        import os
        from PyPDF2 import PdfReader, PdfWriter
        file_type_lower = file_type.lower()
        if file_type_lower in ['jpg', 'jpeg', 'png', 'gif']:
            st.image(file['file_binary_data'], caption=f"ğŸ–¼ï¸ {filename}", use_container_width=True)
            return True
        elif file_type_lower == 'pdf':
            if file.get('file_binary_data'):
                total_size = file.get('file_size', 0)
                if total_size > 1 * 1024 * 1024:
                    key = f"pdf_preview_page_start_{filename}"
                    if key not in st.session_state:
                        st.session_state[key] = 0
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_in:
                        tmp_in.write(file['file_binary_data'])
                        tmp_in_path = tmp_in.name
                    reader = PdfReader(tmp_in_path)
                    total_pages = len(reader.pages)
                    os.unlink(tmp_in_path)
                    page_start = st.session_state[key]
                    page_end = min(page_start + 1, total_pages)
                    col_prev, col_next = st.columns([1, 1])
                    with col_prev:
                        if st.button("â¬…ï¸ ì´ì „", disabled=page_start == 0, key=f"prev_{filename}"):
                            st.session_state[key] = max(0, page_start - 1)
                            st.rerun()
                    with col_next:
                        if st.button("ë‹¤ìŒ â¡ï¸", disabled=page_end >= total_pages, key=f"next_{filename}"):
                            st.session_state[key] = min(total_pages - 1, page_start + 1)
                            st.rerun()
                    try:
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_in:
                            tmp_in.write(file['file_binary_data'])
                            tmp_in_path = tmp_in.name
                        reader = PdfReader(tmp_in_path)
                        writer = PdfWriter()
                        for i in range(page_start, page_end):
                            writer.add_page(reader.pages[i])
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_out:
                            writer.write(tmp_out)
                            tmp_out_path = tmp_out.name
                        with open(tmp_out_path, "rb") as f:
                            preview_pdf_bytes = f.read()
                        os.unlink(tmp_in_path)
                        os.unlink(tmp_out_path)
                        st.markdown(f"**í˜ì´ì§€ {page_start+1} / {total_pages}**")
                        pdf_base64 = base64.b64encode(preview_pdf_bytes).decode('utf-8')
                        pdf_display = f"""
                        <iframe src=\"data:application/pdf;base64,{pdf_base64}\" 
                                width=\"100%\" height=\"600px\" type=\"application/pdf\">
                            <p>PDFë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
                            <a href=\"data:application/pdf;base64,{pdf_base64}\" target=\"_blank\">
                            ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì—¬ ìƒˆ íƒ­ì—ì„œ ì—´ì–´ë³´ì„¸ìš”.</a></p>
                        </iframe>
                        """
                        st.markdown(pdf_display, unsafe_allow_html=True)
                    except Exception as e:
                        st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    st.download_button(
                        label="ğŸ’¾ ì „ì²´ PDF ë‹¤ìš´ë¡œë“œ",
                        data=file['file_binary_data'],
                        file_name=filename,
                        mime="application/pdf"
                    )
                    return True
                else:
                    st.subheader(f"ğŸ“„ PDF ë¯¸ë¦¬ë³´ê¸°: {filename}")
                    pdf_base64 = base64.b64encode(file['file_binary_data']).decode('utf-8')
                    pdf_display = f"""
                    <iframe src=\"data:application/pdf;base64,{pdf_base64}\" 
                            width=\"100%\" height=\"600px\" type=\"application/pdf\">
                        <p>PDFë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
                        <a href=\"data:application/pdf;base64,{pdf_base64}\" target=\"_blank\">
                        ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì—¬ ìƒˆ íƒ­ì—ì„œ ì—´ì–´ë³´ì„¸ìš”.</a></p>
                    </iframe>
                    """
                    st.markdown(pdf_display, unsafe_allow_html=True)
                    return True
        elif file_type_lower in ['txt', 'md']:
            try:
                text_content = file['file_binary_data'].decode('utf-8')
                st.subheader(f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°: {filename}")
                if file_type_lower == 'md':
                    st.markdown(text_content)
                else:
                    st.text_area(
                        "íŒŒì¼ ë‚´ìš©",
                        value=text_content,
                        height=400,
                        disabled=True
                    )
                return True
            except Exception as e:
                st.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                return False
        elif file_type_lower == 'xlsx':
            st.info("ì—‘ì…€ ë¯¸ë¦¬ë³´ê¸°ëŠ” ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤.")
            return False
        elif file_type_lower == 'pptx':
            st.info("PPTX ë¯¸ë¦¬ë³´ê¸°ëŠ” ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤.")
            return False
        elif file_type_lower == 'docx':
            st.info("DOCX ë¯¸ë¦¬ë³´ê¸°ëŠ” ì§€ì› ì˜ˆì •ì…ë‹ˆë‹¤.")
            return False
        return False
    except Exception as e:
        st.error(f"íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {str(e)}")
        return False

# --- ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘/ë¶„ì„ ---
def scrape_website_content(url, max_pages=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        base_url = url.rstrip('/')
        internal_links = set()
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = requests.compat.urljoin(base_url, href)
            if requests.utils.urlparse(full_url).netloc == requests.utils.urlparse(url).netloc:
                clean_url = requests.utils.urlparse(full_url)._replace(query='', fragment='').geturl()
                internal_links.add(clean_url)
        links_to_scrape = list(internal_links)[:max_pages]
        all_content = {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'headings': [],
            'paragraphs': [],
            'lists': [],
            'text_content': '',
            'word_count': 0,
            'sentence_count': 0,
            'paragraph_count': 0,
            'status_code': response.status_code,
            'response_time': response.elapsed.total_seconds(),
            'scraped_pages': [],
            'total_pages': len(links_to_scrape) + 1
        }
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                all_content['meta_description'] = meta.get('content', '')
        main_page_content = extract_page_content(soup, url)
        all_content['scraped_pages'].append(main_page_content)
        st.info(f"ğŸ” {len(links_to_scrape)}ê°œì˜ ë‚´ë¶€ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        with st.spinner("ë‚´ë¶€ í˜ì´ì§€ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            for i, link_url in enumerate(links_to_scrape):
                try:
                    page_response = requests.get(link_url, headers=headers, timeout=5)
                    if page_response.status_code == 200:
                        page_soup = BeautifulSoup(page_response.content, 'html.parser')
                        page_content = extract_page_content(page_soup, link_url)
                        all_content['scraped_pages'].append(page_content)
                        progress = (i + 1) / len(links_to_scrape)
                        st.progress(progress)
                except Exception as e:
                    st.warning(f"í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {link_url} - {str(e)}")
                    continue
        all_content = merge_all_page_content(all_content)
        st.success(f"âœ… ì´ {len(all_content['scraped_pages'])}ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        return all_content
    except Exception as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_page_content(soup, page_url):
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
    page_data = {
        'url': page_url,
        'title': soup.title.string if soup.title else '',
        'headings': [],
        'paragraphs': [],
        'lists': [],
        'text_content': ''
    }
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        headings = soup.find_all(tag)
        for heading in headings:
            text = heading.get_text(strip=True)
            if text:
                page_data['headings'].append({
                    'tag': tag,
                    'text': text,
                    'level': int(tag[1])
                })
    paragraphs = soup.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:
            page_data['paragraphs'].append(text)
    lists = soup.find_all(['ul', 'ol'])
    for lst in lists:
        items = lst.find_all('li')
        list_items = []
        for item in items:
            text = item.get_text(strip=True)
            if text:
                list_items.append(text)
        if list_items:
            page_data['lists'].append({
                'type': lst.name,
                'items': list_items
            })
    text_content = soup.get_text()
    text_content = ' '.join(text_content.split())
    page_data['text_content'] = text_content
    return page_data

def merge_all_page_content(all_content):
    merged_content = all_content.copy()
    all_text = ""
    all_headings = []
    all_paragraphs = []
    all_lists = []
    for page in all_content['scraped_pages']:
        if page['text_content']:
            all_text += page['text_content'] + " "
        all_headings.extend(page['headings'])
        all_paragraphs.extend(page['paragraphs'])
        all_lists.extend(page['lists'])
    merged_content['text_content'] = all_text.strip()
    merged_content['headings'] = all_headings
    merged_content['paragraphs'] = list(set(all_paragraphs))
    merged_content['lists'] = all_lists
    merged_content['word_count'] = len(merged_content['text_content'].split())
    merged_content['sentence_count'] = len([s for s in merged_content['text_content'].split('.') if s])
    merged_content['paragraph_count'] = len(merged_content['paragraphs'])
    return merged_content

# --- AI ë¶„ì„/ë¦¬í¬íŠ¸ (ìƒ˜í”Œ: OpenAI GPT) ---
def ai_business_analysis(website_content, file_texts, model_name="gpt-4o-mini"):
    prompt = f"""
[ê¸°ì—… ì›¹ì‚¬ì´íŠ¸ ë° ì—…ë¡œë“œ íŒŒì¼ í†µí•© ë¶„ì„]

1. ì›¹ì‚¬ì´íŠ¸ ì£¼ìš” ì •ë³´ ë° êµ¬ì¡° ìš”ì•½
2. ì—…ë¡œë“œ íŒŒì¼(ë¬¸ì„œ/ë³´ê³ ì„œ ë“±) ì£¼ìš” ë‚´ìš© ìš”ì•½
3. ì›¹ì‚¬ì´íŠ¸ì™€ íŒŒì¼ì„ ì¢…í•©í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸/ì œí’ˆ/ì‹œì¥/ê²½ìŸ/ê¸°ìˆ /ì „ëµ ë¶„ì„
4. ê¸°ì—…ì˜ ê°•ì , ì•½ì , ê¸°íšŒ, ìœ„í˜‘(SWOT)
5. íˆ¬ì/ì„±ì¥/í˜ì‹ /ë¦¬ìŠ¤í¬ ê´€ì ì—ì„œì˜ ì¢…í•© í‰ê°€
6. êµ¬ì²´ì  ì‹¤í–‰ ì œì•ˆ ë° ì¸ì‚¬ì´íŠ¸

[ì›¹ì‚¬ì´íŠ¸ ìš”ì•½]
{website_content[:3000]}

[ì—…ë¡œë“œ íŒŒì¼ ìš”ì•½]
{file_texts[:3000]}

[ë¶„ì„ ë¦¬í¬íŠ¸]
- ìœ„ í•­ëª©ë³„ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ë¬´ì ì¸ ë¶„ì„ì„ í•´ì£¼ì„¸ìš”.
- í‘œ, ë¦¬ìŠ¤íŠ¸, ë§ˆí¬ë‹¤ìš´, mermaid ì°¨íŠ¸ ë“± ì‹œê°í™”ë„ í¬í•¨í•´ì£¼ì„¸ìš”.
- ì‹¤ì œ ê¸°ì—… ì»¨ì„¤íŒ… ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
    try:
        openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = openai.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=3000,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"AI ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

def get_perplexity_market_research(query, api_key=None, model="sonar-pro"):
    """Perplexity APIë¥¼ í™œìš©í•œ ì‹œì¥/ê²½ìŸì‚¬/ì œí’ˆ ì¡°ì‚¬"""
    import requests
    import os
    if api_key is None:
        api_key = os.environ.get('PERPLEXITY_API_KEY')
    if not api_key:
        return "(Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)"
    url = "https://api.perplexity.ai/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    prompt = f"""
ì•„ë˜ íšŒì‚¬/ì œí’ˆ/ì‹œì¥ì— ëŒ€í•´ ë‹¤ìŒ í•­ëª©ì„ ì¡°ì‚¬í•´ ì£¼ì„¸ìš”:
- ì‹œì¥ ë™í–¥ ë° ì„±ì¥ ì „ë§
- ì£¼ìš” ê²½ìŸì‚¬ ë° ê²½ìŸ êµ¬ë„
- ì œí’ˆ/ê¸°ìˆ  íŠ¸ë Œë“œ
- ê³ ê°/íƒ€ê²Ÿ ì‹œì¥
- SWOT ìš”ì•½
- íˆ¬ì/ì„±ì¥/ë¦¬ìŠ¤í¬ í¬ì¸íŠ¸

[ì¡°ì‚¬ ëŒ€ìƒ]
{query}

[ìš”ì²­]
- í‘œ, ë¦¬ìŠ¤íŠ¸, ë§ˆí¬ë‹¤ìš´, ê°„ê²°í•œ ìš”ì•½ í¬í•¨
- ì‹¤ì œ ì‹œì¥ ì¡°ì‚¬ ë³´ê³ ì„œ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±
"""
    data = {
        "model": model,
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 1800,
        "temperature": 0.5
    }
    try:
        resp = requests.post(url, headers=headers, json=data, timeout=60)
        if resp.status_code == 200:
            result = resp.json()
            return result['choices'][0]['message']['content']
        else:
            return f"(Perplexity API ì˜¤ë¥˜: {resp.status_code} {resp.text})"
    except Exception as e:
        return f"(Perplexity API ì˜ˆì™¸: {str(e)})"

# --- í˜ë¥´ì†Œë‚˜ ì •ì˜ (ë¶„ì•¼ë³„ ì „ë¬¸ê°€) ---
PERSONAS = {
    "ECONOMIST": {
        "name": "ê²½ì œ ì „ë¬¸ê°€",
        "emoji": "ğŸ’¹",
        "role": "ê±°ì‹œê²½ì œ, ì‚°ì—…ë™í–¥, ê²½ì œì •ì±… ë¶„ì„ ì „ë¬¸ê°€",
        "system_prompt": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ê²½ì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê±°ì‹œê²½ì œ, ì‚°ì—…ë™í–¥, ì •ì±…, ê¸€ë¡œë²Œ ì´ìŠˆ, ê²½ì œì§€í‘œ, ë¦¬ìŠ¤í¬, ê¸°íšŒ, ì‹¤í–‰ì „ëµì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
    },
    "MARKET": {
        "name": "ì‹œì¥ ì „ë¬¸ê°€",
        "emoji": "ğŸ“ˆ",
        "role": "ì‹œì¥/ê²½ìŸ/ê³ ê°/íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€",
        "system_prompt": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì‹œì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‹œì¥ê·œëª¨, ì„±ì¥ì„±, ê²½ìŸì‚¬, ê³ ê°, íŠ¸ë Œë“œ, SWOT, ì‹¤í–‰ì „ëµì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
    },
    "TECH": {
        "name": "ê¸°ìˆ  ì „ë¬¸ê°€",
        "emoji": "ğŸ› ï¸",
        "role": "ê¸°ìˆ /ì œí’ˆ/í˜ì‹ /ì‹¤í–‰ ë¶„ì„ ì „ë¬¸ê°€",
        "system_prompt": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê¸°ìˆ ë™í–¥, ì œí’ˆ, í˜ì‹ , êµ¬í˜„, ë¦¬ìŠ¤í¬, ì‹¤í–‰ê³„íšì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
    },
    "INVEST": {
        "name": "íˆ¬ì ì „ë¬¸ê°€",
        "emoji": "ğŸ’°",
        "role": "íˆ¬ì/ì¬ë¬´/ìˆ˜ìµì„±/ë¦¬ìŠ¤í¬ ë¶„ì„ ì „ë¬¸ê°€",
        "system_prompt": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íˆ¬ì, ì¬ë¬´, ìˆ˜ìµì„±, ë¦¬ìŠ¤í¬, ì„±ì¥ì„±, ì‹¤í–‰ì „ëµì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
    },
    "POLICY": {
        "name": "ì •ì±… ì „ë¬¸ê°€",
        "emoji": "ğŸ“œ",
        "role": "ì •ì±…/ê·œì œ/ESG/ì‚¬íšŒì  ì˜í–¥ ë¶„ì„ ì „ë¬¸ê°€",
        "system_prompt": "ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ì •ì±… ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •ì±…, ê·œì œ, ESG, ì‚¬íšŒì  ì˜í–¥, ë¦¬ìŠ¤í¬, ì‹¤í–‰ì „ëµì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”."
    }
}
CEO_PERSONA = {
    "name": "í†µí•© ë¶„ì„ê°€",
    "emoji": "ğŸ§‘â€ğŸ’¼",
    "role": "ìµœì¢… í†µí•© ë¶„ì„ ë° ì‹¤í–‰ ì „ëµ ì œì‹œì",
    "system_prompt": "ë‹¹ì‹ ì€ ê° ë¶„ì•¼ ì „ë¬¸ê°€ì˜ ë¶„ì„ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì‹¤í–‰ ì „ëµê³¼ ì‹œì‚¬ì ì„ ì œì‹œí•˜ëŠ” í†µí•© ë¶„ì„ê°€ì…ë‹ˆë‹¤."
}

def persona_analysis_prompt(persona_info, website_text, file_texts, perplexity_text, selected_url, all_urls):
    return f"""
[í˜ë¥´ì†Œë‚˜: {persona_info['name']}]

1. ë¶„ì„ ëŒ€ìƒ íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸): {selected_url}
2. ì…ë ¥ëœ ëª¨ë“  íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸): {', '.join(all_urls)}
3. Perplexity ì‹œì¥/ê²½ìŸì‚¬/ì œí’ˆ ì¡°ì‚¬ ê²°ê³¼:
{perplexity_text[:2000]}
4. ì—…ë¡œë“œ íŒŒì¼ ì£¼ìš” ë‚´ìš©:
{file_texts[:2000]}
5. ì›¹ì‚¬ì´íŠ¸ ì£¼ìš” ë‚´ìš©:
{website_text[:2000]}

[ë¶„ì„ ìš”ì²­]
- {persona_info['role']} ê´€ì ì—ì„œ ì „ë¬¸ì ìœ¼ë¡œ ìƒì„¸ ë¶„ì„
- í‘œ, ë¦¬ìŠ¤íŠ¸, ë§ˆí¬ë‹¤ìš´, mermaid ì°¨íŠ¸ ë“± ì‹œê°í™” í¬í•¨
- ìµœì‹  íŠ¸ë Œë“œ, ë¦¬ìŠ¤í¬, ì‹¤í–‰ê³„íš, KPI, í˜‘ì—…ë°©ì•ˆ ë“± í¬í•¨
"""

def analyze_with_persona_concurrent(args):
    persona_key, persona_info, website_text, file_texts, perplexity_text, selected_url, all_urls = args
    try:
        prompt = persona_analysis_prompt(persona_info, website_text, file_texts, perplexity_text, selected_url, all_urls)
        openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=2000,
            temperature=0.7
        )
        return persona_key, response.choices[0].message.content, True
    except Exception as e:
        return persona_key, f"ì˜¤ë¥˜: {str(e)}", False

def ceo_synthesis_prompt(persona_results, website_text, file_texts, perplexity_text, selected_url, all_urls):
    summary = "\n\n".join([
        f"--- {PERSONAS[k]['emoji']} {PERSONAS[k]['name']} ---\n{v['result']}" for k, v in persona_results.items() if v['success']
    ])
    return f"""
[CEO ì¢…í•© ë¶„ì„]

1. ë¶„ì„ ëŒ€ìƒ íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸): {selected_url}
2. ì…ë ¥ëœ ëª¨ë“  íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸): {', '.join(all_urls)}
3. Perplexity ì‹œì¥/ê²½ìŸì‚¬/ì œí’ˆ ì¡°ì‚¬ ê²°ê³¼:
{perplexity_text[:2000]}
4. ì—…ë¡œë“œ íŒŒì¼ ì£¼ìš” ë‚´ìš©:
{file_texts[:2000]}
5. ì›¹ì‚¬ì´íŠ¸ ì£¼ìš” ë‚´ìš©:
{website_text[:2000]}
6. ê° C-Level ì„ì›ì§„ ë¶„ì„ ê²°ê³¼:
{summary}

[CEO ë¶„ì„ ìš”ì²­]
- ê° ì„ì›ì§„ ë¶„ì„ì„ í†µí•©í•˜ì—¬ ìµœì¢… ì˜ì‚¬ê²°ì •/ì‹¤í–‰ê³„íš/ë¦¬ìŠ¤í¬/ê¸°íšŒ/ì‹œì‚¬ì /Action Itemsë¥¼ ì œì‹œ
- í‘œ, ë¦¬ìŠ¤íŠ¸, ë§ˆí¬ë‹¤ìš´, mermaid ì°¨íŠ¸ ë“± ì‹œê°í™” í¬í•¨
"""

def analyze_ceo_synthesis(persona_results, website_text, file_texts, perplexity_text, selected_url, all_urls):
    prompt = ceo_synthesis_prompt(persona_results, website_text, file_texts, perplexity_text, selected_url, all_urls)
    try:
        openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=3000,
            temperature=0.7
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"CEO ë¶„ì„ ì˜¤ë¥˜: {str(e)}"

# --- Streamlit UI ---
def main():
    st.set_page_config(page_title="ê¸°ì—… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ í†µí•©", layout="wide")
    st.title("ğŸ¢ ê¸°ì—… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ í†µí•© ì‹œìŠ¤í…œ")
    ensure_tables()
    tab1, tab2 = st.tabs(["ìƒˆ ë¶„ì„", "ì €ì¥ëœ ë¶„ì„/íŒŒì¼ ì¡°íšŒ"])
    with tab1:
        st.header("1ï¸âƒ£ ìµœëŒ€ 3ê°œ íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸) ì •ë³´ ì…ë ¥ ë° íŒŒì¼ ì—…ë¡œë“œ")
        st.subheader("1ï¸âƒ£ ë¶„ì„í•  íšŒì‚¬(ì›¹ì‚¬ì´íŠ¸) ìµœëŒ€ 3ê°œ ì…ë ¥")
        url1 = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL 1", key="url1")
        url2 = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL 2", key="url2")
        url3 = st.text_input("ì›¹ì‚¬ì´íŠ¸ URL 3", key="url3")
        urls = [u for u in [url1, url2, url3] if u.strip()]
        if not urls:
            st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()
        st.markdown("""
        - ì—¬ëŸ¬ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì…ë ¥í•˜ë©´, ë™ì¼ íšŒì‚¬ì˜ ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ë„ í•¨ê»˜ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - ì•„ë˜ì—ì„œ ë¶„ì„ì— í¬í•¨í•  ì›¹ì‚¬ì´íŠ¸ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.
        """)
        selected_urls = st.multiselect(
            "ë¶„ì„ì— í¬í•¨í•  ì›¹ì‚¬ì´íŠ¸ë¥¼ ì„ íƒí•˜ì„¸ìš” (1ê°œ ì´ìƒ)",
            options=urls,
            default=urls
        )
        if not selected_urls:
            st.warning("ë¶„ì„í•  íšŒì‚¬ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
            st.stop()
        uploaded_files = st.file_uploader(
            "ë¶„ì„ì— í™œìš©í•  íŒŒì¼ ì—…ë¡œë“œ (PDF, DOCX, PPTX, XLSX, ì´ë¯¸ì§€, TXT, MD ë“±)",
            type=["pdf", "docx", "pptx", "xlsx", "md", "txt", "jpg", "jpeg", "png", "gif"],
            accept_multiple_files=True
        )
        # Perplexity ì¡°ì‚¬ ì‹¤í–‰
        if st.button("Perplexity ì‹œì¥/ê²½ìŸì‚¬/ì œí’ˆ ì¡°ì‚¬ ì‹¤í–‰", type="secondary"):
            if not urls:
                st.warning("ìµœì†Œ 1ê°œ ì´ìƒì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                st.stop()
            api_key = os.environ.get('PERPLEXITY_API_KEY')
            st.session_state['perplexity_results'] = {}
            for url in urls:
                with st.spinner(f"{url} ì¡°ì‚¬ ì¤‘..."):
                    result = get_perplexity_market_research(url, api_key)
                    st.session_state['perplexity_results'][url] = result
                    st.success(f"{url} ì¡°ì‚¬ ì™„ë£Œ!")
            st.info("Perplexity ì¡°ì‚¬ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ë¶„ì„ ì‹œ ìë™ í™œìš©ë©ë‹ˆë‹¤.")
        if st.button("ì›¹ì‚¬ì´íŠ¸+íŒŒì¼+ì‹œì¥ì¡°ì‚¬ í†µí•© ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
            if not selected_urls:
                st.warning("ë¶„ì„í•  íšŒì‚¬ë¥¼ 1ê°œ ì´ìƒ ì„ íƒí•˜ì„¸ìš”.")
                st.stop()
            with st.spinner("ì›¹ì‚¬ì´íŠ¸/íŒŒì¼/ì‹œì¥ì¡°ì‚¬ ë©€í‹°ì—ì´ì „íŠ¸ ë™ì‹œ ë¶„ì„ ì¤‘..."):
                # ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ì„ íƒëœ ëª¨ë“  ì›¹ì‚¬ì´íŠ¸)
                content_data_map = {}
                for url in selected_urls:
                    content_data_map[url] = scrape_website_content(url)
                # DB ì €ì¥(ì›¹ì‚¬ì´íŠ¸)
                conn = connect_to_db()
                cursor = conn.cursor()
                for url in selected_urls:
                    cursor.execute('''
                        INSERT INTO business_website_analyses
                        (url, title, meta_description, status_code, response_time)
                        VALUES (%s, %s, %s, %s, %s)
                    ''', (
                        url,
                        content_data_map[url].get('title', ''),
                        content_data_map[url].get('meta_description', ''),
                        content_data_map[url].get('status_code', 0),
                        content_data_map[url].get('response_time', 0)
                    ))
                conn.commit()
                analysis_id = cursor.lastrowid
                cursor.close()
                conn.close()
                # íŒŒì¼ ì €ì¥
                file_texts = ""
                for file in uploaded_files:
                    file_data = parse_uploaded_file(file)
                    if file_data:
                        file_texts += f"\n\n[{file_data['filename']}]:\n" + file_data['content'][:2000]
                        save_uploaded_file(analysis_id, file_data)
                # Perplexity ì¡°ì‚¬ ê²°ê³¼ (ì„ íƒëœ ëª¨ë“  ì›¹ì‚¬ì´íŠ¸)
                perplexity_results = st.session_state.get('perplexity_results', {})
                selected_perplexity = "\n\n".join([
                    f"[{url}]\n{perplexity_results.get(url, '(ì¡°ì‚¬ ê²°ê³¼ ì—†ìŒ)')}" for url in selected_urls
                ])
                # ì›¹ì‚¬ì´íŠ¸ í…ìŠ¤íŠ¸ í†µí•©
                website_text = "\n\n".join([
                    f"[{url}]\n{content_data_map[url].get('text_content', '')}" for url in selected_urls
                ])
                # ë©€í‹°ì—ì´ì „íŠ¸ ë™ì‹œ ë¶„ì„
                persona_args = [
                    (k, v, website_text, file_texts, selected_perplexity, ', '.join(selected_urls), urls)
                    for k, v in PERSONAS.items()
                ]
                persona_results = {}
                progress_bar = st.progress(0)  # í•œ ë²ˆë§Œ ìƒì„±
                completed = 0
                total = len(PERSONAS)
                with ThreadPoolExecutor(max_workers=total) as executor:
                    future_to_persona = {
                        executor.submit(analyze_with_persona_concurrent, args): args[0]
                        for args in persona_args
                    }
                    for future in as_completed(future_to_persona):
                        persona_key, result, success = future.result()
                        persona_results[persona_key] = {"result": result, "success": success}
                        completed += 1
                        progress_bar.progress(completed / total)  # í•˜ë‚˜ì˜ barë§Œ ì—…ë°ì´íŠ¸
                        st.info(f"{PERSONAS[persona_key]['emoji']} {PERSONAS[persona_key]['name']} ë¶„ì„ ì™„ë£Œ")
                st.success("ëª¨ë“  C-Level ì„ì›ì§„ ë¶„ì„ ì™„ë£Œ!")
                # ê° í˜ë¥´ì†Œë‚˜ë³„ ê²°ê³¼ í‘œì‹œ
                for k, v in persona_results.items():
                    with st.expander(f"{PERSONAS[k]['emoji']} {PERSONAS[k]['name']} ë¶„ì„ ê²°ê³¼", expanded=True):
                        st.markdown(v['result'])
                # CEO ì¢…í•© ë¶„ì„
                with st.spinner("ìµœì¢… í†µí•© ë¶„ì„ ì¤‘..."):
                    ceo_report = analyze_ceo_synthesis(persona_results, website_text, file_texts, selected_perplexity, ', '.join(selected_urls), urls)
                    st.markdown("---")
                    st.subheader("ğŸ§‘â€ğŸ’¼ ìµœì¢… í†µí•© ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.markdown(ceo_report)
                # DB ì €ì¥(ë¦¬í¬íŠ¸)
                conn = connect_to_db()
                cursor = conn.cursor()
                cursor.execute('''
                    INSERT INTO business_analysis_reports
                    (analysis_id, report_type, report_content)
                    VALUES (%s, %s, %s)
                ''', (analysis_id, "ë¶„ì•¼ë³„ ì „ë¬¸ê°€ í†µí•©ë¦¬í¬íŠ¸", ceo_report))
                conn.commit()
                cursor.close()
                conn.close()
                st.success("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±/ì €ì¥ ì™„ë£Œ!")
                # (ì—¬ê¸°ì„œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°/ë‹¤ìš´ë¡œë“œ UIëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)

    with tab2:
        st.header("2ï¸âƒ£ ì €ì¥ëœ ë¶„ì„/íŒŒì¼/ë¦¬í¬íŠ¸ ì¡°íšŒ")
        conn = connect_to_db()
        cursor = conn.cursor(dictionary=True)
        cursor.execute('''
            SELECT * FROM business_website_analyses ORDER BY analysis_date DESC
        ''')
        analyses = cursor.fetchall()
        cursor.close()
        conn.close()
        for analysis in analyses:
            with st.expander(f"{analysis['url']} ({analysis['analysis_date']})"):
                st.write(f"**ì œëª©:** {analysis['title']}")
                st.write(f"**ìƒíƒœì½”ë“œ:** {analysis['status_code']}, **ì‘ë‹µì‹œê°„:** {analysis['response_time']}s")
                # ê´€ë ¨ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°/ë‹¤ìš´ë¡œë“œ
                files = get_uploaded_files(analysis['analysis_id'])
                if files:
                    st.markdown("**ì—…ë¡œë“œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°/ë‹¤ìš´ë¡œë“œ**")
                    for file in files:
                        st.write(f"**{file['filename']}** ({file['file_type'].upper()}, {file['file_size']} bytes)")
                        file_bin = get_file_binary(file['file_id'])
                        if file_bin:
                            col1, col2 = st.columns([1,1])
                            with col1:
                                st.download_button(
                                    label="ë‹¤ìš´ë¡œë“œ",
                                    data=file_bin['file_binary_data'],
                                    file_name=file_bin['filename'],
                                    mime=get_file_mime_type(file_bin['file_type']),
                                    key=f"download_{file['file_id']}"
                                )
                            with col2:
                                if st.button("ë¯¸ë¦¬ë³´ê¸°", key=f"preview_{file['file_id']}"):
                                    st.session_state[f'preview_file_{file['file_id']}'] = not st.session_state.get(f'preview_file_{file['file_id']}', False)
                            if st.session_state.get(f'preview_file_{file['file_id']}', False):
                                display_file_preview(file_bin, file_bin['file_type'], file_bin['filename'])
                                if st.button("ë¯¸ë¦¬ë³´ê¸° ë‹«ê¸°", key=f"close_preview_{file['file_id']}"):
                                    st.session_state[f'preview_file_{file['file_id']}'] = False
                # ì—°ê²°ëœ ë³´ê³ ì„œ(analysis report) ì¡°íšŒ ë° í‘œì‹œ
                conn = connect_to_db()
                cursor = conn.cursor(dictionary=True)
                cursor.execute('''
                    SELECT * FROM business_analysis_reports WHERE analysis_id = %s ORDER BY created_at DESC
                ''', (analysis['analysis_id'],))
                reports = cursor.fetchall()
                cursor.close()
                conn.close()
                if reports:
                    for report in reports:
                        st.markdown(f"**[ë³´ê³ ì„œ ìœ í˜•]** {report['report_type']}")
                        st.markdown(report['report_content'])
                else:
                    st.info("ì•„ì§ ìƒì„±ëœ ë¶„ì„ ë³´ê³ ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 