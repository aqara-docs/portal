import streamlit as st
from langchain.schema import HumanMessage, AIMessage
from langchain.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.callbacks.base import BaseCallbackHandler

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="LocalLLM OLLAMA",
    page_icon="ğŸ¤–",
)

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í¬ê¸° ìˆœ)
AVAILABLE_MODELS = [
    {"name": "llama3.2:latest", "size": "2.0 GB"},
    {"name": "llama2:latest", "size": "3.8 GB"},
    {"name": "mistral:latest", "size": "4.1 GB"},
    {"name": "llama3.1:latest", "size": "4.9 GB"},
    {"name": "llama3.1:8b", "size": "4.9 GB"},
    {"name": "gemma:latest", "size": "5.0 GB"},
    {"name": "gemma2:latest", "size": "5.4 GB"},
    {"name": "deepseek-r1:14b", "size": "9.0 GB"},
    {"name": "phi4:latest", "size": "9.1 GB"},
    {"name": "deepseek-r1:32b", "size": "19 GB"},
    {"name": "llama3.3:latest", "size": "42 GB"},
    {"name": "deepseek-r1:70b", "size": "42 GB"},
]

# ëª¨ë¸ ì„ íƒ UI
st.sidebar.title("ëª¨ë¸ ì„¤ì •")
selected_model = st.sidebar.selectbox(
    "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
    options=[model["name"] for model in AVAILABLE_MODELS],
    format_func=lambda x: f"{x} ({next(m['size'] for m in AVAILABLE_MODELS if m['name'] == x)})",
    index=0  # ê¸°ë³¸ê°’ìœ¼ë¡œ ê°€ì¥ ì‘ì€ ëª¨ë¸ ì„ íƒ
)

# Temperature ì„¤ì •
temperature = st.sidebar.slider(
    "Temperature:", 
    min_value=0.0, 
    max_value=2.0, 
    value=0.1, 
    step=0.1,
    help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
)

# ChatCallbackHandler í´ë˜ìŠ¤ ì •ì˜
class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

# Ollamaì˜ Chat ëª¨ë¸ ì‚¬ìš©
@st.cache_resource
def get_llm(model_name, temp):
    return ChatOllama(
        model=model_name,
        temperature=temp,
        streaming=True,
        callbacks=[ChatCallbackHandler()],
    )

# ë©”ì‹œì§€ ê¸°ë¡ì„ ì„¸ì…˜ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
def save_message(message, role):
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    st.session_state["messages"].append({"message": message, "role": role})

# ë©”ì‹œì§€ë¥¼ í™”ë©´ì— ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
def send_message(message, role, save=True):
    if isinstance(message, dict) and "content" in message:
        message = message["content"]
    elif hasattr(message, 'content'):
        message = message.content

    if isinstance(message, str) and message.strip():
        st.chat_message(role).markdown(message)
        if save:
            save_message(message, role)

# ë©”ì‹œì§€ ê¸°ë¡ì„ ê·¸ë¦¬ëŠ” í•¨ìˆ˜
def paint_history():
    if "messages" in st.session_state:
        for message in st.session_state["messages"]:
            send_message(message["message"], message["role"], save=False)

# Prompt í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_template(
    """
    Answer the question based on the context below. 
    Context: {context}
    Question: {question}
    """
)

# ë©”ì¸ UI
st.title("LocalLLM with OLLAMA")
st.markdown(f"í˜„ì¬ ì„ íƒëœ ëª¨ë¸: **{selected_model}**")

# ê¸°ì¡´ ë©”ì‹œì§€ ê¸°ë¡ ê·¸ë¦¬ê¸°
paint_history()

# ìƒˆë¡œìš´ ì…ë ¥ì„ ì²˜ë¦¬
message = st.chat_input("Ask anything...")

if message:
    send_message(message, "human")
    
    # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ë¡œ LLM ì´ˆê¸°í™”
    llm = get_llm(selected_model, temperature)
    
    # ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ chain ì •ì˜
    context = "This is a sample context for the conversation."
    
    chain = (
        {
            "context": RunnableLambda(lambda _: context),
            "question": RunnablePassthrough(),
        }
        | prompt
        | llm
    )
    with st.chat_message("ai"):
        chain.invoke(message)
       
if st.sidebar.button("ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”"):
    st.session_state["messages"] = []
    st.rerun()