import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
import json
import base64
import requests
import graphviz
import plotly.express as px
import plotly.graph_objects as go
import mysql.connector
import concurrent.futures
import time
import re
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import requests
from PIL import Image
import io
import hashlib
import PyPDF2
import docx
from pptx import Presentation
import openpyxl
import pandas as pd
from langchain_anthropic import ChatAnthropic
import time
import math
from PyPDF2 import PdfReader, PdfWriter
import tempfile

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¢ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„",
    page_icon="ğŸ¢",
    layout="wide"
)
st.title("ğŸ¯ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì œí’ˆ ë¶„ì„ ì‹œìŠ¤í…œ")

# ì¸ì¦ ê¸°ëŠ¥
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.')
    st.stop()

if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# MySQL Database configuration
db_config = {
    'user': os.getenv('SQL_USER'),
    'password': os.getenv('SQL_PASSWORD'),
    'host': os.getenv('SQL_HOST'),
    'database': os.getenv('SQL_DATABASE_NEWBIZ'),
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_general_ci'
}

# === ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì œí’ˆ ë¶„ì„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ì •ì˜ ===
BUSINESS_ANALYSIS_AGENTS = {
    "business_model_analyzer": {
        "name": "ğŸ¢ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„ê°€",
        "emoji": "ğŸ¢",
        "description": "íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ìˆ˜ìµ êµ¬ì¡° ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìº”ë²„ìŠ¤ ë¶„ì„
- ìˆ˜ìµ ëª¨ë¸ ë° ê°€ì¹˜ ì œì•ˆ ë¶„ì„
- ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë° ì±„ë„ ë¶„ì„
- í•µì‹¬ íŒŒíŠ¸ë„ˆì‹­ ë° ë¦¬ì†ŒìŠ¤ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ìˆ˜ìµì› ë° ìˆ˜ìµ êµ¬ì¡°
- ê³ ê° ê°€ì¹˜ ì œì•ˆ (CVP)
- ë¹„ìš© êµ¬ì¡° ë° í•µì‹¬ í™œë™
- ê²½ìŸ ìš°ìœ„ ë° ì°¨ë³„í™” ìš”ì†Œ
- ì‹œì¥ í¬ì§€ì…”ë‹

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "product_service_analyzer": {
        "name": "ğŸ“¦ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ê°€",
        "emoji": "ğŸ“¦",
        "description": "ì œí’ˆê³¼ ì„œë¹„ìŠ¤ ë¼ì¸ì—… ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 12ë…„ ê²½ë ¥ì˜ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì œí’ˆ/ì„œë¹„ìŠ¤ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„
- ì œí’ˆ íŠ¹ì§• ë° ê¸°ëŠ¥ ë¶„ì„
- ê°€ê²© ì „ëµ ë° ê°€ì¹˜ ë¶„ì„
- ì œí’ˆ ë¼ì´í”„ì‚¬ì´í´ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- í•µì‹¬ ì œí’ˆ/ì„œë¹„ìŠ¤ ì‹ë³„
- ì œí’ˆ íŠ¹ì§• ë° í˜œíƒ
- ê°€ê²© ì •ì±… ë° ê°€ì¹˜ ì œì•ˆ
- ì œí’ˆ ì°¨ë³„í™” ìš”ì†Œ
- ì‹ ì œí’ˆ ê°œë°œ ë°©í–¥

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "market_position_analyzer": {
        "name": "ğŸ¯ ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„ê°€",
        "emoji": "ğŸ¯",
        "description": "ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜ì™€ ê²½ìŸ ìƒí™© ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„
- ê²½ìŸì‚¬ ë¹„êµ ë¶„ì„
- íƒ€ê²Ÿ ì‹œì¥ ë° ê³ ê° ë¶„ì„
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜
- ê²½ìŸ ìš°ìœ„ ë° ì°¨ë³„í™”
- íƒ€ê²Ÿ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜ ìš”ì†Œ
- ë¸Œëœë“œ í¬ì§€ì…”ë‹

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì‹œì¥ í¬ì§€ì…”ë‹ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "technology_stack_analyzer": {
        "name": "âš™ï¸ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„ê°€",
        "emoji": "âš™ï¸",
        "description": "ì‚¬ìš© ê¸°ìˆ ê³¼ ê¸°ìˆ ì  ì—­ëŸ‰ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 8ë…„ ê²½ë ¥ì˜ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ê¸°ìˆ  ìŠ¤íƒ ë° í”Œë«í¼ ë¶„ì„
- ê°œë°œ ì—­ëŸ‰ ë° ê¸°ìˆ ì  íŠ¹ì§•
- ê¸°ìˆ ì  ì°¨ë³„í™” ìš”ì†Œ
- ê¸°ìˆ ì  ì œì•½ ë° ê¸°íšŒ

**ë¶„ì„ ê´€ì :**
- ì‚¬ìš© ê¸°ìˆ  ë° í”Œë«í¼
- ê¸°ìˆ ì  íŠ¹ì§• ë° ì¥ì 
- ê°œë°œ ì—­ëŸ‰ ë° íŒ€ êµ¬ì„±
- ê¸°ìˆ ì  ì°¨ë³„í™” ìš”ì†Œ
- ê¸°ìˆ ì  ì œì•½ ë° í•œê³„

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ê¸°ìˆ ì  íŠ¹ì§•ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "growth_strategy_analyzer": {
        "name": "ğŸ“ˆ ì„±ì¥ ì „ëµ ë¶„ì„ê°€",
        "emoji": "ğŸ“ˆ",
        "description": "ì„±ì¥ ì „ëµê³¼ ë¹„ì „ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 12ë…„ ê²½ë ¥ì˜ ì„±ì¥ ì „ëµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì„±ì¥ ì „ëµ ë° ë¹„ì „ ë¶„ì„
- ì‹œì¥ í™•ì¥ ê³„íš ë¶„ì„
- íˆ¬ì ë° íŒŒì´ë‚¸ì‹± ë¶„ì„
- ë¯¸ë˜ ê³„íš ë° ë¡œë“œë§µ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì„±ì¥ ì „ëµ ë° ë°©í–¥
- ì‹œì¥ í™•ì¥ ê³„íš
- íˆ¬ì ìœ ì¹˜ ë° íŒŒì´ë‚¸ì‹±
- ë¯¸ë˜ ë¹„ì „ ë° ëª©í‘œ
- ì „ëµì  íŒŒíŠ¸ë„ˆì‹­

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì„±ì¥ ì „ëµì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "financial_health_analyzer": {
        "name": "ğŸ’° ì¬ë¬´ ìƒíƒœ ë¶„ì„ê°€",
        "emoji": "ğŸ’°",
        "description": "ì¬ë¬´ ìƒíƒœì™€ íˆ¬ì ì •ë³´ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì¬ë¬´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì¬ë¬´ ìƒíƒœ ë° ì„±ê³¼ ë¶„ì„
- íˆ¬ì ì •ë³´ ë° íŒŒì´ë‚¸ì‹± ë¶„ì„
- ìˆ˜ìµì„± ë° ì„±ì¥ì„± ë¶„ì„
- ì¬ë¬´ì  ìœ„í—˜ ìš”ì†Œ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì¬ë¬´ ì„±ê³¼ ë° ì§€í‘œ
- íˆ¬ì ìœ ì¹˜ í˜„í™©
- ìˆ˜ìµì„± ë° ì„±ì¥ì„±
- ì¬ë¬´ì  ìœ„í—˜ ìš”ì†Œ
- ìë³¸ êµ¬ì¡° ë° ìœ ë™ì„±

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì¬ë¬´ ìƒíƒœë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    }
}

# === ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë“¤ ===
def scrape_website_basic(url, max_pages=10):
    """ê¸°ë³¸ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        data = {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'meta_keywords': '',
            'headings': [],
            'links': [],
            'images': [],
            'text_content': '',
            'html_structure': str(soup)[:5000],  # HTML êµ¬ì¡° (ì²˜ìŒ 5000ì)
            'status_code': response.status_code,
            'response_time': response.elapsed.total_seconds(),
            'content_length': len(response.content)
        }
        
        # ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                data['meta_description'] = meta.get('content', '')
            elif meta.get('name') == 'keywords':
                data['meta_keywords'] = meta.get('content', '')
        
        # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
        for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            headings = soup.find_all(tag)
            for heading in headings:
                data['headings'].append({
                    'tag': tag,
                    'text': heading.get_text(strip=True)
                })
        
        # ë§í¬ ì¶”ì¶œ
        links = soup.find_all('a', href=True)
        for link in links[:50]:  # ì²˜ìŒ 50ê°œë§Œ
            data['links'].append({
                'text': link.get_text(strip=True),
                'href': link['href'],
                'title': link.get('title', '')
            })
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = soup.find_all('img')
        for img in images[:20]:  # ì²˜ìŒ 20ê°œë§Œ
            data['images'].append({
                'src': img.get('src', ''),
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
        text_content = soup.get_text()
        data['text_content'] = ' '.join(text_content.split())[:10000]  # ì²˜ìŒ 10000ë‹¨ì–´
        
        return data
        
    except Exception as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def analyze_website_performance(url):
    """ì›¹ì‚¬ì´íŠ¸ ì„±ëŠ¥ ë¶„ì„ (PageSpeed Insights API ì‚¬ìš©)"""
    try:
        # Google PageSpeed Insights API í‚¤ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
        api_key = os.getenv('GOOGLE_PAGESPEED_API_KEY')
        
        if api_key:
            api_url = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed"
            params = {
                'url': url,
                'key': api_key,
                'strategy': 'mobile'  # ëª¨ë°”ì¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„
            }
            
            response = requests.get(api_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            return {
                'performance_score': data.get('lighthouseResult', {}).get('categories', {}).get('performance', {}).get('score', 0) * 100,
                'accessibility_score': data.get('lighthouseResult', {}).get('categories', {}).get('accessibility', {}).get('score', 0) * 100,
                'best_practices_score': data.get('lighthouseResult', {}).get('categories', {}).get('best-practices', {}).get('score', 0) * 100,
                'seo_score': data.get('lighthouseResult', {}).get('categories', {}).get('seo', {}).get('score', 0) * 100,
                'first_contentful_paint': data.get('lighthouseResult', {}).get('audits', {}).get('first-contentful-paint', {}).get('numericValue', 0),
                'largest_contentful_paint': data.get('lighthouseResult', {}).get('audits', {}).get('largest-contentful-paint', {}).get('numericValue', 0),
                'cumulative_layout_shift': data.get('lighthouseResult', {}).get('audits', {}).get('cumulative-layout-shift', {}).get('numericValue', 0)
            }
        else:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„
            return {
                'performance_score': None,
                'accessibility_score': None,
                'best_practices_score': None,
                'seo_score': None,
                'note': 'Google PageSpeed Insights API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }
            
    except Exception as e:
        st.warning(f"ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def analyze_website_security(url):
    """ì›¹ì‚¬ì´íŠ¸ ë³´ì•ˆ ë¶„ì„"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        security_data = {
            'https_enabled': url.startswith('https://'),
            'security_headers': {},
            'ssl_certificate': None,
            'content_security_policy': None,
            'x_frame_options': None,
            'x_content_type_options': None,
            'x_xss_protection': None,
            'strict_transport_security': None
        }
        
        # ë³´ì•ˆ í—¤ë” ë¶„ì„
        headers_to_check = [
            'Content-Security-Policy',
            'X-Frame-Options',
            'X-Content-Type-Options',
            'X-XSS-Protection',
            'Strict-Transport-Security'
        ]
        
        for header in headers_to_check:
            value = response.headers.get(header)
            if value:
                security_data['security_headers'][header] = value
                if header == 'Content-Security-Policy':
                    security_data['content_security_policy'] = value
                elif header == 'X-Frame-Options':
                    security_data['x_frame_options'] = value
                elif header == 'X-Content-Type-Options':
                    security_data['x_content_type_options'] = value
                elif header == 'X-XSS-Protection':
                    security_data['x_xss_protection'] = value
                elif header == 'Strict-Transport-Security':
                    security_data['strict_transport_security'] = value
        
        return security_data
        
    except Exception as e:
        st.warning(f"ë³´ì•ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# === ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë“¤ ===
def scrape_website_content(url, max_pages=20):
    """ì›¹ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ (ëª¨ë“  í˜ì´ì§€ í¬í•¨)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ë©”ì¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ëª¨ë“  ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘
        base_url = url.rstrip('/')
        internal_links = set()
        
        # ë§í¬ ì¶”ì¶œ - ì œí’ˆ/ì†”ë£¨ì…˜ ê´€ë ¨ í˜ì´ì§€ ìš°ì„  ìˆ˜ì§‘
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            
            # ê°™ì€ ë„ë©”ì¸ì˜ ë‚´ë¶€ ë§í¬ë§Œ ìˆ˜ì§‘
            if urlparse(full_url).netloc == urlparse(url).netloc:
                # ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
                clean_url = urlparse(full_url)._replace(query='', fragment='').geturl()
                internal_links.add(clean_url)
        
        # ì œí’ˆ/ì†”ë£¨ì…˜ ê´€ë ¨ í˜ì´ì§€ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
        product_related_urls = []
        other_urls = []
        
        for link_url in internal_links:
            link_lower = link_url.lower()
            # ì œí’ˆ/ì†”ë£¨ì…˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ URL ìš°ì„  ìˆ˜ì§‘
            if any(keyword in link_lower for keyword in ['product', 'solution', 'service', 'ì œí’ˆ', 'ì†”ë£¨ì…˜', 'ì„œë¹„ìŠ¤', 'ê¸°ìˆ ', 'ê¸°ëŠ¥', 'íŠ¹ì§•']):
                product_related_urls.append(link_url)
            else:
                other_urls.append(link_url)
        
        # ì œí’ˆ ê´€ë ¨ í˜ì´ì§€ë¥¼ ë¨¼ì €, ê·¸ ë‹¤ìŒ ë‹¤ë¥¸ í˜ì´ì§€ë“¤
        links_to_scrape = product_related_urls[:max_pages//2] + other_urls[:max_pages//2]
        if len(links_to_scrape) < max_pages:
            remaining_urls = [url for url in internal_links if url not in links_to_scrape]
            links_to_scrape.extend(remaining_urls[:max_pages - len(links_to_scrape)])
        
        # ì „ì²´ ì½˜í…ì¸  ë°ì´í„°
        all_content = {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'headings': [],
            'paragraphs': [],
            'lists': [],
            'text_content': '',
            'word_count': 0,
            'sentence_count': 0,
            'paragraph_count': 0,
            'status_code': response.status_code,
            'response_time': response.elapsed.total_seconds(),
            'scraped_pages': [],
            'total_pages': len(links_to_scrape) + 1,
            'product_info': [],  # ì œí’ˆ ì •ë³´ ìˆ˜ì§‘
            'solution_info': [],  # ì†”ë£¨ì…˜ ì •ë³´ ìˆ˜ì§‘
            'technical_specs': [],  # ê¸°ìˆ  ì‚¬ì–‘ ìˆ˜ì§‘
            'pricing_info': [],  # ê°€ê²© ì •ë³´ ìˆ˜ì§‘
            'features_info': []  # ê¸°ëŠ¥ ì •ë³´ ìˆ˜ì§‘
        }
        
        # ë©”ì¸ í˜ì´ì§€ ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                all_content['meta_description'] = meta.get('content', '')
        
        # ë©”ì¸ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ
        main_page_content = extract_page_content(soup, url)
        all_content['scraped_pages'].append(main_page_content)
        
        # ë‚´ë¶€ í˜ì´ì§€ë“¤ ìŠ¤í¬ë˜í•‘
        st.info(f"ğŸ” {len(links_to_scrape)}ê°œì˜ ë‚´ë¶€ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        
        with st.spinner("ë‚´ë¶€ í˜ì´ì§€ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            progress_bar = st.progress(0)  # í•˜ë‚˜ì˜ í”„ë¡œê·¸ë ˆìŠ¤ë°”ë§Œ ì‚¬ìš©
            for i, link_url in enumerate(links_to_scrape):
                try:
                    page_response = requests.get(link_url, headers=headers, timeout=5)
                    if page_response.status_code == 200:
                        page_soup = BeautifulSoup(page_response.content, 'html.parser')
                        page_content = extract_page_content(page_soup, link_url)
                        all_content['scraped_pages'].append(page_content)
                        
                        # ì œí’ˆ/ì†”ë£¨ì…˜ ê´€ë ¨ ì •ë³´ ì¶”ê°€ ì¶”ì¶œ
                        extract_product_solution_info(page_soup, link_url, all_content)
                        
                        # ì§„í–‰ë¥  í‘œì‹œ (í•˜ë‚˜ì˜ í”„ë¡œê·¸ë ˆìŠ¤ë°”ë§Œ ê°±ì‹ )
                        progress = (i + 1) / len(links_to_scrape)
                        progress_bar.progress(progress)
                        
                except Exception as e:
                    st.warning(f"í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {link_url} - {str(e)}")
                    continue
        
        # ëª¨ë“  í˜ì´ì§€ ì½˜í…ì¸  í†µí•©
        all_content = merge_all_page_content(all_content)
        
        st.success(f"âœ… ì´ {len(all_content['scraped_pages'])}ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        
        return all_content
        
    except Exception as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_page_content(soup, page_url):
    """ê°œë³„ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ"""
    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
    
    page_data = {
        'url': page_url,
        'title': soup.title.string if soup.title else '',
        'headings': [],
        'paragraphs': [],
        'lists': [],
        'text_content': ''
    }
    
    # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        headings = soup.find_all(tag)
        for heading in headings:
            text = heading.get_text(strip=True)
            if text:
                page_data['headings'].append({
                    'tag': tag,
                    'text': text,
                    'level': int(tag[1])
                })
    
    # ë¬¸ë‹¨ ì¶”ì¶œ
    paragraphs = soup.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ
            page_data['paragraphs'].append(text)
    
    # ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    lists = soup.find_all(['ul', 'ol'])
    for lst in lists:
        items = lst.find_all('li')
        list_items = []
        for item in items:
            text = item.get_text(strip=True)
            if text:
                list_items.append(text)
        if list_items:
            page_data['lists'].append({
                'type': lst.name,
                'items': list_items
            })
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
    text_content = soup.get_text()
    text_content = ' '.join(text_content.split())
    page_data['text_content'] = text_content
    
    return page_data

def merge_all_page_content(all_content):
    """ëª¨ë“  í˜ì´ì§€ ì½˜í…ì¸  í†µí•©"""
    merged_content = all_content.copy()
    
    # ëª¨ë“  í˜ì´ì§€ì˜ ì½˜í…ì¸  í†µí•©
    all_text = ""
    all_headings = []
    all_paragraphs = []
    all_lists = []
    
    for page in all_content['scraped_pages']:
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  í†µí•©
        if page['text_content']:
            all_text += page['text_content'] + " "
        
        # í—¤ë”© í†µí•©
        all_headings.extend(page['headings'])
        
        # ë¬¸ë‹¨ í†µí•©
        all_paragraphs.extend(page['paragraphs'])
        
        # ë¦¬ìŠ¤íŠ¸ í†µí•©
        all_lists.extend(page['lists'])
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    merged_content['text_content'] = all_text.strip()
    merged_content['headings'] = all_headings
    merged_content['paragraphs'] = list(set(all_paragraphs))  # ì¤‘ë³µ ì œê±°
    merged_content['lists'] = all_lists
    
    # í†µê³„ ì¬ê³„ì‚°
    merged_content['word_count'] = len(merged_content['text_content'].split())
    merged_content['sentence_count'] = len(re.split(r'[.!?]+', merged_content['text_content']))
    merged_content['paragraph_count'] = len(merged_content['paragraphs'])
    
    return merged_content

def analyze_content_readability(text_content):
    """ì½˜í…ì¸  ê°€ë…ì„± ë¶„ì„"""
    try:
        # ê°„ë‹¨í•œ ê°€ë…ì„± ì§€ìˆ˜ ê³„ì‚°
        sentences = re.split(r'[.!?]+', text_content)
        words = text_content.split()
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        # ê¸´ ë¬¸ì¥ ë¹„ìœ¨ (20ë‹¨ì–´ ì´ìƒ)
        long_sentences = sum(1 for s in sentences if len(s.split()) > 20)
        long_sentence_ratio = long_sentences / len(sentences) if sentences else 0
        
        # ê°€ë…ì„± ë“±ê¸‰
        if avg_sentence_length < 10:
            readability_grade = "ë§¤ìš° ì‰¬ì›€"
        elif avg_sentence_length < 15:
            readability_grade = "ì‰¬ì›€"
        elif avg_sentence_length < 20:
            readability_grade = "ë³´í†µ"
        elif avg_sentence_length < 25:
            readability_grade = "ì–´ë ¤ì›€"
        else:
            readability_grade = "ë§¤ìš° ì–´ë ¤ì›€"
        
        return {
            'avg_sentence_length': round(avg_sentence_length, 2),
            'long_sentence_ratio': round(long_sentence_ratio * 100, 2),
            'readability_grade': readability_grade,
            'total_sentences': len(sentences),
            'total_words': len(words)
        }
        
    except Exception as e:
        st.warning(f"ê°€ë…ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_ai_response(prompt, model_name="gpt-4o-mini", system_prompt="", enable_thinking=False):
    """AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        if model_name.startswith("gpt"):
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = openai.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.7,
                max_tokens=3000
            )
            
            return {
                'content': response.choices[0].message.content,
                'success': True,
                'error': None
            }
            
        elif model_name.startswith("claude"):
            response = anthropic_client.messages.create(
                model=model_name,
                max_tokens=3000,
                temperature=0.7,
                system=system_prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                'content': response.content[0].text,
                'success': True,
                'error': None
            }
            
    except Exception as e:
        return {
            'content': f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
            'success': False,
            'error': str(e)
        }

def analyze_with_business_agent(args):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ í•¨ìˆ˜"""
    agent_key, agent_info, content_data, model_name, url, enable_thinking = args
    
    try:
        # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” JSON ì°¨íŠ¸ ë°ì´í„° ê°€ì´ë“œ
        json_chart_guides = {
            "business_model_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "sankey",
  "title": "ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í”Œë¡œìš°",
  "nodes": ["ê³ ê°", "ì œí’ˆ", "ì„œë¹„ìŠ¤", "ìˆ˜ìµ"],
  "source": [0, 1, 2],
  "target": [1, 2, 3],
  "value": [1, 1, 1]
}
```

```json
{
  "type": "pie",
  "title": "ìˆ˜ìµ ëª¨ë¸ êµ¬ì„±",
  "labels": ["ì œí’ˆ íŒë§¤", "ì„œë¹„ìŠ¤ ìˆ˜ìµ", "ë¼ì´ì„¼ìŠ¤"],
  "values": [60, 30, 10]
}
```
""",
            "product_service_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "pie",
  "title": "ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„í¬",
  "labels": ["LED ì¡°ëª…", "ìŠ¤ë§ˆíŠ¸ ì¡°ëª…", "ì¡°ëª… ì œì–´ ì‹œìŠ¤í…œ"],
  "values": [40, 35, 25]
}
```

```json
{
  "type": "bar",
  "title": "ì œí’ˆë³„ ë§¤ì¶œ ë¹„ì¤‘",
  "x": ["ì œí’ˆA", "ì œí’ˆB", "ì œí’ˆC"],
  "y": [30, 45, 25],
  "x_title": "ì œí’ˆ",
  "y_title": "ë§¤ì¶œ ë¹„ì¤‘ (%)"
}
```
""",
            "market_position_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "scatter",
  "title": "ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ",
  "x": [0.8, 0.6, 0.9, 0.4],
  "y": [0.7, 0.8, 0.5, 0.9],
  "labels": ["ìš°ë¦¬íšŒì‚¬", "ê²½ìŸì‚¬A", "ê²½ìŸì‚¬B", "ê²½ìŸì‚¬C"],
  "x_title": "ê°€ê²© ê²½ìŸë ¥",
  "y_title": "í’ˆì§ˆ"
}
```
""",
            "technology_stack_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "pie",
  "title": "ê¸°ìˆ  íˆ¬ì ë¹„ì¤‘",
  "labels": ["LED ê¸°ìˆ ", "IoT ê¸°ìˆ ", "AI ê¸°ìˆ ", "ì œì–´ ì‹œìŠ¤í…œ"],
  "values": [35, 25, 20, 20]
}
```
""",
            "growth_strategy_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "bar",
  "title": "ì„±ì¥ ë‹¨ê³„ë³„ ëª©í‘œ",
  "x": ["1ë‹¨ê³„", "2ë‹¨ê³„", "3ë‹¨ê³„", "4ë‹¨ê³„"],
  "y": [100, 150, 200, 300],
  "x_title": "ì„±ì¥ ë‹¨ê³„",
  "y_title": "ëª©í‘œ ë§¤ì¶œ (ë°±ë§Œì›)"
}
```
""",
            "financial_health_analyzer": """
**JSON ì°¨íŠ¸ ë°ì´í„° ìš”ì²­:**
ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ í¬í•¨í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
```json
{
  "type": "pie",
  "title": "ìˆ˜ìµ êµ¬ì¡° ë¶„ì„",
  "labels": ["ì œí’ˆ ë§¤ì¶œ", "ì„œë¹„ìŠ¤ ë§¤ì¶œ", "ë¼ì´ì„¼ìŠ¤ ë§¤ì¶œ"],
  "values": [70, 20, 10]
}
```

```json
{
  "type": "line",
  "title": "ì¬ë¬´ ì„±ê³¼ ì¶”ì´",
  "x": ["2020", "2021", "2022", "2023"],
  "y": [100, 120, 150, 180],
  "x_title": "ì—°ë„",
  "y_title": "ë§¤ì¶œ (ë°±ë§Œì›)"
}
```
"""
        }
        
        # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
        if agent_key == "product_service_analyzer":
            # ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¶”ê°€
            uploaded_files_info = ""
            if content_data.get('uploaded_files'):
                uploaded_files_info = "\n**ì²¨ë¶€ íŒŒì¼ ì •ë³´:**\n"
                for i, file_data in enumerate(content_data['uploaded_files']):
                    uploaded_files_info += f"""
**íŒŒì¼ {i+1}: {file_data['filename']} ({file_data['file_type'].upper()})**
- íŒŒì¼ í¬ê¸°: {file_data['size']:,} bytes
- íŒŒì¼ ë‚´ìš©: {file_data['content'][:3000]}...
"""
            
            agent_prompt = f"""
{agent_info['system_prompt']}

ë‹¤ìŒì€ {url} ì›¹ì‚¬ì´íŠ¸ì˜ ì½˜í…ì¸  ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìˆ˜:** {len(content_data.get('scraped_pages', []))}ê°œ
**ì´ ë‹¨ì–´ ìˆ˜:** {content_data.get('word_count', 0):,}ê°œ

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½:**
{content_data.get('content_summary', 'N/A')}

**ì½˜í…ì¸  (ì „ì²´ í˜ì´ì§€ í†µí•©):**
{content_data.get('text_content', 'N/A')}

{uploaded_files_info}

**ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ ìš”ì²­ì‚¬í•­:**

1. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**
   - ì£¼ìš” ì œí’ˆ ë¼ì¸ì—… (ì¹´í…Œê³ ë¦¬ë³„)
   - ì„œë¹„ìŠ¤ ìœ í˜• (B2B, B2C, SaaS ë“±)
   - ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¡°

2. **ì œí’ˆ ìƒì„¸ ë¶„ì„**
   - ê° ì œí’ˆì˜ êµ¬ì²´ì ì¸ ì´ë¦„ê³¼ ì„¤ëª…
   - ì œí’ˆë³„ ì£¼ìš” ê¸°ëŠ¥ê³¼ íŠ¹ì§•
   - ì œí’ˆë³„ ê°€ê²© ì •ì±… (ê°€ëŠ¥í•œ ê²½ìš°)
   - ì œí’ˆë³„ íƒ€ê²Ÿ ê³ ê°

3. **ì„œë¹„ìŠ¤ ë¶„ì„**
   - ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ ìœ í˜•
   - ì„œë¹„ìŠ¤ íŠ¹ì§•ê³¼ í˜œíƒ
   - ì„œë¹„ìŠ¤ ê°€ê²© ì •ì±…
   - ì„œë¹„ìŠ¤ ì´ìš© ë°©ë²•

4. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì°¨ë³„í™” ìš”ì†Œ**
   - ê²½ìŸì‚¬ ëŒ€ë¹„ ì¥ì 
   - ë…ìì  ê¸°ìˆ ì´ë‚˜ íŠ¹í—ˆ
   - í’ˆì§ˆ ë³´ì¦ ë° ì¸ì¦
   - ê³ ê° ì§€ì› ì„œë¹„ìŠ¤

5. **ì œí’ˆ ê°œë°œ í˜„í™©**
   - ì‹ ì œí’ˆ ê°œë°œ ê³„íš
   - ì œí’ˆ ì—…ë°ì´íŠ¸ ë° ê°œì„ 
   - ì œí’ˆ ë¼ì´í”„ì‚¬ì´í´ ë‹¨ê³„

6. **ì œí’ˆ ê´€ë ¨ ë°ì´í„°**
   - ì œí’ˆ ìˆ˜ëŸ‰ ë° ì¢…ë¥˜
   - ì œí’ˆë³„ ì¸ê¸°ë„ë‚˜ ì¸ì§€ë„
   - ì œí’ˆ ê´€ë ¨ ê³ ê° í”¼ë“œë°±

**ì¤‘ìš”**: ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ê³¼ ì „ì²´ í˜ì´ì§€ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëˆ„ë½ëœ ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ê°€ ì—†ë„ë¡ í•´ì£¼ì„¸ìš”.

{json_chart_guides.get(agent_key, "")}

**ì¤‘ìš”**: ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ 1-2ê°œ í¬í•¨í•´ì£¼ì„¸ìš”. ì°¨íŠ¸ëŠ” ```json ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¶„ì„ ë‚´ìš©ê³¼ ì˜ ì—°ê³„ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        else:
            # ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¶”ê°€
            uploaded_files_info = ""
            if content_data.get('uploaded_files'):
                uploaded_files_info = "\n**ì²¨ë¶€ íŒŒì¼ ì •ë³´:**\n"
                for i, file_data in enumerate(content_data['uploaded_files']):
                    uploaded_files_info += f"""
**íŒŒì¼ {i+1}: {file_data['filename']} ({file_data['file_type'].upper()})**
- íŒŒì¼ í¬ê¸°: {file_data['size']:,} bytes
- íŒŒì¼ ë‚´ìš©: {file_data['content'][:3000]}...
"""
            
            agent_prompt = f"""
{agent_info['system_prompt']}

ë‹¤ìŒì€ {url} ì›¹ì‚¬ì´íŠ¸ì˜ ì½˜í…ì¸  ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½:**
{content_data.get('content_summary', 'N/A')}

**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìˆ˜:** {len(content_data.get('scraped_pages', []))}ê°œ
**ì´ ë‹¨ì–´ ìˆ˜:** {content_data.get('word_count', 0):,}ê°œ

**ì½˜í…ì¸  (ì „ì²´ í˜ì´ì§€ í†µí•©):**
{content_data.get('text_content', 'N/A')}

{uploaded_files_info}

ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì¸ {agent_info['description']} ê´€ì ì—ì„œ ì´ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ìš”ì²­ì‚¬í•­:**
1. ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ë°œê²¬ì‚¬í•­ (3-5ê°œ)
2. ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì˜ ê°•ì ê³¼ ìš°ìˆ˜í•œ ì 
3. ê°œì„ ì´ í•„ìš”í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­
4. êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê°œì„  ê¶Œê³ ì‚¬í•­
5. ë¹„ì¦ˆë‹ˆìŠ¤ ì ì¬ë ¥ í‰ê°€ (1-10ì , ì´ìœ  í¬í•¨)

{json_chart_guides.get(agent_key, "")}

**ì¤‘ìš”**: ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ JSON í˜•íƒœë¡œ 1-2ê°œ í¬í•¨í•´ì£¼ì„¸ìš”. ì°¨íŠ¸ëŠ” ```json ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¶„ì„ ë‚´ìš©ê³¼ ì˜ ì—°ê³„ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
        
        # AI ì‘ë‹µ ìƒì„±
        response = get_ai_response(
            prompt=agent_prompt,
            model_name=model_name,
            system_prompt=agent_info['system_prompt'],
            enable_thinking=enable_thinking
        )
        
        return {
            'agent_key': agent_key,
            'agent_name': agent_info['name'],
            'agent_emoji': agent_info['emoji'],
            'analysis': response['content'],
            'success': response['success'],
            'error': response.get('error', None)
        }
        
    except Exception as e:
        return {
            'agent_key': agent_key,
            'agent_name': agent_info['name'], 
            'agent_emoji': agent_info['emoji'],
            'analysis': f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            'success': False,
            'error': str(e)
        }

def run_business_multi_agent_analysis(url, content_data, selected_agents, model_name, enable_thinking=False):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰"""
    
    # ì§„í–‰ ìƒí™© í‘œì‹œìš© ì»¨í…Œì´ë„ˆ
    progress_container = st.container()
    
    with progress_container:
        st.info("ğŸš€ **ë¹„ì¦ˆë‹ˆìŠ¤ ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹œì‘**")
        
        # ì—ì´ì „íŠ¸ë³„ ìƒíƒœ í‘œì‹œ
        agent_status = {}
        agent_progress = {}
        
        cols = st.columns(len(selected_agents))
        for i, agent_key in enumerate(selected_agents):
            with cols[i]:
                agent_info = BUSINESS_ANALYSIS_AGENTS[agent_key]
                agent_status[agent_key] = st.empty()
                agent_progress[agent_key] = st.progress(0)
                
                agent_status[agent_key].info(f"{agent_info['emoji']} {agent_info['name']}\nëŒ€ê¸° ì¤‘...")
        
        # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰
        st.info("âš¡ **ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘...**")
        
        # ë¶„ì„ ì¸ì ì¤€ë¹„
        analysis_args = []
        for agent_key in selected_agents:
            agent_info = BUSINESS_ANALYSIS_AGENTS[agent_key]
            args = (agent_key, agent_info, content_data, model_name, url, enable_thinking)
            analysis_args.append(args)
        
        # ë³‘ë ¬ ì‹¤í–‰
        agent_analyses = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(selected_agents)) as executor:
            # ëª¨ë“  ì—ì´ì „íŠ¸ ì‘ì—… ì œì¶œ
            future_to_agent = {
                executor.submit(analyze_with_business_agent, args): args[0] 
                for args in analysis_args
            }
            
            # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
            completed = 0
            for future in concurrent.futures.as_completed(future_to_agent):
                agent_key = future_to_agent[future]
                
                try:
                    result = future.result()
                    agent_analyses.append(result)
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    completed += 1
                    progress = completed / len(selected_agents)
                    
                    agent_progress[agent_key].progress(1.0)
                    
                    if result['success']:
                        agent_status[agent_key].success(f"{result['agent_emoji']} {result['agent_name']}\nâœ… ë¶„ì„ ì™„ë£Œ")
                    else:
                        agent_status[agent_key].error(f"{result['agent_emoji']} {result['agent_name']}\nâŒ ë¶„ì„ ì‹¤íŒ¨")
                    
                except Exception as e:
                    st.error(f"ì—ì´ì „íŠ¸ {agent_key} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        st.success("âœ… **ëª¨ë“  ì—ì´ì „íŠ¸ ë¶„ì„ ì™„ë£Œ**")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸
        st.info("ğŸ“Š **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...**")
        business_analysis = synthesize_business_analysis(url, agent_analyses, content_data, model_name)
        
        if business_analysis['success']:
            st.success("âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ì™„ë£Œ**")
        else:
            st.error("âŒ **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ì‹¤íŒ¨**")
    
    return agent_analyses, business_analysis

def synthesize_business_analysis(url, agent_analyses, content_data, model_name="gpt-4o-mini"):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ê°€ ëª¨ë“  ì—ì´ì „íŠ¸ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¦¬í¬íŠ¸ ì œì‹œ"""
    
    # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ì •ë¦¬
    agent_summaries = []
    for analysis in agent_analyses:
        if analysis['success']:
            agent_summaries.append(f"""
**{analysis['agent_name']} ë¶„ì„:**
{analysis['analysis']}
""")
    
    # ì²¨ë¶€ íŒŒì¼ ì •ë³´ ì¶”ê°€
    uploaded_files_info = ""
    if content_data.get('uploaded_files'):
        uploaded_files_info = "\n**ì²¨ë¶€ íŒŒì¼ ì •ë³´:**\n"
        for i, file_data in enumerate(content_data['uploaded_files']):
            uploaded_files_info += f"""
**íŒŒì¼ {i+1}: {file_data['filename']} ({file_data['file_type'].upper()})**
- íŒŒì¼ í¬ê¸°: {file_data['size']:,} bytes
- íŒŒì¼ ë‚´ìš©: {file_data['content'][:3000]}...
"""
    
    business_prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë“¤ì´ {url} íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆì— ëŒ€í•´ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½:**
{content_data.get('content_summary', 'N/A')}

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë°ì´í„°:**
- ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìˆ˜: {len(content_data.get('scraped_pages', []))}ê°œ
- ì´ ë‹¨ì–´ ìˆ˜: {content_data.get('word_count', 0):,}ê°œ
- ì´ ë¬¸ì¥ ìˆ˜: {content_data.get('sentence_count', 0):,}ê°œ

{uploaded_files_info}

**ì¤‘ìš”: ë°˜ë“œì‹œ ì²¨ë¶€ íŒŒì¼ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë¶„ì„ì— ë°˜ì˜í•˜ì„¸ìš”. ì²¨ë¶€ íŒŒì¼ì—ë§Œ ìˆëŠ” ì •ë³´ë„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.**

**ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼:**
{''.join(agent_summaries)}

**ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìš”ì²­ì‚¬í•­:**
1. **Executive Summary** (ì‹¤í–‰ ìš”ì•½)
2. **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë°œê²¬ì‚¬í•­** (ê° ì „ë¬¸ê°€ ì˜ê²¬ì˜ ê³µí†µì ê³¼ ì°¨ì´ì )
3. **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í‰ê°€** (ì „ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì )
4. **ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„** (í•µì‹¬ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ íŠ¹ì§•)
5. **ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„** (ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜ì™€ ê²½ìŸ ìƒí™©)
6. **ì„±ì¥ ì „ëµ ì œì–¸** (êµ¬ì²´ì ì¸ ì„±ì¥ ë°©í–¥)
7. **íˆ¬ì ê°€ì¹˜ í‰ê°€** (1-10ì , ìƒì„¸ ì´ìœ )

**JSON ì°¨íŠ¸ ë°ì´í„° í•„ìˆ˜ í¬í•¨:**
- **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìº”ë²„ìŠ¤**: Sankey ì°¨íŠ¸ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ êµ¬ì„±ìš”ì†Œ ì‹œê°í™”
- **ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„**: Pie ì°¨íŠ¸ë¡œ ì œí’ˆë³„ ë§¤ì¶œ ë¹„ì¤‘ í‘œì‹œ
- **ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ**: Scatter ì°¨íŠ¸ë¡œ ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜ í‘œì‹œ
- **ì„±ì¥ ë¡œë“œë§µ**: Bar ì°¨íŠ¸ë¡œ ë‹¨ê³„ë³„ ì„±ì¥ ê³„íš í‘œì‹œ

**ì¤‘ìš”**: ë°˜ë“œì‹œ 2-3ê°œì˜ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ```json ì½”ë“œë¸”ë¡ìœ¼ë¡œ í¬í•¨í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.

íˆ¬ììì™€ ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = get_ai_response(
            prompt=business_prompt,
            model_name=model_name,
            system_prompt="ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆ ë¶„ì„ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
            enable_thinking=False
        )
        
        return {
            'content': response['content'],
            'success': response['success'],
            'error': response.get('error', None)
        }
        
    except Exception as e:
        return {
            'content': f"ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            'success': False,
            'error': str(e)
        }

def save_website_analysis(url, website_data, agent_analyses, cto_analysis, analysis_date, uploaded_files=None):
    """ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()

        # 1. website_analyses í…Œì´ë¸”ì— ê¸°ë³¸ ì •ë³´ ì €ì¥
        cursor.execute('''
            INSERT INTO website_analyses 
            (url, analysis_date, title, meta_description, status_code, response_time)
            VALUES (%s, %s, %s, %s, %s, %s)
        ''', (
            url,
            analysis_date,
            website_data.get('title', ''),
            website_data.get('meta_description', ''),
            website_data.get('status_code', 0),
            website_data.get('response_time', 0)
        ))
        analysis_id = cursor.lastrowid

        # 2. website_content_data í…Œì´ë¸”ì— ì½˜í…ì¸  ë°ì´í„° ì €ì¥ (Perplexity ìš”ì•½ í¬í•¨)
        cursor.execute('''
            INSERT INTO website_content_data
            (analysis_id, headings_count, links_count, images_count, text_content_length, perplexity_summary)
            VALUES (%s, %s, %s, %s, %s, %s)
        ''', (
            analysis_id,
            len(website_data.get('headings', [])),
            len(website_data.get('links', [])),
            len(website_data.get('images', [])),
            len(website_data.get('text_content', '')),
            website_data.get('perplexity_summary', '')  # Perplexity ìš”ì•½ ì •ë³´ ì¶”ê°€
        ))

        # 3. ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì €ì¥
        if uploaded_files:
            for file_data in uploaded_files:
                cursor.execute('''
                    INSERT INTO website_analysis_files 
                    (analysis_id, filename, file_type, file_content, file_binary_data, file_size)
                    VALUES (%s, %s, %s, %s, %s, %s)
                ''', (
                    analysis_id,
                    file_data['filename'],
                    file_data['file_type'],
                    file_data['content'],
                    file_data['binary_data'],
                    file_data['size']
                ))

        # 4. website_agent_analyses í…Œì´ë¸”ì— AI ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
        for agent_analysis in agent_analyses:
            cursor.execute('''
                INSERT INTO website_agent_analyses
                (analysis_id, agent_type, analysis_content)
                VALUES (%s, %s, %s)
            ''', (
                analysis_id,
                agent_analysis['agent_key'],
                agent_analysis['analysis']
            ))

        # 5. website_cto_analysis í…Œì´ë¸”ì— CTO ì¢…í•© ë¶„ì„ ì €ì¥
        cursor.execute('''
            INSERT INTO website_cto_analysis
            (analysis_id, cto_analysis_content)
            VALUES (%s, %s)
        ''', (
            analysis_id,
            cto_analysis['content']
        ))

        conn.commit()
        cursor.close()
        conn.close()
        return True, analysis_id

    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return False, None

def migrate_website_content_data():
    """ê¸°ì¡´ website_content_data í…Œì´ë¸”ì— Perplexity ìš”ì•½ ì»¬ëŸ¼ ì¶”ê°€"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        # ê¸°ì¡´ í…Œì´ë¸”ì— perplexity_summary ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        cursor.execute('''
            SELECT COLUMN_NAME 
            FROM INFORMATION_SCHEMA.COLUMNS 
            WHERE TABLE_SCHEMA = %s 
            AND TABLE_NAME = 'website_content_data' 
            AND COLUMN_NAME = 'perplexity_summary'
        ''', (db_config['database'],))
        
        column_exists = cursor.fetchone()
        
        if not column_exists:
            # perplexity_summary ì»¬ëŸ¼ ì¶”ê°€
            cursor.execute('''
                ALTER TABLE website_content_data 
                ADD COLUMN perplexity_summary LONGTEXT
            ''')
            conn.commit()
            st.success("âœ… Perplexity ìš”ì•½ ì»¬ëŸ¼ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("â„¹ï¸ Perplexity ìš”ì•½ ì»¬ëŸ¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        cursor.close()
        conn.close()
        return True
        
    except mysql.connector.Error as err:
        st.error(f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return False

def create_website_analysis_tables():
    """ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()

        # 1. ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ê¸°ë³¸ ì •ë³´ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_analyses (
                analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                url VARCHAR(500) NOT NULL,
                analysis_date DATETIME DEFAULT CURRENT_TIMESTAMP,
                title VARCHAR(500),
                meta_description TEXT,
                status_code INT,
                response_time DECIMAL(10,3),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # 2. ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë°ì´í„° í…Œì´ë¸” (Perplexity ìš”ì•½ ì •ë³´ ì¶”ê°€)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_content_data (
                content_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                headings_count INT DEFAULT 0,
                links_count INT DEFAULT 0,
                images_count INT DEFAULT 0,
                text_content_length INT DEFAULT 0,
                perplexity_summary LONGTEXT,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        # 3. ì›¹ì‚¬ì´íŠ¸ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_agent_analyses (
                agent_analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                agent_type VARCHAR(100),
                analysis_content LONGTEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        # 4. CTO ì¢…í•© ë¶„ì„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_cto_analysis (
                cto_analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                cto_analysis_content LONGTEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        conn.commit()
        cursor.close()
        conn.close()
        return True

    except mysql.connector.Error as err:
        st.error(f"í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return False

def get_saved_website_analyses():
    """ì €ì¥ëœ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ëª©ë¡ ì¡°íšŒ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute('''
            SELECT 
                wa.analysis_id,
                wa.url,
                wa.title,
                wa.analysis_date,
                wa.status_code,
                wa.response_time,
                wcd.headings_count,
                wcd.links_count,
                wcd.images_count
            FROM website_analyses wa
            LEFT JOIN website_content_data wcd ON wa.analysis_id = wcd.analysis_id
            ORDER BY wa.analysis_date DESC
        ''')
        
        results = cursor.fetchall()
        return results
    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return []
    finally:
        if 'conn' in locals():
            cursor.close()
            conn.close()

def get_analysis_detail(analysis_id):
    """íŠ¹ì • ë¶„ì„ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True)
        # 1. ê¸°ë³¸ ì •ë³´
        cursor.execute('''
            SELECT * FROM website_analyses WHERE analysis_id = %s
        ''', (analysis_id,))
        base = cursor.fetchone()
        # 2. ì½˜í…ì¸  ë°ì´í„° (Perplexity ìš”ì•½ í¬í•¨)
        cursor.execute('''
            SELECT * FROM website_content_data WHERE analysis_id = %s
        ''', (analysis_id,))
        content = cursor.fetchone()
        # 3. ì—ì´ì „íŠ¸ ë¶„ì„
        cursor.execute('''
            SELECT agent_type, analysis_content FROM website_agent_analyses WHERE analysis_id = %s
        ''', (analysis_id,))
        agents = cursor.fetchall()
        # 4. ì¢…í•© ë¦¬í¬íŠ¸
        cursor.execute('''
            SELECT cto_analysis_content FROM website_cto_analysis WHERE analysis_id = %s
        ''', (analysis_id,))
        cto = cursor.fetchone()
        
        # 5. ì²¨ë¶€ íŒŒì¼ ì¡°íšŒ
        cursor.execute('''
            SELECT * FROM website_analysis_files WHERE analysis_id = %s
        ''', (analysis_id,))
        files = cursor.fetchall()
        
        cursor.close()
        conn.close()
        return {
            'base': base,
            'content': content,
            'agents': agents,
            'cto': cto['cto_analysis_content'] if cto else None,
            'files': files
        }
    except Exception as e:
        st.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None

def delete_analysis(analysis_id):
    """íŠ¹ì • ë¶„ì„ ì‚­ì œ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        # ì‚­ì œ ì „ í™•ì¸
        cursor.execute('SELECT COUNT(*) FROM website_analyses WHERE analysis_id = %s', (analysis_id,))
        before_count = cursor.fetchone()[0]
        
        if before_count == 0:
            st.error(f"ë¶„ì„ ID {analysis_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì‚­ì œ ì‹¤í–‰
        cursor.execute('DELETE FROM website_analyses WHERE analysis_id = %s', (analysis_id,))
        deleted_rows = cursor.rowcount
        
        if deleted_rows == 0:
            st.error("ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        conn.commit()
        cursor.close()
        conn.close()
        
        st.success(f"âœ… ë¶„ì„ ID {analysis_id} ì‚­ì œ ì™„ë£Œ (ì‚­ì œëœ í–‰: {deleted_rows})")
        return True
        
    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {err}")
        return False
    except Exception as e:
        st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

# === ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ë¶„ì„ í•¨ìˆ˜ë“¤ ===
def create_content_summary_agent():
    """ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸"""
    return {
        "name": "ğŸ“‹ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ì „ë¬¸ê°€",
        "emoji": "ğŸ“‹",
        "description": "ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  ì½˜í…ì¸ ë¥¼ ì¢…í•©í•˜ì—¬ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì™€ ì œí’ˆì— ëŒ€í•œ ì‹¤ì œì ì¸ ìš”ì•½ ì œê³µ",
        "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ë¶„ì„
- íšŒì‚¬ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ íŒŒì•…
- ì œí’ˆ/ì„œë¹„ìŠ¤ ë¼ì¸ì—… ì •ë¦¬
- íšŒì‚¬ ì •ë³´ ë° íŠ¹ì§• ìš”ì•½

**ë¶„ì„ ê´€ì :**
- íšŒì‚¬ê°€ í•˜ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ë³¸ì§ˆ
- ì£¼ìš” ì œí’ˆê³¼ ì„œë¹„ìŠ¤
- íšŒì‚¬ì˜ íŠ¹ì§•ê³¼ ì°¨ë³„í™” ìš”ì†Œ
- ê³ ê° ëŒ€ìƒ ë° ì‹œì¥ í¬ì§€ì…”ë‹
- íšŒì‚¬ì˜ ë¹„ì „ê³¼ ë¯¸ì…˜

ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ ì½˜í…ì¸ ë¥¼ ì¢…í•©í•˜ì—¬ íšŒì‚¬ì— ëŒ€í•œ ì‹¤ì œì ì´ê³  ì •í™•í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”."""
    }

def extract_mermaid_charts(text):
    """í…ìŠ¤íŠ¸ì—ì„œ Mermaid ì°¨íŠ¸ ì½”ë“œë¥¼ ì¶”ì¶œ"""
    import re
    mermaid_pattern = r'```mermaid\s*\n(.*?)\n```'
    matches = re.findall(mermaid_pattern, text, re.DOTALL)
    return matches

def extract_chart_data(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ì°¨íŠ¸ ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
    import re
    import json
    
    # JSON ì°¨íŠ¸ ë°ì´í„° íŒ¨í„´ ì°¾ê¸°
    json_pattern = r'```json\s*\n(.*?)\n```'
    matches = re.findall(json_pattern, text, re.DOTALL)
    
    chart_data = []
    for match in matches:
        try:
            data = json.loads(match)
            chart_data.append(data)
        except:
            continue
    
    return chart_data

def create_plotly_chart_from_data(chart_data):
    """ì°¨íŠ¸ ë°ì´í„°ë¡œë¶€í„° Plotly ì°¨íŠ¸ ìƒì„±"""
    try:
        chart_type = chart_data.get('type', '').lower()
        
        if chart_type == 'pie':
            return create_pie_chart_from_data(chart_data)
        elif chart_type == 'bar':
            return create_bar_chart_from_data(chart_data)
        elif chart_type == 'scatter':
            return create_scatter_chart_from_data(chart_data)
        elif chart_type == 'sankey':
            return create_sankey_chart_from_data(chart_data)
        elif chart_type == 'line':
            return create_line_chart_from_data(chart_data)
        else:
            return None
    except Exception as e:
        st.warning(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_pie_chart_from_data(chart_data):
    """Pie ì°¨íŠ¸ ìƒì„±"""
    try:
        labels = chart_data.get('labels', [])
        values = chart_data.get('values', [])
        title = chart_data.get('title', 'Pie Chart')
        
        if not labels or not values:
            return None
        
        fig = go.Figure(data=[go.Pie(
            labels=labels,
            values=values,
            hole=0.3
        )])
        
        fig.update_layout(
            title_text=title,
            height=400
        )
        
        return fig
    except Exception as e:
        st.warning(f"Pie ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_bar_chart_from_data(chart_data):
    """Bar ì°¨íŠ¸ ìƒì„±"""
    try:
        x = chart_data.get('x', [])
        y = chart_data.get('y', [])
        title = chart_data.get('title', 'Bar Chart')
        x_title = chart_data.get('x_title', 'Xì¶•')
        y_title = chart_data.get('y_title', 'Yì¶•')
        
        if not x or not y:
            return None
        
        fig = go.Figure(data=[go.Bar(
            x=x,
            y=y,
            text=y,
            textposition='auto'
        )])
        
        fig.update_layout(
            title_text=title,
            xaxis_title=x_title,
            yaxis_title=y_title,
            height=400
        )
        
        return fig
    except Exception as e:
        st.warning(f"Bar ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_scatter_chart_from_data(chart_data):
    """Scatter ì°¨íŠ¸ ìƒì„±"""
    try:
        x = chart_data.get('x', [])
        y = chart_data.get('y', [])
        labels = chart_data.get('labels', [])
        title = chart_data.get('title', 'Scatter Chart')
        x_title = chart_data.get('x_title', 'Xì¶•')
        y_title = chart_data.get('y_title', 'Yì¶•')
        
        if not x or not y:
            return None
        
        fig = go.Figure(data=[go.Scatter(
            x=x,
            y=y,
            mode='markers+text',
            text=labels if labels else None,
            textposition="top center",
            marker=dict(size=15, color='blue')
        )])
        
        fig.update_layout(
            title_text=title,
            xaxis_title=x_title,
            yaxis_title=y_title,
            height=400
        )
        
        return fig
    except Exception as e:
        st.warning(f"Scatter ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_sankey_chart_from_data(chart_data):
    """Sankey ì°¨íŠ¸ ìƒì„±"""
    try:
        nodes = chart_data.get('nodes', [])
        source = chart_data.get('source', [])
        target = chart_data.get('target', [])
        value = chart_data.get('value', [])
        title = chart_data.get('title', 'Sankey Chart')
        
        if not nodes or not source or not target:
            return None
        
        fig = go.Figure(data=[go.Sankey(
            node=dict(
                pad=15,
                thickness=20,
                line=dict(color="black", width=0.5),
                label=nodes,
                color="blue"
            ),
            link=dict(
                source=source,
                target=target,
                value=value if value else [1] * len(source)
            )
        )])
        
        fig.update_layout(
            title_text=title,
            font_size=10,
            height=400
        )
        
        return fig
    except Exception as e:
        st.warning(f"Sankey ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_line_chart_from_data(chart_data):
    """Line ì°¨íŠ¸ ìƒì„±"""
    try:
        x = chart_data.get('x', [])
        y = chart_data.get('y', [])
        title = chart_data.get('title', 'Line Chart')
        x_title = chart_data.get('x_title', 'Xì¶•')
        y_title = chart_data.get('y_title', 'Yì¶•')
        
        if not x or not y:
            return None
        
        fig = go.Figure(data=[go.Scatter(
            x=x,
            y=y,
            mode='lines+markers'
        )])
        
        fig.update_layout(
            title_text=title,
            xaxis_title=x_title,
            yaxis_title=y_title,
            height=400
        )
        
        return fig
    except Exception as e:
        st.warning(f"Line ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def display_analysis_with_plots(analysis_text, title="ë¶„ì„ ê²°ê³¼", unique_id=None):
    """ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ Plotly ì°¨íŠ¸ì™€ í•¨ê»˜ í‘œì‹œ (Streamlit key ì¤‘ë³µ ë°©ì§€)"""
    st.markdown(f"### {title}")
    # ì°¨íŠ¸ ë°ì´í„° ì¶”ì¶œ ë° ë³€í™˜
    chart_data_list = extract_chart_data(analysis_text)
    plotly_charts = []
    for chart_data in chart_data_list:
        plotly_chart = create_plotly_chart_from_data(chart_data)
        if plotly_chart:
            plotly_charts.append(plotly_chart)
    # JSON ì½”ë“œë¥¼ ì œê±°í•œ í…ìŠ¤íŠ¸ ìƒì„±
    clean_text = analysis_text
    import re
    json_pattern = r'```json\s*\n.*?\n```'
    clean_text = re.sub(json_pattern, '', clean_text, flags=re.DOTALL)
    # í…ìŠ¤íŠ¸ í‘œì‹œ
    st.markdown(clean_text)
    # Plotly ì°¨íŠ¸ í‘œì‹œ
    if plotly_charts:
        st.markdown("### ğŸ“Š ì‹œê°í™” ì°¨íŠ¸")
        for i, chart in enumerate(plotly_charts):
            # ê³ ìœ  í‚¤ ìƒì„±: title, i, unique_id, chart_data í•´ì‹œ
            chart_hash = hashlib.md5(str(chart_data_list[i]).encode('utf-8')).hexdigest()[:8]
            key_parts = ["plotly_chart", str(title), str(i)]
            if unique_id is not None:
                key_parts.append(str(unique_id))
            key_parts.append(chart_hash)
            chart_key = "_".join(key_parts)
            st.plotly_chart(chart, use_container_width=True, key=chart_key)
            if i < len(plotly_charts) - 1:
                st.markdown("---")

def analyze_website_content_summary(url, content_data, model_name="gpt-4o-mini"):
    """ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ìš”ì•½ ë¶„ì„"""
    
    summary_agent = create_content_summary_agent()
    
    # ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ì •ë³´ ì •ë¦¬
    pages_info = []
    for i, page in enumerate(content_data.get('scraped_pages', [])):
        pages_info.append(f"""
**í˜ì´ì§€ {i+1}: {page['url']}**
- ì œëª©: {page['title']}
- í—¤ë”© ìˆ˜: {len(page['headings'])}
- ë¬¸ë‹¨ ìˆ˜: {len(page['paragraphs'])}
- ì£¼ìš” í—¤ë”©: {', '.join([h['text'] for h in page['headings'][:5]])}
- ì½˜í…ì¸  ìƒ˜í”Œ: {page['text_content'][:200]}...
""")
    
    # ì œí’ˆ ë° ì†”ë£¨ì…˜ ì •ë³´ ì •ë¦¬
    product_info_summary = ""
    if content_data.get('product_info'):
        product_info_summary = "\n**ì œí’ˆ ì •ë³´ ìƒì„¸:**\n"
        for i, product in enumerate(content_data['product_info']):
            product_info_summary += f"""
**ì œí’ˆ {i+1}:**
- ì œí’ˆëª…: {product.get('product_name', 'N/A')}
- ì„¤ëª…: {product.get('product_description', 'N/A')}
- ê¸°ëŠ¥: {', '.join(product.get('product_features', []))}
- ê¸°ìˆ ì‚¬ì–‘: {', '.join([f"{k}: {v}" for k, v in product.get('product_specs', {}).items()])}
- URL: {product.get('url', 'N/A')}
"""
    
    solution_info_summary = ""
    if content_data.get('solution_info'):
        solution_info_summary = "\n**ì†”ë£¨ì…˜ ì •ë³´ ìƒì„¸:**\n"
        for i, solution in enumerate(content_data['solution_info']):
            solution_info_summary += f"""
**ì†”ë£¨ì…˜ {i+1}:**
- ì†”ë£¨ì…˜ëª…: {solution.get('solution_name', 'N/A')}
- ì„¤ëª…: {solution.get('solution_description', 'N/A')}
- í˜œíƒ: {', '.join(solution.get('solution_benefits', []))}
- URL: {solution.get('url', 'N/A')}
"""
    
    pricing_info_summary = ""
    if content_data.get('pricing_info'):
        pricing_info_summary = "\n**ê°€ê²© ì •ë³´:**\n"
        for price in content_data['pricing_info']:
            pricing_info_summary += f"- {price.get('price_text', 'N/A')} (ì»¨í…ìŠ¤íŠ¸: {price.get('context', 'N/A')})\n"
    
    technical_specs_summary = ""
    if content_data.get('technical_specs'):
        technical_specs_summary = "\n**ê¸°ìˆ  ì‚¬ì–‘:**\n"
        for spec in content_data['technical_specs']:
            technical_specs_summary += f"- URL: {spec.get('url', 'N/A')}\n"
            for key, value in spec.get('specs', {}).items():
                technical_specs_summary += f"  - {key}: {value}\n"
    
    features_info_summary = ""
    if content_data.get('features_info'):
        features_info_summary = "\n**ê¸°ëŠ¥ ì •ë³´:**\n"
        for feature in content_data['features_info']:
            features_info_summary += f"- URL: {feature.get('url', 'N/A')}\n"
            for func in feature.get('features', []):
                features_info_summary += f"  - {func}\n"
    
    # ì²¨ë¶€ íŒŒì¼ ì •ë³´ ìš”ì•½
    uploaded_files_summary = ""
    if content_data.get('uploaded_files'):
        uploaded_files_summary = "\n**ì²¨ë¶€ íŒŒì¼ ì •ë³´:**\n"
        for i, file_data in enumerate(content_data['uploaded_files']):
            uploaded_files_summary += f"""
**íŒŒì¼ {i+1}: {file_data['filename']} ({file_data['file_type'].upper()})**
- íŒŒì¼ í¬ê¸°: {file_data['size']:,} bytes
- íŒŒì¼ ë‚´ìš© ìƒ˜í”Œ: {file_data['content'][:3000]}...
"""
    
    summary_prompt = f"""
{summary_agent['system_prompt']}

ë‹¤ìŒì€ {url} ì›¹ì‚¬ì´íŠ¸ì˜ ì¢…í•© ì½˜í…ì¸  ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

**ìŠ¤í¬ë˜í•‘ ê°œìš”:**
- ì´ ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€: {len(content_data.get('scraped_pages', []))}ê°œ
- ì´ ë‹¨ì–´ ìˆ˜: {content_data.get('word_count', 0):,}ê°œ
- ì´ ë¬¸ì¥ ìˆ˜: {content_data.get('sentence_count', 0):,}ê°œ
- ì´ ë¬¸ë‹¨ ìˆ˜: {content_data.get('paragraph_count', 0):,}ê°œ

**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìƒì„¸ ì •ë³´:**
{''.join(pages_info)}

**í†µí•© ì½˜í…ì¸  (ëª¨ë“  í˜ì´ì§€ ì¢…í•©):**
{content_data.get('text_content', 'N/A')}

**ì£¼ìš” í—¤ë”© êµ¬ì¡° (ì „ì²´ í˜ì´ì§€ í†µí•©):**
{chr(10).join([f"- {h['text']}" for h in content_data.get('headings', [])[:20]])}

{product_info_summary}
{solution_info_summary}
{pricing_info_summary}
{technical_specs_summary}
{features_info_summary}
{uploaded_files_summary}

**ì¤‘ìš”: ë°˜ë“œì‹œ ì²¨ë¶€ íŒŒì¼ì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë¶„ì„ì— ë°˜ì˜í•˜ì„¸ìš”. ì²¨ë¶€ íŒŒì¼ì—ë§Œ ìˆëŠ” ì •ë³´ë„ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”.**

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ìš”ì²­ì‚¬í•­:**

1. **íšŒì‚¬ ê¸°ë³¸ ì •ë³´ (ìƒì„¸)**
   - íšŒì‚¬ëª… ë° ì£¼ìš” ë¸Œëœë“œ (ì •í™•í•œ ëª…ì¹­)
   - íšŒì‚¬ì˜ í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­ (êµ¬ì²´ì ì¸ ì—…ë¬´ ë‚´ìš©)
   - íšŒì‚¬ ì„¤ë¦½ ë°°ê²½ ë° ì—­ì‚¬ (ê°€ëŠ¥í•œ í•œ ìƒì„¸í•˜ê²Œ)
   - íšŒì‚¬ ê·œëª¨ ë° ì¡°ì§ êµ¬ì¡° (ì§ì› ìˆ˜, ì§€ì , ì¡°ì§ë„ ë“±)
   - íšŒì‚¬ ìœ„ì¹˜ ë° ì—°ë½ì²˜ ì •ë³´

2. **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìš”ì•½ (ìƒì„¸)**
   - íšŒì‚¬ê°€ í•˜ëŠ” ì¼ì˜ ë³¸ì§ˆ (êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë°©ì‹)
   - ì£¼ìš” ìˆ˜ìµì› ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë°©ì‹ (ë§¤ì¶œ êµ¬ì¡°, ìˆ˜ìµ ëª¨ë¸)
   - ê³ ê° ëŒ€ìƒ ë° ì‹œì¥ (íƒ€ê²Ÿ ê³ ê°ì¸µ, ì‹œì¥ ê·œëª¨)
   - í•µì‹¬ ê°€ì¹˜ ì œì•ˆ (ê³ ê°ì—ê²Œ ì œê³µí•˜ëŠ” ê°€ì¹˜)
   - ê²½ìŸ ìš°ìœ„ ë° ì°¨ë³„í™” ìš”ì†Œ

3. **ì œí’ˆ/ì„œë¹„ìŠ¤ ë¼ì¸ì—… (ë§¤ìš° ìƒì„¸)**
   - ì£¼ìš” ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
   - ê° ì œí’ˆì˜ êµ¬ì²´ì ì¸ ì´ë¦„ê³¼ ìƒì„¸ ì„¤ëª…
   - ì œí’ˆë³„ ì£¼ìš” ê¸°ëŠ¥ê³¼ íŠ¹ì§• (ê¸°ìˆ ì  íŠ¹ì§• í¬í•¨)
   - ì œí’ˆë³„ ê°€ê²© ì •ì±… (ê°€ëŠ¥í•œ ê²½ìš° êµ¬ì²´ì ì¸ ê°€ê²©)
   - ì œí’ˆë³„ íƒ€ê²Ÿ ê³ ê° ë° ì‚¬ìš© ì‚¬ë¡€
   - ì œí’ˆë³„ ê¸°ìˆ  ì‚¬ì–‘ ë° ì„±ëŠ¥ ì§€í‘œ
   - ì œí’ˆë³„ ì¸ì¦ ë° í’ˆì§ˆ ë³´ì¦ ì •ë³´

4. **ì†”ë£¨ì…˜ ë° ì„œë¹„ìŠ¤ (ìƒì„¸)**
   - ì œê³µí•˜ëŠ” ì†”ë£¨ì…˜ ìœ í˜• ë° ì´ë¦„
   - ê° ì†”ë£¨ì…˜ì˜ ìƒì„¸ ì„¤ëª… ë° íŠ¹ì§•
   - ì†”ë£¨ì…˜ë³„ ì£¼ìš” í˜œíƒ ë° íš¨ê³¼
   - ì†”ë£¨ì…˜ë³„ ì ìš© ë¶„ì•¼ ë° ì‚¬ìš© ì‚¬ë¡€
   - ì†”ë£¨ì…˜ë³„ ê°€ê²© ì •ì±… ë° ê³„ì•½ ì¡°ê±´
   - ì†”ë£¨ì…˜ë³„ ê¸°ìˆ ì  íŠ¹ì§• ë° êµ¬í˜„ ë°©ì‹

5. **íšŒì‚¬ íŠ¹ì§• ë° ì°¨ë³„í™” ìš”ì†Œ (ìƒì„¸)**
   - ê²½ìŸì‚¬ ëŒ€ë¹„ ì¥ì  ë° ì°¨ë³„í™” ìš”ì†Œ
   - ë…ìì  ê¸°ìˆ ì´ë‚˜ íŠ¹í—ˆ ì •ë³´
   - í’ˆì§ˆ ë³´ì¦ ë° ì¸ì¦ ì •ë³´
   - íšŒì‚¬ì˜ ë¹„ì „ê³¼ ë¯¸ì…˜
   - íšŒì‚¬ì˜ í•µì‹¬ ê°€ì¹˜ ë° ë¬¸í™”

6. **ê³ ê° ë° ì‹œì¥ ì •ë³´ (ìƒì„¸)**
   - ì£¼ìš” ê³ ê°ì¸µ ë° íƒ€ê²Ÿ ì‹œì¥
   - ì„œë¹„ìŠ¤ ì§€ì—­ ë° ê¸€ë¡œë²Œ ì§„ì¶œ í˜„í™©
   - ì‹œì¥ì—ì„œì˜ í¬ì§€ì…”ë‹
   - íŒŒíŠ¸ë„ˆì‹­ ë° í˜‘ë ¥ ê´€ê³„
   - ê³ ê° ì§€ì› ë° ì„œë¹„ìŠ¤ ì²´ê³„

7. **ì—°ë½ì²˜ ë° ì„œë¹„ìŠ¤ ì •ë³´ (ìƒì„¸)**
   - ê³ ê° ë¬¸ì˜ ë°©ë²• ë° ì—°ë½ì²˜
   - ì„œë¹„ìŠ¤ ì´ìš© ë°©ë²• ë° í”„ë¡œì„¸ìŠ¤
   - êµ¬ë§¤/ê³„ì•½ í”„ë¡œì„¸ìŠ¤
   - ê³ ê° ì§€ì› ì„œë¹„ìŠ¤ ë° A/S ì •ë³´
   - ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ ì„œë¹„ìŠ¤ ì±„ë„

**ì¤‘ìš”**: 
- ì›¹ì‚¬ì´íŠ¸ì™€ ì²¨ë¶€ íŒŒì¼ì— ì‹¤ì œë¡œ ë‚˜ì™€ìˆëŠ” ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”.
- ì œí’ˆê³¼ ì†”ë£¨ì…˜ ì •ë³´ëŠ” ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•˜ê²Œ ê¸°ìˆ í•´ì£¼ì„¸ìš”.
- ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ í•˜ì§€ ë§ê³ , ì½˜í…ì¸ ì—ì„œ í™•ì¸ëœ ì‚¬ì‹¤ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ì •ë³´ê°€ ë¶€ì¡±í•œ ë¶€ë¶„ì€ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.
- ìš”ì•½ì´ë¼ê³  í•´ì„œ ê°„ë‹¨í•˜ê²Œ í•˜ì§€ ë§ê³ , ëª¨ë“  ì¤‘ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ìƒì„¸í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ì™€ ì²¨ë¶€ íŒŒì¼ì„ ì¢…í•©í•˜ì—¬ ì´ íšŒì‚¬ì— ëŒ€í•œ ì‹¤ì œì ì´ê³  ìƒì„¸í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = get_ai_response(
            prompt=summary_prompt,
            model_name=model_name,
            system_prompt=summary_agent['system_prompt'],
            enable_thinking=False
        )
        
        return {
            'content': response['content'],
            'success': response['success'],
            'error': response.get('error', None)
        }
        
    except Exception as e:
        return {
            'content': f"ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            'success': False,
            'error': str(e)
        }

def analyze_content_structure(content_data):
    """ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„"""
    try:
        structure_analysis = {
            'total_pages': len(content_data.get('scraped_pages', [])),
            'pages_by_type': {},
            'content_distribution': {},
            'key_topics': [],
            'content_quality': {}
        }
        
        # í˜ì´ì§€ë³„ ì½˜í…ì¸  ë¶„í¬ ë¶„ì„
        for page in content_data.get('scraped_pages', []):
            page_url = page['url']
            page_title = page['title']
            
            # í˜ì´ì§€ ìœ í˜• ë¶„ë¥˜
            if 'about' in page_url.lower() or 'íšŒì‚¬' in page_title or 'about' in page_title.lower():
                structure_analysis['pages_by_type']['íšŒì‚¬ì†Œê°œ'] = structure_analysis['pages_by_type'].get('íšŒì‚¬ì†Œê°œ', 0) + 1
            elif 'product' in page_url.lower() or 'ì œí’ˆ' in page_title or 'product' in page_title.lower():
                structure_analysis['pages_by_type']['ì œí’ˆ/ì„œë¹„ìŠ¤'] = structure_analysis['pages_by_type'].get('ì œí’ˆ/ì„œë¹„ìŠ¤', 0) + 1
            elif 'contact' in page_url.lower() or 'ë¬¸ì˜' in page_title or 'contact' in page_title.lower():
                structure_analysis['pages_by_type']['ì—°ë½ì²˜'] = structure_analysis['pages_by_type'].get('ì—°ë½ì²˜', 0) + 1
            elif 'news' in page_url.lower() or 'ë‰´ìŠ¤' in page_title or 'news' in page_title.lower():
                structure_analysis['pages_by_type']['ë‰´ìŠ¤/ì†Œì‹'] = structure_analysis['pages_by_type'].get('ë‰´ìŠ¤/ì†Œì‹', 0) + 1
            else:
                structure_analysis['pages_by_type']['ê¸°íƒ€'] = structure_analysis['pages_by_type'].get('ê¸°íƒ€', 0) + 1
        
        # ì½˜í…ì¸  ë¶„í¬ ë¶„ì„
        total_words = content_data.get('word_count', 0)
        total_headings = len(content_data.get('headings', []))
        total_paragraphs = len(content_data.get('paragraphs', []))
        
        structure_analysis['content_distribution'] = {
            'í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´': total_words // len(content_data.get('scraped_pages', [])),
            'í‰ê·  í˜ì´ì§€ë‹¹ í—¤ë”©': total_headings // len(content_data.get('scraped_pages', [])),
            'í‰ê·  í˜ì´ì§€ë‹¹ ë¬¸ë‹¨': total_paragraphs // len(content_data.get('scraped_pages', []))
        }
        
        # ì£¼ìš” í† í”½ ì¶”ì¶œ (í—¤ë”© ê¸°ë°˜)
        all_headings = [h['text'] for h in content_data.get('headings', [])]
        topic_keywords = ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ì†”ë£¨ì…˜', 'ê¸°ìˆ ', 'íšŒì‚¬', 'ê³ ê°', 'ì‹œì¥', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'í˜ì‹ ', 'í’ˆì§ˆ']
        
        for keyword in topic_keywords:
            count = sum(1 for heading in all_headings if keyword in heading)
            if count > 0:
                structure_analysis['key_topics'].append({
                    'topic': keyword,
                    'frequency': count
                })
        
        # ì½˜í…ì¸  í’ˆì§ˆ í‰ê°€
        avg_words_per_page = total_words // len(content_data.get('scraped_pages', []))
        if avg_words_per_page > 500:
            quality_grade = "ìš°ìˆ˜"
        elif avg_words_per_page > 200:
            quality_grade = "ë³´í†µ"
        else:
            quality_grade = "ë¶€ì¡±"
        
        structure_analysis['content_quality'] = {
            'í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´': avg_words_per_page,
            'í’ˆì§ˆ ë“±ê¸‰': quality_grade,
            'ì½˜í…ì¸  ì™„ì„±ë„': f"{len(content_data.get('scraped_pages', []))}ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ"
        }
        
        return structure_analysis
        
    except Exception as e:
        st.warning(f"ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_product_solution_info(soup, page_url, all_content):
    """ì œí’ˆ ë° ì†”ë£¨ì…˜ ê´€ë ¨ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
    try:
        # ì œí’ˆ ê´€ë ¨ í‚¤ì›Œë“œë“¤
        product_keywords = ['product', 'ì œí’ˆ', 'ìƒí’ˆ', 'item', 'goods', 'solution', 'ì†”ë£¨ì…˜', 'service', 'ì„œë¹„ìŠ¤']
        tech_keywords = ['technology', 'ê¸°ìˆ ', 'tech', 'specification', 'ì‚¬ì–‘', 'ê¸°ëŠ¥', 'feature', 'íŠ¹ì§•']
        price_keywords = ['price', 'ê°€ê²©', 'cost', 'ë¹„ìš©', 'pricing', 'ìš”ê¸ˆ', 'quote', 'ê²¬ì ']
        
        # í˜ì´ì§€ URLê³¼ ì œëª©ì—ì„œ ì œí’ˆ/ì†”ë£¨ì…˜ ê´€ë ¨ì„± í™•ì¸
        page_lower = page_url.lower()
        page_title = soup.title.string.lower() if soup.title else ""
        
        # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
        product_info = {
            'url': page_url,
            'title': soup.title.string if soup.title else '',
            'product_name': '',
            'product_description': '',
            'product_features': [],
            'product_specs': {},
            'product_category': '',
            'target_audience': ''
        }
        
        # ì œí’ˆëª… ì¶”ì¶œ (h1, h2 íƒœê·¸ì—ì„œ)
        for tag in ['h1', 'h2']:
            headings = soup.find_all(tag)
            for heading in headings:
                heading_text = heading.get_text(strip=True)
                if any(keyword in heading_text.lower() for keyword in product_keywords):
                    product_info['product_name'] = heading_text
                    break
            if product_info['product_name']:
                break
        
        # ì œí’ˆ ì„¤ëª… ì¶”ì¶œ (p íƒœê·¸ì—ì„œ)
        paragraphs = soup.find_all('p')
        product_descriptions = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 20 and any(keyword in text.lower() for keyword in product_keywords):
                product_descriptions.append(text)
        
        if product_descriptions:
            product_info['product_description'] = ' '.join(product_descriptions[:3])  # ì²˜ìŒ 3ê°œ ë¬¸ë‹¨
        
        # ì œí’ˆ ê¸°ëŠ¥ ì¶”ì¶œ (li íƒœê·¸ì—ì„œ)
        lists = soup.find_all(['ul', 'ol'])
        for lst in lists:
            items = lst.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if any(keyword in text.lower() for keyword in tech_keywords + ['ê¸°ëŠ¥', 'feature', 'íŠ¹ì§•']):
                    product_info['product_features'].append(text)
        
        # ê¸°ìˆ  ì‚¬ì–‘ ì¶”ì¶œ (í…Œì´ë¸”ì´ë‚˜ íŠ¹ì • í˜•ì‹ì—ì„œ)
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if any(keyword in key.lower() for keyword in tech_keywords):
                        product_info['product_specs'][key] = value
        
        # ê°€ê²© ì •ë³´ ì¶”ì¶œ
        price_elements = soup.find_all(text=re.compile(r'[\$â‚¬Â¥â‚©]?\d+[,\d]*\.?\d*'))
        for element in price_elements:
            if any(keyword in str(element).lower() for keyword in price_keywords):
                all_content['pricing_info'].append({
                    'url': page_url,
                    'price_text': str(element),
                    'context': element.parent.get_text(strip=True) if element.parent else ''
                })
        
        # ì œí’ˆ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        if (product_info['product_name'] or product_info['product_description'] or 
            product_info['product_features'] or product_info['product_specs']):
            all_content['product_info'].append(product_info)
        
        # ì†”ë£¨ì…˜ ì •ë³´ ì¶”ì¶œ
        solution_info = {
            'url': page_url,
            'title': soup.title.string if soup.title else '',
            'solution_name': '',
            'solution_description': '',
            'solution_benefits': [],
            'use_cases': [],
            'target_industry': ''
        }
        
        # ì†”ë£¨ì…˜ëª… ì¶”ì¶œ
        for tag in ['h1', 'h2', 'h3']:
            headings = soup.find_all(tag)
            for heading in headings:
                heading_text = heading.get_text(strip=True)
                if any(keyword in heading_text.lower() for keyword in ['solution', 'ì†”ë£¨ì…˜', 'service', 'ì„œë¹„ìŠ¤']):
                    solution_info['solution_name'] = heading_text
                    break
            if solution_info['solution_name']:
                break
        
        # ì†”ë£¨ì…˜ ì„¤ëª… ì¶”ì¶œ
        solution_descriptions = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 20 and any(keyword in text.lower() for keyword in ['solution', 'ì†”ë£¨ì…˜', 'service', 'ì„œë¹„ìŠ¤']):
                solution_descriptions.append(text)
        
        if solution_descriptions:
            solution_info['solution_description'] = ' '.join(solution_descriptions[:3])
        
        # ì†”ë£¨ì…˜ í˜œíƒ ì¶”ì¶œ
        for lst in lists:
            items = lst.find_all('li')
            for item in items:
                text = item.get_text(strip=True)
                if any(keyword in text.lower() for keyword in ['benefit', 'í˜œíƒ', 'advantage', 'ì¥ì ', 'íš¨ê³¼']):
                    solution_info['solution_benefits'].append(text)
        
        # ì†”ë£¨ì…˜ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
        if (solution_info['solution_name'] or solution_info['solution_description'] or 
            solution_info['solution_benefits']):
            all_content['solution_info'].append(solution_info)
        
        # ê¸°ìˆ  ì‚¬ì–‘ ì •ë³´ ì¶”ê°€
        if product_info['product_specs']:
            all_content['technical_specs'].append({
                'url': page_url,
                'specs': product_info['product_specs']
            })
        
        # ê¸°ëŠ¥ ì •ë³´ ì¶”ê°€
        if product_info['product_features']:
            all_content['features_info'].append({
                'url': page_url,
                'features': product_info['product_features']
            })
            
    except Exception as e:
        st.warning(f"ì œí’ˆ/ì†”ë£¨ì…˜ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

def extract_page_content(soup, page_url):
    """ê°œë³„ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ"""
    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
    
    page_data = {
        'url': page_url,
        'title': soup.title.string if soup.title else '',
        'headings': [],
        'paragraphs': [],
        'lists': [],
        'text_content': ''
    }
    
    # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        headings = soup.find_all(tag)
        for heading in headings:
            text = heading.get_text(strip=True)
            if text:
                page_data['headings'].append({
                    'tag': tag,
                    'text': text,
                    'level': int(tag[1])
                })
    
    # ë¬¸ë‹¨ ì¶”ì¶œ
    paragraphs = soup.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ
            page_data['paragraphs'].append(text)
    
    # ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    lists = soup.find_all(['ul', 'ol'])
    for lst in lists:
        items = lst.find_all('li')
        list_items = []
        for item in items:
            text = item.get_text(strip=True)
            if text:
                list_items.append(text)
        if list_items:
            page_data['lists'].append({
                'type': lst.name,
                'items': list_items
            })
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
    text_content = soup.get_text()
    text_content = ' '.join(text_content.split())
    page_data['text_content'] = text_content
    
    return page_data

def display_file_preview(file_data, file_type, filename):
    """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ (PDF í˜ì´ì§• í¬í•¨)"""
    import base64
    import tempfile
    from PyPDF2 import PdfReader, PdfWriter
    import os
    try:
        file_type_lower = file_type.lower()
        if file_type_lower in ['jpg', 'jpeg', 'png', 'gif']:
            if file_data.get('binary_data'):
                st.image(
                    file_data['binary_data'],
                    caption=f"ğŸ–¼ï¸ {filename}",
                    use_container_width=True
                )
                return True
        elif file_type_lower == 'pdf':
            if file_data.get('binary_data'):
                key = f"pdf_preview_page_start_{filename}"
                # PDF ì „ì²´ í˜ì´ì§€ ìˆ˜ êµ¬í•˜ê¸°
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_in:
                    tmp_in.write(file_data['binary_data'])
                    tmp_in_path = tmp_in.name
                reader = PdfReader(tmp_in_path)
                total_pages = len(reader.pages)
                os.unlink(tmp_in_path)
                if key not in st.session_state:
                    st.session_state[key] = 0  # 0-based index
                page_start = st.session_state[key]
                page_end = min(page_start + 1, total_pages)
                col_prev, col_next = st.columns([1, 1])
                with col_prev:
                    if st.button("â¬…ï¸ ì´ì „", disabled=page_start == 0, key=f"prev_{filename}"):
                        st.session_state[key] = max(0, page_start - 1)
                        st.rerun()
                with col_next:
                    if st.button("ë‹¤ìŒ â¡ï¸", disabled=page_end >= total_pages, key=f"next_{filename}"):
                        st.session_state[key] = min(total_pages - 1, page_start + 1)
                        st.rerun()
                # ë¯¸ë¦¬ë³´ê¸° PDF ìƒì„± (1í˜ì´ì§€)
                try:
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_in:
                        tmp_in.write(file_data['binary_data'])
                        tmp_in_path = tmp_in.name
                    reader = PdfReader(tmp_in_path)
                    writer = PdfWriter()
                    for i in range(page_start, page_end):
                        writer.add_page(reader.pages[i])
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_out:
                        writer.write(tmp_out)
                        tmp_out_path = tmp_out.name
                    with open(tmp_out_path, "rb") as f:
                        preview_pdf_bytes = f.read()
                    os.unlink(tmp_in_path)
                    os.unlink(tmp_out_path)
                    st.markdown(f"**í˜ì´ì§€ {page_start+1} / {total_pages}**")
                    pdf_base64 = base64.b64encode(preview_pdf_bytes).decode('utf-8')
                    pdf_display = f"""
                    <iframe src=\"data:application/pdf;base64,{pdf_base64}\" 
                            width=\"100%\" height=\"600px\" type=\"application/pdf\">
                        <p>PDFë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
                        <a href=\"data:application/pdf;base64,{pdf_base64}\" target=\"_blank\">
                        ì—¬ê¸°ë¥¼ í´ë¦­í•˜ì—¬ ìƒˆ íƒ­ì—ì„œ ì—´ì–´ë³´ì„¸ìš”.</a></p>
                    </iframe>
                    """
                    st.markdown(pdf_display, unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                st.download_button(
                    label="ğŸ’¾ ì „ì²´ PDF ë‹¤ìš´ë¡œë“œ",
                    data=file_data['binary_data'],
                    file_name=filename,
                    mime="application/pdf"
                )
                return True
        elif file_type_lower in ['txt', 'md']:
            if file_data.get('binary_data'):
                try:
                    text_content = file_data['binary_data'].decode('utf-8')
                    st.subheader(f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°: {filename}")
                    st.text_area(
                        "íŒŒì¼ ë‚´ìš©",
                        value=text_content,
                        height=400,
                        disabled=True
                    )
                    return True
                except Exception as e:
                    st.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    return False
        else:
            st.info(f"{file_type.upper()} íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œ í›„ í™•ì¸í•˜ì„¸ìš”.")
            return False
    except Exception as e:
        st.error(f"íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {str(e)}")
        return False

def main():
    # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
    create_website_analysis_tables()
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ìƒˆ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„", "ì €ì¥ëœ ë¶„ì„ ì¡°íšŒ"])
    
    with tab1:
        st.header("ğŸ¢ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„")
        
        # URL ì…ë ¥
        url = st.text_input("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://example.com")
        
        # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
        st.subheader("ğŸ“ ê´€ë ¨ íŒŒì¼ ì—…ë¡œë“œ (ì„ íƒì‚¬í•­)")
        st.info("ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ê³¼ í•¨ê»˜ ì°¸ê³ í•  íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        uploaded_files = st.file_uploader(
            "ğŸ“ íŒŒì¼ ì„ íƒ (PDF, DOCX, PPTX, XLSX, Markdown, txt, jpeg, png)",
            type=['pdf', 'docx', 'pptx', 'xlsx', 'md', 'txt', 'jpg', 'jpeg', 'png'],
            accept_multiple_files=True,
            help="ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ê³¼ í•¨ê»˜ ì°¸ê³ í•  íŒŒì¼ë“¤ì„ ì—…ë¡œë“œí•˜ì„¸ìš”. ìµœëŒ€ 10ê°œ íŒŒì¼ê¹Œì§€ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
        )
        
        # ì—…ë¡œë“œëœ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ë° íŒŒì‹±
        if uploaded_files:
            st.write(f"**ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼ ({len(uploaded_files)}ê°œ):**")
            files_content = []
            for i, uploaded_file in enumerate(uploaded_files):
                st.write(f"{i+1}. {uploaded_file.name} ({uploaded_file.size:,} bytes)")
                file_data = parse_uploaded_file(uploaded_file)
                if file_data:
                    files_content.append(file_data)
            st.session_state['uploaded_files_data'] = files_content
        else:
            st.session_state['uploaded_files_data'] = []
        
        if url:
            # ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘
            with st.spinner("ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                content_data = scrape_website_content(url)
                
                # í•­ìƒ ì²¨ë¶€ íŒŒì¼ì„ content_dataì— í¬í•¨
                content_data['uploaded_files'] = st.session_state.get('uploaded_files_data', [])
                
                if content_data:
                    st.success("âœ… ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì œëª©", content_data['title'][:50] + "..." if len(content_data['title']) > 50 else content_data['title'])
                        st.metric("ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€", f"{len(content_data['scraped_pages'])}ê°œ")
                        st.metric("ë‹¨ì–´ ìˆ˜", f"{content_data['word_count']:,}")
                    with col2:
                        st.metric("ë¬¸ì¥ ìˆ˜", f"{content_data['sentence_count']:,}")
                        st.metric("ë¬¸ë‹¨ ìˆ˜", f"{content_data['paragraph_count']:,}")
                        st.metric("í—¤ë”© ìˆ˜", len(content_data['headings']))
                    with col3:
                        st.metric("ë¦¬ìŠ¤íŠ¸ ìˆ˜", len(content_data['lists']))
                        st.metric("ì‘ë‹µ ì‹œê°„", f"{content_data['response_time']:.2f}ì´ˆ")
                        st.metric("ìƒíƒœ ì½”ë“œ", content_data['status_code'])
                    with col4:
                        st.metric("ì´ í˜ì´ì§€", content_data['total_pages'])
                        st.metric("í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´", f"{content_data['word_count'] // len(content_data['scraped_pages']):,}")
                        st.metric("ë°ì´í„° í’ˆì§ˆ", "ğŸŸ¢ ìš°ìˆ˜" if content_data['word_count'] > 1000 else "ğŸŸ¡ ë³´í†µ" if content_data['word_count'] > 500 else "ğŸ”´ ë¶€ì¡±")
                    
                    # ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„
                    structure_analysis = analyze_content_structure(content_data)
                    if structure_analysis:
                        st.subheader("ğŸ“Š ì½˜í…ì¸  êµ¬ì¡° ë¶„ì„")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("í˜ì´ì§€ ìœ í˜• ë¶„í¬", f"{len(structure_analysis['pages_by_type'])}ê°œ ìœ í˜•")
                            for page_type, count in structure_analysis['pages_by_type'].items():
                                st.write(f"â€¢ {page_type}: {count}ê°œ")
                        with col2:
                            st.metric("í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´", f"{structure_analysis['content_distribution']['í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´']:,}ê°œ")
                            st.metric("í‰ê·  í˜ì´ì§€ë‹¹ í—¤ë”©", f"{structure_analysis['content_distribution']['í‰ê·  í˜ì´ì§€ë‹¹ í—¤ë”©']}ê°œ")
                        with col3:
                            st.metric("ì½˜í…ì¸  í’ˆì§ˆ", structure_analysis['content_quality']['í’ˆì§ˆ ë“±ê¸‰'])
                            st.metric("ì£¼ìš” í† í”½", f"{len(structure_analysis['key_topics'])}ê°œ")
                    
                    # ê°€ë…ì„± ë¶„ì„
                    readability_data = analyze_content_readability(content_data['text_content'])
                    if readability_data:
                        st.subheader("ğŸ“– ê°€ë…ì„± ë¶„ì„")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{readability_data['avg_sentence_length']:.1f}ë‹¨ì–´")
                        with col2:
                            st.metric("ê¸´ ë¬¸ì¥ ë¹„ìœ¨", f"{readability_data['long_sentence_ratio']}%")
                        with col3:
                            st.metric("ê°€ë…ì„± ë“±ê¸‰", readability_data['readability_grade'])
                    
                    # ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ë¶„ì„
                    st.subheader("ğŸ“‹ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë° ì²¨ë¶€ íŒŒì¼ ìš”ì•½")
                    
                    # ì²¨ë¶€ íŒŒì¼ ì •ë³´ í‘œì‹œ
                    if uploaded_files:
                        st.info(f"ğŸ” ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ì™€ ì²¨ë¶€ëœ {len(uploaded_files)}ê°œ íŒŒì¼ì„ ì¢…í•©í•˜ì—¬ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì™€ ì œí’ˆì— ëŒ€í•œ ì‹¤ì œì ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    else:
                        st.info("ğŸ” ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ì¢…í•©í•˜ì—¬ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì™€ ì œí’ˆì— ëŒ€í•œ ì‹¤ì œì ì¸ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.")
                    
                    # ìš”ì•½ ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                    if st.button("ğŸ“‹ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë° ì²¨ë¶€ íŒŒì¼ ìš”ì•½ ìƒì„±", type="primary"):
                        with st.spinner("ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ì™€ ì²¨ë¶€ íŒŒì¼ì„ ì¢…í•©í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                            # í•­ìƒ ì²¨ë¶€ íŒŒì¼ì„ content_dataì— í¬í•¨
                            content_data['uploaded_files'] = st.session_state.get('uploaded_files_data', [])
                            # ì½˜í…ì¸  ìš”ì•½ ë¶„ì„ ì‹¤í–‰
                            content_summary = analyze_website_content_summary(url, content_data, "gpt-4o-mini")
                            
                            if content_summary['success']:
                                st.success("âœ… ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ì™„ë£Œ")
                                
                                # ìš”ì•½ ê²°ê³¼ í‘œì‹œ
                                st.markdown("## ğŸ“‹ **ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ìš”ì•½**")
                                st.markdown(content_summary['content'])
                                
                                # ìš”ì•½ ê²°ê³¼ë¥¼ ì„¸ì…˜ì— ì €ì¥
                                st.session_state['content_summary'] = content_summary
                                
                                # ì½˜í…ì¸  ìš”ì•½ì„ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ë°ì´í„°ì— ì¶”ê°€
                                content_data['perplexity_summary'] = content_summary['content']
                            else:
                                st.error(f"âŒ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ì‹¤íŒ¨: {content_summary['error']}")
                    
                    # ì´ì „ì— ìƒì„±ëœ ìš”ì•½ì´ ìˆìœ¼ë©´ í‘œì‹œ
                    if 'content_summary' in st.session_state and st.session_state['content_summary']['success']:
                        st.markdown("## ğŸ“‹ **ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ìš”ì•½**")
                        st.markdown(st.session_state['content_summary']['content'])
                        
                        # ì½˜í…ì¸  ìš”ì•½ì„ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ìš© ë°ì´í„°ì— ì¶”ê°€
                        content_data['perplexity_summary'] = st.session_state['content_summary']['content']
                    
                    # ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°
                    with st.expander("ğŸ“‹ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        st.write("**ë©”íƒ€ ì„¤ëª…:**")
                        st.write(content_data['meta_description'] if content_data['meta_description'] else "ë©”íƒ€ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ëª©ë¡
                        st.write("**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ëª©ë¡:**")
                        for i, page in enumerate(content_data['scraped_pages'][:10]):  # ì²˜ìŒ 10ê°œë§Œ
                            st.write(f"{i+1}. {page['url']} - {page['title'][:50]}...")
                        
                        if len(content_data['scraped_pages']) > 10:
                            st.write(f"... ì™¸ {len(content_data['scraped_pages']) - 10}ê°œ í˜ì´ì§€")
                        
                        # ì œí’ˆ ì •ë³´ í‘œì‹œ
                        if content_data.get('product_info'):
                            st.write("**ğŸ” ë°œê²¬ëœ ì œí’ˆ ì •ë³´:**")
                            for i, product in enumerate(content_data['product_info'][:5]):  # ì²˜ìŒ 5ê°œë§Œ
                                st.write(f"**ì œí’ˆ {i+1}:** {product.get('product_name', 'N/A')}")
                                if product.get('product_description'):
                                    st.write(f"  ì„¤ëª…: {product['product_description'][:100]}...")
                                if product.get('product_features'):
                                    st.write(f"  ê¸°ëŠ¥: {', '.join(product['product_features'][:3])}")
                        
                        # ì†”ë£¨ì…˜ ì •ë³´ í‘œì‹œ
                        if content_data.get('solution_info'):
                            st.write("**ğŸ” ë°œê²¬ëœ ì†”ë£¨ì…˜ ì •ë³´:**")
                            for i, solution in enumerate(content_data['solution_info'][:5]):  # ì²˜ìŒ 5ê°œë§Œ
                                st.write(f"**ì†”ë£¨ì…˜ {i+1}:** {solution.get('solution_name', 'N/A')}")
                                if solution.get('solution_description'):
                                    st.write(f"  ì„¤ëª…: {solution['solution_description'][:100]}...")
                                if solution.get('solution_benefits'):
                                    st.write(f"  í˜œíƒ: {', '.join(solution['solution_benefits'][:3])}")
                        
                        # ê°€ê²© ì •ë³´ í‘œì‹œ
                        if content_data.get('pricing_info'):
                            st.write("**ğŸ’° ë°œê²¬ëœ ê°€ê²© ì •ë³´:**")
                            for price in content_data['pricing_info'][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                                st.write(f"- {price.get('price_text', 'N/A')}")
                        
                        st.write("**ì£¼ìš” í—¤ë”© (ì „ì²´ í˜ì´ì§€ í†µí•©):**")
                        for heading in content_data['headings'][:15]:  # ì²˜ìŒ 15ê°œë§Œ
                            st.write(f"{'  ' * (heading['level'] - 1)}â€¢ {heading['text']}")
                        
                        st.write("**ì½˜í…ì¸  ìƒ˜í”Œ (ì „ì²´ í˜ì´ì§€ í†µí•©, ì²˜ìŒ 1000ì):**")
                        st.write(content_data['text_content'][:1000] + "..." if len(content_data['text_content']) > 1000 else content_data['text_content'])
                        
                        # ì œí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
                        product_keywords = ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ì†”ë£¨ì…˜', 'ìƒí’ˆ', 'ê¸°ëŠ¥', 'íŠ¹ì§•', 'ê°€ê²©', 'êµ¬ë§¤', 'ì£¼ë¬¸', 'ë¬¸ì˜']
                        highlighted_content = content_data['text_content'][:2000]
                        for keyword in product_keywords:
                            if keyword in highlighted_content:
                                st.write(f"**ğŸ” ë°œê²¬ëœ í‚¤ì›Œë“œ:** {keyword}")
                        
                        st.write("**ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„:**")
                        st.write(f"- ì´ ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€: {len(content_data['scraped_pages'])}ê°œ")
                        st.write(f"- í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´ ìˆ˜: {content_data['word_count'] // len(content_data['scraped_pages']):,}ê°œ")
                        st.write(f"- ì œí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ í—¤ë”©: {len([h for h in content_data['headings'] if any(kw in h['text'] for kw in product_keywords)])}ê°œ")
                        st.write(f"- ë°œê²¬ëœ ì œí’ˆ ìˆ˜: {len(content_data.get('product_info', []))}ê°œ")
                        st.write(f"- ë°œê²¬ëœ ì†”ë£¨ì…˜ ìˆ˜: {len(content_data.get('solution_info', []))}ê°œ")
                        st.write(f"- ë°œê²¬ëœ ê°€ê²© ì •ë³´: {len(content_data.get('pricing_info', []))}ê°œ")
                    
                    # AI ì—ì´ì „íŠ¸ ì„¤ì •
                    st.header("ğŸ¤– AI ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€")
                    st.subheader("í™œì„±í™”í•  ì „ë¬¸ê°€")
                    
                    # ì½˜í…ì¸  ìš”ì•½ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ í™•ì¸
                    content_summary_available = 'content_summary' in st.session_state and st.session_state['content_summary']['success']
                    
                    if not content_summary_available:
                        st.warning("âš ï¸ ë¨¼ì € ìœ„ì˜ 'ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ìƒì„±' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì½˜í…ì¸  ìš”ì•½ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”.")
                        st.info("ğŸ“‹ ì½˜í…ì¸  ìš”ì•½ì´ ì™„ë£Œë˜ë©´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    else:
                        st.success("âœ… ì½˜í…ì¸  ìš”ì•½ì´ ì™„ë£Œë˜ì–´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        
                        default_agents = ["business_model_analyzer", "product_service_analyzer", "market_position_analyzer"]
                        selected_agents = []
                        
                        cols = st.columns(3)
                        for i, (agent_key, agent_info) in enumerate(BUSINESS_ANALYSIS_AGENTS.items()):
                            with cols[i % 3]:
                                is_selected = st.checkbox(
                                    f"{agent_info['emoji']} **{agent_info['name']}**",
                                    value=(agent_key in default_agents),
                                    help=agent_info['description'],
                                    key=f"business_agent_{agent_key}"
                                )
                                if is_selected:
                                    selected_agents.append(agent_key)
                        
                        if len(selected_agents) == 0:
                            st.warning("âš ï¸ ìµœì†Œ 1ëª…ì˜ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                        else:
                            st.success(f"âœ… {len(selected_agents)}ëª…ì˜ ì „ë¬¸ê°€ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
                            # ëª¨ë¸ ì„ íƒ
                            model_name = st.selectbox(
                                "ì‚¬ìš©í•  AI ëª¨ë¸",
                                ["gpt-4o-mini", "gpt-4", "claude-3-5-sonnet-20241022"],
                                index=0
                            )
                            
                            # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                            if st.button("ğŸš€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¢…í•© ë¶„ì„ ì‹¤í–‰"):
                                with st.spinner("AI ì „ë¬¸ê°€ë“¤ì´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                    # ê°€ë…ì„± ë°ì´í„° ì¶”ê°€
                                    if readability_data:
                                        content_data['readability'] = readability_data
                                    
                                    # ì½˜í…ì¸  ìš”ì•½ ë°ì´í„° ì¶”ê°€
                                    content_data['content_summary'] = st.session_state['content_summary']['content']
                                    
                                    # ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰
                                    agent_analyses, business_analysis = run_business_multi_agent_analysis(
                                        url, 
                                        content_data, 
                                        selected_agents, 
                                        model_name
                                    )
                                    
                                    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                                    st.success("âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì¢…í•© ë¶„ì„ ì™„ë£Œ**")
                                    
                                    # ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                                    st.markdown("## ğŸ“‹ ì „ë¬¸ê°€ë³„ ë¶„ì„ ê²°ê³¼")
                                    for analysis in agent_analyses:
                                        if analysis['success']:
                                            with st.expander(f"{analysis['agent_emoji']} {analysis['agent_name']} ë¶„ì„", expanded=False):
                                                display_analysis_with_plots(analysis['analysis'], f"{analysis['agent_emoji']} {analysis['agent_name']} ë¶„ì„", unique_id=analysis['agent_key'])
                                        else:
                                            st.error(f"âŒ {analysis['agent_emoji']} {analysis['agent_name']}: {analysis['error']}")
                                    
                                    # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ í‘œì‹œ
                                    st.markdown("## ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸")
                                    if business_analysis['success']:
                                        display_analysis_with_plots(business_analysis['content'], "ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸", unique_id="business_report")
                                    else:
                                        st.error(f"ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {business_analysis['error']}")
                                    
                                    # ë¶„ì„ ê²°ê³¼ ì €ì¥
                                    analysis_date = datetime.now()
                                    
                                    # ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°
                                    uploaded_files_data = st.session_state.get('uploaded_files_data', [])
                                    
                                    save_success, analysis_id = save_website_analysis(
                                        url, content_data, agent_analyses, business_analysis, analysis_date, uploaded_files_data
                                    )
                                    
                                    if save_success:
                                        st.success(f"âœ… ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {analysis_id})")
                                    else:
                                        st.warning("âš ï¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with tab2:
        st.header("ì €ì¥ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¡°íšŒ")
        
        # ì €ì¥ëœ ë¶„ì„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        saved_analyses = get_saved_website_analyses()
        
        if saved_analyses:
            for analysis in saved_analyses:
                # ìƒì„¸ë³´ê¸° ìƒíƒœë¥¼ ì„¸ì…˜ì— ì €ì¥
                if f"show_detail_{analysis['analysis_id']}" not in st.session_state:
                    st.session_state[f"show_detail_{analysis['analysis_id']}"] = False
                
                # ìƒì„¸ë³´ê¸°ê°€ í™œì„±í™”ëœ ê²½ìš° ì „ì²´ í™”ë©´ìœ¼ë¡œ í‘œì‹œ
                if st.session_state[f"show_detail_{analysis['analysis_id']}"]:
                    # ì „ì²´ í™”ë©´ ìƒì„¸ë³´ê¸°
                    st.markdown("---")
                    st.markdown("## ğŸ“Š **ì „ì²´ í™”ë©´ ìƒì„¸ ë¶„ì„**")
                    
                    # ë¶„ì„ ì •ë³´ ìš”ì•½
                    col_info1, col_info2, col_info3 = st.columns(3)
                    with col_info1:
                        st.metric("ë¶„ì„ ID", analysis['analysis_id'])
                        st.metric("ìƒíƒœ ì½”ë“œ", analysis['status_code'])
                    with col_info2:
                        st.metric("ì‘ë‹µ ì‹œê°„", f"{analysis['response_time']:.2f}ì´ˆ")
                        st.metric("í—¤ë”© ìˆ˜", analysis['headings_count'])
                    with col_info3:
                        st.metric("ë§í¬ ìˆ˜", analysis['links_count'])
                        st.metric("ë¶„ì„ ë‚ ì§œ", analysis['analysis_date'].strftime('%Y-%m-%d %H:%M'))
                    
                    detail = get_analysis_detail(analysis['analysis_id'])
                    if detail:
                        # íƒ­ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
                        tab_basic, tab_content, tab_agents, tab_report, tab_files = st.tabs([
                            "ğŸ“‹ ê¸°ë³¸ ì •ë³´", 
                            "ğŸ“„ ì½˜í…ì¸  ë°ì´í„°", 
                            "ğŸ¤– AI ë¶„ì„ ê²°ê³¼", 
                            "ğŸ“ˆ ì¢…í•© ë¦¬í¬íŠ¸",
                            "ğŸ“ ì²¨ë¶€ íŒŒì¼"
                        ])
                        
                        with tab_basic:
                            st.markdown("### ğŸ“‹ ê¸°ë³¸ ì •ë³´")
                            st.json(detail['base'])
                        
                        with tab_content:
                            st.markdown("### ğŸ“„ ì½˜í…ì¸  ë°ì´í„°")
                            st.json(detail['content'])
                            
                            # ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ ì •ë³´ í‘œì‹œ
                            if detail['content'] and detail['content'].get('perplexity_summary'):
                                st.markdown("### ğŸ“‹ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ìš”ì•½")
                                st.info("ğŸ” AIê°€ ìƒì„±í•œ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ì¢…í•© ìš”ì•½")
                                st.markdown(detail['content']['perplexity_summary'])
                            else:
                                st.info("ğŸ“ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        with tab_agents:
                            st.markdown("### ğŸ¤– **AI ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼**")
                            for i, agent in enumerate(detail['agents']):
                                agent_name = agent['agent_type'].replace('_', ' ').title()
                                st.markdown(f"#### ğŸ” {agent_name} ë¶„ì„")
                                display_analysis_with_plots(agent['analysis_content'], f"ğŸ” {agent_name} ë¶„ì„", unique_id=agent['agent_type'])
                                if i < len(detail['agents']) - 1:
                                    st.markdown("---")
                        
                        with tab_report:
                            st.markdown("### ğŸ“ˆ **ì¢…í•© ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸**")
                            if detail['cto']:
                                display_analysis_with_plots(detail['cto'], "ğŸ“ˆ ì¢…í•© ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸", unique_id=detail['base']['analysis_id'])
                            else:
                                st.info("ì¢…í•© ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        
                        with tab_files:
                            st.markdown("### ğŸ“ **ì²¨ë¶€ íŒŒì¼**")
                            if detail.get('files'):
                                st.write(f"**ì´ {len(detail['files'])}ê°œì˜ íŒŒì¼ì´ ì²¨ë¶€ë˜ì–´ ìˆìŠµë‹ˆë‹¤.**")
                                
                                for i, file_info in enumerate(detail['files']):
                                    col1, col2, col3 = st.columns([3, 1, 1])
                                    
                                    with col1:
                                        st.write(f"**{i+1}. {file_info['filename']}** ({file_info['file_type'].upper()})")
                                        st.caption(f"í¬ê¸°: {file_info['file_size']:,} bytes")
                                    
                                    with col2:
                                        if st.button(f"ğŸ‘ï¸ ë¯¸ë¦¬ë³´ê¸°", key=f"detail_preview_{file_info['file_id']}"):
                                            st.session_state.preview_file_id = file_info['file_id']
                                            st.session_state.preview_filename = file_info['filename']
                                            st.session_state.preview_file_type = file_info['file_type']
                                    
                                    with col3:
                                        file_data = get_file_binary_data(file_info['file_id'])
                                        if file_data:
                                            mime_type = get_file_mime_type(file_data['file_type'])
                                            st.download_button(
                                                label="ğŸ’¾ ë‹¤ìš´ë¡œë“œ",
                                                data=file_data['binary_data'],
                                                file_name=file_data['filename'],
                                                mime=mime_type,
                                                key=f"detail_download_{file_info['file_id']}"
                                            )
                                
                                # ì „ì²´ í™”ë©´ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
                                if st.session_state.get('preview_file_id'):
                                    st.markdown("---")
                                    st.markdown("## ï¿½ï¿½ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ì „ì²´ í™”ë©´)")
                                    file_id = st.session_state.preview_file_id
                                    col_preview_1, col_preview_2 = st.columns([10, 1])
                                    with col_preview_2:
                                        if st.button("âŒ ë‹«ê¸°", key=f"close_detail_preview_{file_id}"):
                                            del st.session_state.preview_file_id
                                            del st.session_state.preview_filename
                                            del st.session_state.preview_file_type
                                            st.rerun()
                                    with col_preview_1:
                                        file_data = get_file_binary_data(file_id)
                                        if file_data:
                                            display_file_preview(
                                                file_data, 
                                                st.session_state.preview_file_type, 
                                                st.session_state.preview_filename
                                            )
                                        else:
                                            st.error("íŒŒì¼ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            else:
                                st.info("ì²¨ë¶€ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ë‹«ê¸° ë²„íŠ¼
                        if st.button("âŒ ìƒì„¸ë³´ê¸° ë‹«ê¸°", key=f"close_detail_{analysis['analysis_id']}"):
                            st.session_state[f"show_detail_{analysis['analysis_id']}"] = False
                            st.rerun()
                
                else:
                    # ì¼ë°˜ ëª©ë¡ í‘œì‹œ
                    with st.expander(f"ğŸ“„ {analysis['url']} - {analysis['analysis_date']}", expanded=False):
                        col1, col2 = st.columns([3, 1])
                        
                        with col1:
                            st.write(f"**ì œëª©:** {analysis['title']}")
                            st.write(f"**ìƒíƒœ ì½”ë“œ:** {analysis['status_code']}")
                            st.write(f"**ì‘ë‹µ ì‹œê°„:** {analysis['response_time']:.2f}ì´ˆ")
                            st.write(f"**í—¤ë”© ìˆ˜:** {analysis['headings_count']}")
                            st.write(f"**ë§í¬ ìˆ˜:** {analysis['links_count']}")
                            
                        with col2:
                            detail_btn = st.button("ğŸ“‹ ìƒì„¸ë³´ê¸°", key=f"detail_{analysis['analysis_id']}")
                            
                            if detail_btn:
                                st.session_state[f"show_detail_{analysis['analysis_id']}"] = True
                                st.rerun()
                        # ì‚­ì œ ê¸°ëŠ¥ - íšŒì˜ë¡ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
                        st.markdown("### âš ï¸ ë¶„ì„ ê´€ë¦¬")
                        delete_button_key = f"delete_button_{analysis['analysis_id']}"
                        
                        # ì‚­ì œ í™•ì¸ì„ ìœ„í•œ ì²´í¬ë°•ìŠ¤
                        confirm_delete = st.checkbox(f"ì‚­ì œ í™•ì¸", key=f"confirm_{analysis['analysis_id']}")
                        
                        if confirm_delete:
                            if st.button("ğŸ—‘ï¸ ë¶„ì„ ì‚­ì œ", key=delete_button_key, type="primary", use_container_width=True):
                                if delete_analysis(analysis['analysis_id']):
                                    st.success("âœ… ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    st.rerun()
                                else:
                                    st.error("âŒ ë¶„ì„ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.button("ğŸ—‘ï¸ ë¶„ì„ ì‚­ì œ", key=delete_button_key, disabled=True, use_container_width=True)
                            st.caption("ì‚­ì œí•˜ë ¤ë©´ ë¨¼ì € 'ì‚­ì œ í™•ì¸' ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        else:
            st.info("ì €ì¥ëœ ì½˜í…ì¸  ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤.")

def parse_uploaded_file(uploaded_file):
    """ì—…ë¡œë“œëœ íŒŒì¼ íŒŒì‹±"""
    try:
        file_extension = uploaded_file.name.split('.')[-1].lower()
        content = ""
        
        uploaded_file.seek(0)
        binary_data = uploaded_file.read()
        binary_base64 = base64.b64encode(binary_data).decode('utf-8')
        
        uploaded_file.seek(0)
        
        if file_extension == 'pdf':
            pdf_reader = PyPDF2.PdfReader(uploaded_file)
            for page in pdf_reader.pages:
                content += page.extract_text() + "\n"
                
        elif file_extension == 'docx':
            doc = docx.Document(uploaded_file)
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
        
        elif file_extension == 'pptx':
            prs = Presentation(uploaded_file)
            for slide in prs.slides:
                for shape in slide.shapes:
                    if hasattr(shape, "text"):
                        content += shape.text + "\n"
        
        elif file_extension == 'xlsx':
            workbook = openpyxl.load_workbook(uploaded_file, data_only=True)
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                content += f"=== {sheet_name} ===\n"
                for row in sheet.iter_rows(values_only=True):
                    row_text = '\t'.join([str(cell) if cell is not None else '' for cell in row])
                    if row_text.strip():
                        content += row_text + "\n"
                content += "\n"
                
        elif file_extension in ['txt', 'md']:
            uploaded_file.seek(0)
            content = uploaded_file.read().decode('utf-8')
            
        elif file_extension in ['jpg', 'jpeg', 'png', 'gif']:
            content = f"[{file_extension.upper()} ì´ë¯¸ì§€ íŒŒì¼ - {uploaded_file.name}]"
            
        else:
            try:
                uploaded_file.seek(0)
                content = uploaded_file.read().decode('utf-8', errors='ignore')
                if not content.strip():
                    content = f"[{file_extension.upper()} íŒŒì¼ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€]"
            except:
                content = f"[{file_extension.upper()} íŒŒì¼ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë¶ˆê°€]"
            
        return {
            'filename': uploaded_file.name,
            'file_type': file_extension,
            'content': content,
            'binary_data': binary_base64,
            'size': len(binary_data)
        }
        
    except Exception as e:
        st.error(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        return None

def get_file_mime_type(file_type):
    """íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ MIME íƒ€ì… ë°˜í™˜"""
    mime_types = {
        'pdf': 'application/pdf',
        'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
        'pptx': 'application/vnd.openxmlformats-officedocument.presentationml.presentation',
        'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'txt': 'text/plain',
        'md': 'text/markdown',
        'jpg': 'image/jpeg',
        'jpeg': 'image/jpeg',
        'png': 'image/png',
        'gif': 'image/gif'
    }
    return mime_types.get(file_type.lower(), 'application/octet-stream')

def get_file_binary_data(file_id):
    """íŒŒì¼ì˜ ì›ë³¸ ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì¡°íšŒ"""
    conn = mysql.connector.connect(**db_config)
    cursor = conn.cursor(dictionary=True)
    try:
        cursor.execute(
            '''
            SELECT filename, file_type, file_binary_data, file_size
            FROM website_analysis_files
            WHERE file_id = %s
            ''',
            (file_id,)
        )
        result = cursor.fetchone()
        if result and result['file_binary_data']:
            binary_data = base64.b64decode(result['file_binary_data'])
            return {
                'filename': result['filename'],
                'file_type': result['file_type'],
                'binary_data': binary_data,
                'file_size': result['file_size']
            }
        return None
    except mysql.connector.Error as err:
        st.error(f"íŒŒì¼ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {err}")
        return None
    finally:
        cursor.close()
        conn.close()

if __name__ == "__main__":
    main() 