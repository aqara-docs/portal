from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOllama
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import Document
import streamlit as st
import os
import nltk
import pandas as pd
from pathlib import Path
import hashlib
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# NLTK punkt íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ
try:
    nltk.download('punkt', quiet=True)
    # NLTK ë°ì´í„° ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    nltk.data.path.append('/Users/aqaralife/nltk_data')
except Exception as e:
    logger.warning(f"NLTK ì„¤ì • ê²½ê³ : {e}")

st.set_page_config(
    page_title="ğŸ“„ File GPT",
    page_icon="ğŸ“„",
    layout="wide"
)

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ (í¬ê¸° ìˆœ)
AVAILABLE_MODELS = [
    {"name": "llama3.2:latest", "size": "2.0 GB"},
    {"name": "llama2:latest", "size": "3.8 GB"},
    {"name": "mistral:latest", "size": "4.1 GB"},
    {"name": "llama3.1:latest", "size": "4.9 GB"},
    {"name": "llama3.1:8b", "size": "4.9 GB"},
    {"name": "gemma:latest", "size": "5.0 GB"},
    {"name": "gemma2:latest", "size": "5.4 GB"},
    {"name": "deepseek-r1:14b", "size": "9.0 GB"},
    {"name": "phi4:latest", "size": "9.1 GB"},
    {"name": "deepseek-r1:32b", "size": "19 GB"},
    {"name": "llama3.3:latest", "size": "42 GB"}
]

# ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ê³¼ ìµœëŒ€ í¬ê¸° ì„¤ì •
SUPPORTED_FILE_TYPES = {
    "txt": {"max_size_mb": 10, "description": "í…ìŠ¤íŠ¸ íŒŒì¼"},
    "md": {"max_size_mb": 10, "description": "ë§ˆí¬ë‹¤ìš´ íŒŒì¼"},
    "pdf": {"max_size_mb": 50, "description": "PDF ë¬¸ì„œ"},
    "docx": {"max_size_mb": 50, "description": "Word ë¬¸ì„œ"},
    "csv": {"max_size_mb": 20, "description": "CSV íŒŒì¼"},
    "json": {"max_size_mb": 20, "description": "JSON íŒŒì¼"},
    "xml": {"max_size_mb": 20, "description": "XML íŒŒì¼"},
    "html": {"max_size_mb": 10, "description": "HTML íŒŒì¼"}
}

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("ğŸ”§ ëª¨ë¸ ì„¤ì •")
selected_model = st.sidebar.selectbox(
    "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
    options=[model["name"] for model in AVAILABLE_MODELS],
    format_func=lambda x: f"{x} ({next(m['size'] for m in AVAILABLE_MODELS if m['name'] == x)})",
    index=2  # mistral:latestë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
)

# Temperature ì„¤ì •
temperature = st.sidebar.slider(
    "Temperature:", 
    min_value=0.0, 
    max_value=2.0, 
    value=0.1, 
    step=0.1,
    help="ê°’ì´ ë†’ì„ìˆ˜ë¡ ë” ì°½ì˜ì ì¸ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."
)

# ê³ ê¸‰ ì„¤ì •
st.sidebar.title("âš™ï¸ ê³ ê¸‰ ì„¤ì •")
chunk_size = st.sidebar.slider(
    "ì²­í¬ í¬ê¸°:",
    min_value=200,
    max_value=1500,
    value=800,
    step=100,
    help="í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” ë‹¨ìœ„ í¬ê¸°ì…ë‹ˆë‹¤. í´ìˆ˜ë¡ ë§¥ë½ì´ ë” ìœ ì§€ë˜ì§€ë§Œ ì •í™•ë„ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
)

chunk_overlap = st.sidebar.slider(
    "ì²­í¬ ì˜¤ë²„ë©:",
    min_value=0,
    max_value=300,
    value=150,
    step=50,
    help="ì²­í¬ ê°„ ì¤‘ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ê¸¸ì´ì…ë‹ˆë‹¤. ì ì ˆí•œ ì˜¤ë²„ë©ì€ ë§¥ë½ ìœ ì§€ì— ë„ì›€ë©ë‹ˆë‹¤."
)

retrieval_k = st.sidebar.slider(
    "ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜:",
    min_value=3,
    max_value=10,
    value=5,
    help="ì§ˆë¬¸ ë‹µë³€ì‹œ ì°¸ì¡°í•  ê´€ë ¨ ë¬¸ì„œì˜ ê°œìˆ˜ì…ë‹ˆë‹¤."
)

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

# Ollama ëª¨ë¸ ì´ˆê¸°í™” í•¨ìˆ˜
@st.cache_resource
def get_llm_file(model_name, temp):
    return ChatOllama(
        model=model_name,
        temperature=temp,
        streaming=True,
        callbacks=[ChatCallbackHandler()],
    )

def validate_file(file):
    """íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    errors = []
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    file_ext = file.name.split('.')[-1].lower()
    if file_ext not in SUPPORTED_FILE_TYPES:
        errors.append(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_ext}")
        return errors
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    file_size_mb = file.size / (1024 * 1024)
    max_size = SUPPORTED_FILE_TYPES[file_ext]["max_size_mb"]
    if file_size_mb > max_size:
        errors.append(f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size_mb:.1f}MB (ìµœëŒ€: {max_size}MB)")
    
    # íŒŒì¼ëª… ê¸¸ì´ í™•ì¸
    if len(file.name) > 255:
        errors.append("íŒŒì¼ëª…ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ (ìµœëŒ€ 255ì)")
    
    return errors

def get_file_hash(file_content):
    """íŒŒì¼ í•´ì‹œ ìƒì„±"""
    return hashlib.md5(file_content).hexdigest()

def analyze_file_content(file_path):
    """íŒŒì¼ ë‚´ìš© ë¶„ì„"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except UnicodeDecodeError:
        try:
            with open(file_path, 'r', encoding='latin-1') as f:
                content = f.read()
        except Exception as e:
            return {"error": f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}"}
    except Exception as e:
        return {"error": f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {e}"}
    
    # ê¸°ë³¸ í†µê³„
    lines = content.split('\n')
    words = content.split()
    
    return {
        "char_count": len(content),
        "word_count": len(words),
        "line_count": len(lines),
        "avg_line_length": sum(len(line) for line in lines) / len(lines) if lines else 0,
        "preview": content[:500] + "..." if len(content) > 500 else content
    }

@st.cache_data(show_spinner="íŒŒì¼ì„ ì„ë² ë”©í•˜ëŠ” ì¤‘...")
def embed_files(files, model_name, chunk_size_val, chunk_overlap_val, retrieval_k_val):
    """ê°œì„ ëœ íŒŒì¼ ì„ë² ë”© í•¨ìˆ˜"""
    # ë…ë¦½ì ì¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì‚¬ìš©
    directory = "./file_uploads/"
    embeddings_dir = "./file_embeddings/"
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    for dir_path in [directory, embeddings_dir]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)
    
    all_docs = []
    file_info = []
    processing_errors = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_files = len(files)
    
    for idx, file in enumerate(files):
        try:
            status_text.text(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file.name} ({idx + 1}/{total_files})")
            
            # íŒŒì¼ ì €ì¥
            file_path = os.path.join(directory, file.name)
            file_content = file.read()
            
            with open(file_path, "wb") as f:
                f.write(file_content)
            
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            file_hash = get_file_hash(file_content)
            file_analysis = analyze_file_content(file_path)
            
            if "error" in file_analysis:
                processing_errors.append(f"{file.name}: {file_analysis['error']}")
                continue
            
            file_info.append({
                "name": file.name,
                "size_mb": file.size / (1024 * 1024),
                "hash": file_hash,
                "char_count": file_analysis["char_count"],
                "word_count": file_analysis["word_count"],
                "line_count": file_analysis["line_count"]
            })
            
            # ë…ë¦½ì ì¸ ì„ë² ë”© ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            cache_dir = LocalFileStore(f"./file_embeddings/{file_hash}")
            
            # í–¥ìƒëœ í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì •
            splitter = CharacterTextSplitter.from_tiktoken_encoder(
                separator="\n\n",  # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
                chunk_size=chunk_size_val,
                chunk_overlap=chunk_overlap_val,
            )
            
            # ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 
            loader = UnstructuredFileLoader(file_path)
            docs = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for doc in docs:
                doc.metadata.update({
                    "filename": file.name,
                    "file_hash": file_hash,
                    "file_size": file.size,
                    "processed_at": time.strftime("%Y-%m-%d %H:%M:%S")
                })
            
            # ë¬¸ì„œ ë¶„í• 
            split_docs = splitter.split_documents(docs)
            all_docs.extend(split_docs)
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.progress((idx + 1) / total_files)
            
        except Exception as e:
            error_msg = f"{file.name}: {str(e)}"
            processing_errors.append(error_msg)
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {error_msg}")
            continue
    
    progress_bar.empty()
    status_text.empty()
    
    if not all_docs:
        st.error("âŒ ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        if processing_errors:
            with st.expander("ğŸ” ì˜¤ë¥˜ ì„¸ë¶€ì‚¬í•­"):
                for error in processing_errors:
                    st.error(error)
        return None, None
    
    try:
        # ì„ë² ë”© ìƒì„± ë° ë²¡í„°ìŠ¤í† ì–´ êµ¬ì„±
        status_text.text("ğŸ”„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
        
        embeddings = OllamaEmbeddings(model=model_name)
        # ë§ˆì§€ë§‰ íŒŒì¼ì˜ í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        cache_dir = LocalFileStore(f"./file_embeddings/combined_{hash(str(file_info))}")
        cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
        
        vectorstore = FAISS.from_documents(all_docs, cached_embeddings)
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": retrieval_k_val}
        )
        
        status_text.empty()
        
        # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
        processing_stats = {
            "total_files": len(file_info),
            "total_chunks": len(all_docs),
            "total_chars": sum(info["char_count"] for info in file_info),
            "total_words": sum(info["word_count"] for info in file_info),
            "processing_errors": processing_errors
        }
        
        return retriever, {"file_info": file_info, "stats": processing_stats}
        
    except Exception as e:
        st.error(f"âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return None, None

def save_message(message, role):
    if "file_messages" not in st.session_state:
        st.session_state["file_messages"] = []
    st.session_state["file_messages"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    if "file_messages" in st.session_state:
        for message in st.session_state["file_messages"]:
            send_message(
                message["message"],
                message["role"],
                save=False,
            )

def format_docs_enhanced(docs):
    """í–¥ìƒëœ ë¬¸ì„œ í¬ë§·íŒ…"""
    formatted_docs = []
    for i, doc in enumerate(docs):
        content = doc.page_content
        filename = doc.metadata.get('filename', 'Unknown')
        
        formatted_docs.append(f"[ë¬¸ì„œ {i+1} - {filename}]\n{content}")
    
    return "\n\n".join(formatted_docs)

# í•œêµ­ì–´ ì§€ì› í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_template(
    """ì•„ë˜ ì œê³µëœ ë¬¸ì„œë“¤ì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. 
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.
    ë‹µë³€í•  ë•ŒëŠ” ì–´ë–¤ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì•˜ëŠ”ì§€ë„ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

ì œê³µëœ ë¬¸ì„œë“¤:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""
)

# ë©”ì¸ UI
st.title("ğŸ“„ File GPT")
st.markdown(f"ğŸ¤– **í˜„ì¬ ì„ íƒëœ ëª¨ë¸:** {selected_model}")

# ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í‘œì‹œ
with st.expander("ğŸ“‹ ì‹œìŠ¤í…œ ì •ë³´"):
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**ì§€ì› íŒŒì¼ í˜•ì‹:** {', '.join(SUPPORTED_FILE_TYPES.keys())}")
        st.write(f"**ì²­í¬ í¬ê¸°:** {chunk_size}")
    with col2:
        st.write(f"**ì²­í¬ ì˜¤ë²„ë©:** {chunk_overlap}")
        st.write(f"**ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜:** {retrieval_k}")

st.markdown(
    """
    ğŸ’¡ **ì‚¬ìš©ë²•:**
    1. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
    2. ëª¨ë¸ê³¼ ì„¤ì •ì„ ì¡°ì •í•˜ì„¸ìš” (ì„ íƒì‚¬í•­)
    3. íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ AIì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”!
    
    **ì˜ˆì‹œ ì§ˆë¬¸:**
    - "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”"
    - "íŠ¹ì • í‚¤ì›Œë“œê°€ ì–¸ê¸‰ëœ ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”"
    - "ë¬¸ì„œë“¤ ê°„ì˜ ê³µí†µì ì´ë‚˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    - "ì´ ë‚´ìš©ê³¼ ê´€ë ¨ëœ ì¶”ì²œì‚¬í•­ì´ ìˆë‚˜ìš”?"
    """
)

# íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
with st.sidebar:
    st.title("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    
    # ì§€ì› íŒŒì¼ í˜•ì‹ ì •ë³´
    with st.expander("ğŸ“‹ ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹"):
        for ext, info in SUPPORTED_FILE_TYPES.items():
            st.write(f"**{ext.upper()}**: {info['description']} (ìµœëŒ€ {info['max_size_mb']}MB)")
    
    files = st.file_uploader(
        "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
        type=list(SUPPORTED_FILE_TYPES.keys()),
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    # íŒŒì¼ ê²€ì¦
    if files:
        st.subheader("ğŸ“‹ ì—…ë¡œë“œëœ íŒŒì¼")
        valid_files = []
        total_size = 0
        
        for file in files:
            errors = validate_file(file)
            file_size_mb = file.size / (1024 * 1024)
            total_size += file_size_mb
            
            if errors:
                st.error(f"âŒ {file.name}")
                for error in errors:
                    st.write(f"   â€¢ {error}")
            else:
                st.success(f"âœ… {file.name} ({file_size_mb:.1f}MB)")
                valid_files.append(file)
        
        st.write(f"**ì´ íŒŒì¼ í¬ê¸°:** {total_size:.1f}MB")
        
        if total_size > 200:
            st.warning("âš ï¸ íŒŒì¼ í¬ê¸°ê°€ í´ ê²½ìš° ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# ì±„íŒ… ê¸°ë¡ í‘œì‹œ
paint_history()

# íŒŒì¼ ì²˜ë¦¬ ë° RAG ì‹œìŠ¤í…œ
if files:
    valid_files = [file for file in files if not validate_file(file)]
    
    if valid_files:
        try:
            retriever, file_data = embed_files(
                valid_files, 
                selected_model, 
                chunk_size, 
                chunk_overlap, 
                retrieval_k
            )
            
            if retriever and file_data:
                # ì²˜ë¦¬ ì™„ë£Œ ë©”ì‹œì§€
                stats = file_data["stats"]
                success_msg = f"""ğŸ‰ íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ğŸ“Š **ì²˜ë¦¬ ê²°ê³¼:**
- ì²˜ë¦¬ëœ íŒŒì¼: {stats['total_files']}ê°œ
- ìƒì„±ëœ ì²­í¬: {stats['total_chunks']}ê°œ  
- ì´ ë¬¸ì ìˆ˜: {stats['total_chars']:,}ì
- ì´ ë‹¨ì–´ ìˆ˜: {stats['total_words']:,}ê°œ

ì´ì œ íŒŒì¼ ë‚´ìš©ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ì§ˆë¬¸í•´ë³´ì„¸ìš”!"""
                
                send_message(success_msg, "ai", save=False)
                
                # ì²˜ë¦¬ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
                if stats["processing_errors"]:
                    with st.expander("âš ï¸ ì²˜ë¦¬ ì¤‘ ë°œìƒí•œ ì˜¤ë¥˜ë“¤"):
                        for error in stats["processing_errors"]:
                            st.warning(error)
                
                # í˜„ì¬ ë¡œë“œëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
                st.write("### ğŸ“Š ë¡œë“œëœ íŒŒì¼ ì •ë³´")
                
                file_info_df = pd.DataFrame(file_data["file_info"])
                if not file_info_df.empty:
                    # ì»¬ëŸ¼ëª… í•œêµ­ì–´ë¡œ ë³€ê²½
                    file_info_df = file_info_df.rename(columns={
                        "name": "íŒŒì¼ëª…",
                        "size_mb": "í¬ê¸°(MB)", 
                        "char_count": "ë¬¸ììˆ˜",
                        "word_count": "ë‹¨ì–´ìˆ˜",
                        "line_count": "ì¤„ìˆ˜"
                    })
                    
                    # ìˆ«ì í¬ë§·íŒ…
                    file_info_df["í¬ê¸°(MB)"] = file_info_df["í¬ê¸°(MB)"].round(2)
                    file_info_df["ë¬¸ììˆ˜"] = file_info_df["ë¬¸ììˆ˜"].apply(lambda x: f"{x:,}")
                    file_info_df["ë‹¨ì–´ìˆ˜"] = file_info_df["ë‹¨ì–´ìˆ˜"].apply(lambda x: f"{x:,}")
                    file_info_df["ì¤„ìˆ˜"] = file_info_df["ì¤„ìˆ˜"].apply(lambda x: f"{x:,}")
                    
                    st.dataframe(file_info_df.drop(columns=["hash"], errors="ignore"), use_container_width=True)
                
                # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                st.session_state["file_retriever"] = retriever
                st.session_state["file_data"] = file_data
                
            else:
                st.error("âŒ íŒŒì¼ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
    else:
        st.warning("âš ï¸ ìœ íš¨í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

# ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "file_retriever" in st.session_state:
    message = st.chat_input("íŒŒì¼ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”...")
    if message:
        send_message(message, "human")
        
        with st.chat_message("ai"):
            try:
                # í˜„ì¬ ì„ íƒëœ ëª¨ë¸ë¡œ LLM ì´ˆê¸°í™”
                llm = get_llm_file(selected_model, temperature)
                
                chain = (
                    {
                        "context": st.session_state["file_retriever"] | RunnableLambda(format_docs_enhanced),
                        "question": RunnablePassthrough(),
                    }
                    | prompt
                    | llm
                )
                
                chain.invoke(message)
                
            except Exception as e:
                st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                st.info("ğŸ’¡ **í•´ê²° ë°©ë²•:** ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•˜ê±°ë‚˜ íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•´ë³´ì„¸ìš”.")

# ì‚¬ì´ë“œë°” í•˜ë‹¨ ì •ë³´
with st.sidebar:
    st.markdown("---")
    
    # í˜„ì¬ ì„¸ì…˜ ì •ë³´
    if "file_data" in st.session_state:
        st.markdown("### ğŸ“Š í˜„ì¬ ì„¸ì…˜ ì •ë³´")
        stats = st.session_state["file_data"]["stats"]
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ë¡œë“œëœ íŒŒì¼", f"{stats['total_files']}ê°œ")
            st.metric("ìƒì„±ëœ ì²­í¬", f"{stats['total_chunks']}ê°œ")
        with col2:
            st.metric("ì´ ë¬¸ììˆ˜", f"{stats['total_chars']:,}")
            st.metric("ì´ ë‹¨ì–´ìˆ˜", f"{stats['total_words']:,}")
    
    # ë„ì›€ë§
    st.markdown("### ğŸ’¡ íŒ")
    with st.expander("íš¨ê³¼ì ì¸ ì§ˆë¬¸ ë°©ë²•"):
        st.markdown("""
        **ì¢‹ì€ ì§ˆë¬¸ ì˜ˆì‹œ:**
        - "ë¬¸ì„œì˜ í•µì‹¬ ì•„ì´ë””ì–´ 3ê°€ì§€ëŠ”?"
        - "íŠ¹ì • ê°œë…ì— ëŒ€í•œ ì„¤ëª…ì„ ì°¾ì•„ì£¼ì„¸ìš”"
        - "ë¬¸ì„œì—ì„œ ìˆ˜ì¹˜ë‚˜ í†µê³„ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”"
        - "ê²°ë¡  ë¶€ë¶„ì˜ ë‚´ìš©ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”"
        
        **í”¼í•´ì•¼ í•  ì§ˆë¬¸:**
        - ë¬¸ì„œì— ì—†ëŠ” ì¼ë°˜ì ì¸ ì§€ì‹ ìš”ì²­
        - ë„ˆë¬´ ì¶”ìƒì ì´ê±°ë‚˜ ëª¨í˜¸í•œ ì§ˆë¬¸
        - ë¬¸ì„œ ì™¸ë¶€ ì •ë³´ì™€ì˜ ë¹„êµ ìš”ì²­
        """)
    
    with st.expander("íŒŒì¼ ì²˜ë¦¬ ìµœì í™”"):
        st.markdown("""
        **ì²˜ë¦¬ ì†ë„ í–¥ìƒ:**
        - íŒŒì¼ í¬ê¸°ë¥¼ ì ì • ìˆ˜ì¤€ìœ¼ë¡œ ìœ ì§€
        - ë¶ˆí•„ìš”í•œ ì„œì‹ì´ë‚˜ ì´ë¯¸ì§€ ì œê±°
        - í…ìŠ¤íŠ¸ ìœ„ì£¼ì˜ ê¹”ë”í•œ íŒŒì¼ ì‚¬ìš©
        
        **ì •í™•ë„ í–¥ìƒ:**
        - ì²­í¬ í¬ê¸°ë¥¼ ë‚´ìš©ì— ë§ê²Œ ì¡°ì •
        - ê´€ë ¨ ë¬¸ì„œë“¤ì„ í•¨ê»˜ ì—…ë¡œë“œ
        - ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì‘ì„±
        """)

# ëŒ€í™” ì´ˆê¸°í™” ë²„íŠ¼
if st.sidebar.button("ğŸ’¬ ëŒ€í™” ë‚´ìš© ì´ˆê¸°í™”"):
    st.session_state["file_messages"] = []
    st.rerun()

# íŒŒì¼ ë°ì´í„° ì´ˆê¸°í™” ë²„íŠ¼  
if st.sidebar.button("ğŸ—‘ï¸ íŒŒì¼ ë°ì´í„° ì´ˆê¸°í™”"):
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    keys_to_remove = ["file_messages", "file_retriever", "file_data"]
    for key in keys_to_remove:
        if key in st.session_state:
            del st.session_state[key]
    st.rerun()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "file_messages" not in st.session_state:
    st.session_state["file_messages"] = []