import streamlit as st
import os
from dotenv import load_dotenv
import openai
import numpy as np
import time
import json
from datetime import datetime
import hashlib
import glob
import requests
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib import colors
from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_RIGHT
import io
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

# FPDF2ë¥¼ ì‚¬ìš©í•œ í•œê¸€ PDF ìƒì„±
try:
    from fpdf import FPDF
    FPDF2_AVAILABLE = True
except ImportError:
    FPDF2_AVAILABLE = False

# WeasyPrintëŠ” ì‹œìŠ¤í…œ ì˜ì¡´ì„± ë¬¸ì œë¡œ ì œê±°
WEASYPRINT_AVAILABLE = False

# DOCX ìƒì„±ì„ ìœ„í•œ python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from docx import Document
    from docx.shared import Inches, Pt
    from docx.enum.text import WD_ALIGN_PARAGRAPH
    from docx.oxml.shared import OxmlElement, qn
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

# langchain ë° FAISS ê´€ë ¨
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import (
    PyPDFLoader, UnstructuredPowerPointLoader, UnstructuredExcelLoader, 
    UnstructuredWordDocumentLoader, UnstructuredMarkdownLoader, UnstructuredFileLoader
)
from langchain.schema import Document as LangchainDocument

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Q-Li",
    page_icon="ğŸ¤”",
    layout="wide"
)

load_dotenv()

# CSS ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
.main-header {
    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
    padding: 1rem;
    border-radius: 10px;
    text-align: center;
    color: white;
    margin-bottom: 2rem;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
}

.metric-container {
    background: white;
    padding: 1rem;
    border-radius: 8px;
    border-left: 4px solid #667eea;
    margin: 0.5rem 0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.agent-card {
    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    padding: 1rem;
    border-radius: 10px;
    margin: 0.5rem 0;
    border: 1px solid #e1e5e9;
}

.supplier-card {
    background: white;
    border: 1px solid #e1e5e9;
    border-radius: 8px;
    padding: 1rem;
    margin: 0.5rem 0;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
}

.search-result {
    background: #f8f9fa;
    border-left: 4px solid #28a745;
    padding: 1rem;
    margin: 0.5rem 0;
    border-radius: 4px;
}
</style>
""", unsafe_allow_html=True)

# ë©”ì¸ í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ¤” Q-Li (aQara-LIfe | íë¦¬)</h1>
    <p>ë¨¼ì € ì‚¬ì´ë“œë°”ì˜ ë§¨ í•˜ë‹¨ ë©”ë‰´ì—ì„œ ì›í•˜ëŠ” LLMë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.</p>
    ğŸ’¡ LLMì—ì„œ ollamaëŠ” ë¡œì»¬ ì„œë²„ì—ì„œ ë™ì‘í•˜ëŠ” sLLMìœ¼ë¡œ ì„±ëŠ¥ì€ ì•„ì§ ë§ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë‹¤ë§Œ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ë¯€ë¡œ ë¬´ë£Œì…ë‹ˆë‹¤.
</div>
""", unsafe_allow_html=True) 

# ì¸ì¦ ê¸°ëŠ¥ (ê°„ë‹¨í•œ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸)
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.')
    st.stop()

if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:  # ë¹„ë°€ë²ˆí˜¸ê°€ ì…ë ¥ëœ ê²½ìš°ì—ë§Œ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()



# ===== LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ =====
class LLMClient:
    """ë‹¤ì–‘í•œ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì„¤ì •"""
        # OpenAI í´ë¼ì´ì–¸íŠ¸ (ê¸°ë³¸)
        openai_key = os.getenv('OPENAI_API_KEY')
        if openai_key:
            try:
                self.clients['openai'] = openai.OpenAI(api_key=openai_key)
                self.models['openai'] = [
                    'gpt-4o-mini',
                    'gpt-4o',
                    'gpt-4-turbo',
                    'gpt-4',
                    'gpt-3.5-turbo'
                ]
            except Exception as e:
                st.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ LLM) - ì„ íƒì 
        try:
            import requests
            # Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì§§ì€ íƒ€ì„ì•„ì›ƒ)
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                self.clients['ollama'] = requests
                self.models['ollama'] = [
                    'mistral:latest',
                    'llama3.1:latest',
                    'llama3.1:8b',
                    'phi4:latest',
                    'llama2:latest',
                    'gemma2:latest',
                    'gemma:latest',
                    'llama3.2:latest',
                    'deepseek-r1:14b',
                    'nomic-embed-text:latest'
                ]
        except Exception as e:
            # Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
            pass
        
        # Perplexity í´ë¼ì´ì–¸íŠ¸ (ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ êµ¬í˜„ ë°©ì‹ ì°¸ê³ )
        perplexity_key = os.getenv('PERPLEXITY_API_KEY')
        if perplexity_key:
            try:
                self.clients['perplexity'] = openai.OpenAI(
                    api_key=perplexity_key,
                    base_url="https://api.perplexity.ai"
                )
                self.models['perplexity'] = [
                    "sonar-pro",
                    "sonar-small-chat"
                ]
                import logging
                logging.info(f"âœ… [DEBUG] Perplexity í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì„±ê³µ")
                logging.info(f"âœ… [DEBUG] Perplexity API í‚¤: {perplexity_key[:10]}...")
                
                # ê°„ë‹¨í•œ ì—°ê²° í…ŒìŠ¤íŠ¸
                try:
                    test_response = self.clients['perplexity'].chat.completions.create(
                        model="sonar-pro",
                        messages=[{"role": "user", "content": "Hello"}],
                        max_tokens=10
                    )
                    logging.info(f"âœ… [DEBUG] Perplexity ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                except Exception as test_e:
                    logging.error(f"âŒ [DEBUG] Perplexity ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(test_e)}")
                    
            except Exception as e:
                import logging
                logging.error(f"âŒ [DEBUG] Perplexity í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {str(e)}")
                logging.error(f"âŒ [DEBUG] Perplexity ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                # ì¡°ìš©íˆ ì‹¤íŒ¨ ì²˜ë¦¬ (ë‹¤ë¥¸ íŒŒì¼ë“¤ê³¼ ë™ì¼í•˜ê²Œ)
                pass
        else:
            import logging
            logging.warning(f"âš ï¸ [DEBUG] Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
        
        # Anthropic í´ë¼ì´ì–¸íŠ¸ (Claude)
        anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        if anthropic_api_key:
            try:
                from langchain_anthropic import ChatAnthropic
                self.clients['anthropic'] = ChatAnthropic(
                    model="claude-3-5-sonnet-20241022",
                    anthropic_api_key=anthropic_api_key,
                    temperature=0.7,
                    max_tokens=4000
                )
                self.models['anthropic'] = [
                    'claude-3-7-sonnet-latest',
                    'claude-3-5-sonnet-20241022',
                    'claude-3-5-haiku-20241022',
                    'claude-3-opus-20240229',
                    'claude-3-sonnet-20240229',
                    'claude-3-haiku-20240307'
                ]
            except Exception as e:
                st.warning(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
                # í´ë¼ì´ì–¸íŠ¸ ì œê±°
                if 'anthropic' in self.clients:
                    del self.clients['anthropic']
                if 'anthropic' in self.models:
                    del self.models['anthropic']
        
        # Google Gemini í´ë¼ì´ì–¸íŠ¸ (í• ë‹¹ëŸ‰ ë¬¸ì œë¡œ ì„ì‹œ ë¹„í™œì„±í™”)
        google_api_key = os.getenv('GOOGLE_API_KEY')
        if google_api_key:
            try:
                from langchain_google_genai import ChatGoogleGenerativeAI
                self.clients['google'] = ChatGoogleGenerativeAI(
                    model="gemini-2.5-flash",
                    google_api_key=google_api_key,
                    temperature=0.7,
                    max_output_tokens=4000
                )
                self.models['google'] = [
                    'gemini-2.5-pro',
                    'gemini-2.5-flash',
                    'gemini-2.5-flash-lite-preview-06-17',
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-pro',
                    'gemini-pro-vision'
                ]
            except Exception as e:
                st.warning(f"Google Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
                # í´ë¼ì´ì–¸íŠ¸ ì œê±°
                if 'google' in self.clients:
                    del self.clients['google']
                if 'google' in self.models:
                    del self.models['google']
        else:
            st.info("Google Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ LLM ì œê³µìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
    
    def get_available_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ ë°˜í™˜"""
        return list(self.clients.keys())
    
    def get_models_for_provider(self, provider):
        """íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.models.get(provider, [])
    
    def generate_response(self, provider, model, messages, temperature=0.7, max_tokens=2000):
        """ì„ íƒëœ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            if provider not in self.clients:
                return None, f"í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ì œê³µì: {provider}"
            
            # ë””ë²„ê·¸ ë¡œê·¸ ì¶”ê°€
            import logging
            logging.info(f"ğŸ”§ [DEBUG] generate_response í˜¸ì¶œ: provider={provider}, model={model}")
            logging.info(f"ğŸ”§ [DEBUG] ì‚¬ìš© ê°€ëŠ¥í•œ í´ë¼ì´ì–¸íŠ¸: {list(self.clients.keys())}")
            
            if provider == 'ollama':
                return self._generate_ollama_response(model, messages, temperature, max_tokens)
            elif provider == 'openai':
                return self._generate_openai_response(model, messages, temperature, max_tokens)
            elif provider == 'perplexity':
                return self._generate_perplexity_response(model, messages, temperature, max_tokens)
            elif provider == 'anthropic':
                return self._generate_anthropic_response(model, messages, temperature, max_tokens)
            elif provider == 'google':
                return self._generate_google_response(model, messages, temperature, max_tokens)
            else:
                return None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}"
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] generate_response ì˜ˆì™¸: {str(e)}")
            return None, f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_ollama_response(self, model, messages, temperature, max_tokens):
        """Ollama ì‘ë‹µ ìƒì„±"""
        try:
            # Ollama API í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ë³€í™˜
            ollama_messages = []
            for msg in messages:
                if msg['role'] == 'system':
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” í”„ë¡¬í”„íŠ¸ì— í¬í•¨
                    continue
                elif msg['role'] == 'user':
                    ollama_messages.append({
                        'role': 'user',
                        'content': msg['content']
                    })
                elif msg['role'] == 'assistant':
                    ollama_messages.append({
                        'role': 'assistant',
                        'content': msg['content']
                    })
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì— í¬í•¨
            system_content = ""
            for msg in messages:
                if msg['role'] == 'system':
                    system_content = msg['content']
                    break
            
            if system_content and ollama_messages:
                ollama_messages[0]['content'] = f"{system_content}\n\n{ollama_messages[0]['content']}"
            
            # Ollama API í˜¸ì¶œ
            response = self.clients['ollama'].post(
                "http://localhost:11434/api/chat",
                json={
                    "model": model,
                    "messages": ollama_messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['message']['content'], None
            else:
                return None, f"Ollama API ì˜¤ë¥˜: {response.status_code}"
                
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] Ollama ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            logging.error(f"âŒ [DEBUG] Ollama ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, f"Ollama ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_openai_response(self, model, messages, temperature, max_tokens):
        """OpenAI ì‘ë‹µ ìƒì„±"""
        try:
            response = self.clients['openai'].chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content, None
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] OpenAI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            logging.error(f"âŒ [DEBUG] OpenAI ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, f"OpenAI API ì˜¤ë¥˜: {str(e)}"
    
    def _generate_perplexity_response(self, model, messages, temperature, max_tokens):
        """Perplexity ì‘ë‹µ ìƒì„± (ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ êµ¬í˜„ ë°©ì‹ ì°¸ê³ )"""
        try:
            import logging
            logging.info(f"ğŸ”§ [DEBUG] Perplexity ì‘ë‹µ ìƒì„± ì‹œì‘")
            logging.info(f"ğŸ”§ [DEBUG] ì‚¬ìš©í•  ëª¨ë¸: {model}")
            logging.info(f"ğŸ”§ [DEBUG] ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
            logging.info(f"ğŸ”§ [DEBUG] í´ë¼ì´ì–¸íŠ¸ ì¡´ì¬ ì—¬ë¶€: {'perplexity' in self.clients}")
            logging.info(f"ğŸ”§ [DEBUG] ë©”ì‹œì§€ ë‚´ìš©: {messages}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤
            test_models = ["sonar-pro", "sonar-small-chat"]
            
            # ê° ëª¨ë¸ë¡œ ì‹œë„
            for test_model in test_models:
                try:
                    logging.info(f"ğŸ”§ [DEBUG] Perplexity ëª¨ë¸ {test_model} ì‹œë„ ì¤‘...")
                    
                    # ë©”ì‹œì§€ í˜•ì‹ í™•ì¸ ë° ìˆ˜ì • (ì¤‘ë³µ ì œê±° ë° ì˜¬ë°”ë¥¸ ìˆœì„œ)
                    formatted_messages = []
                    seen_messages = set()  # ì¤‘ë³µ ë©”ì‹œì§€ ë°©ì§€
                    
                    for msg in messages:
                        # ë©”ì‹œì§€ ë‚´ìš©ì„ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
                        message_key = f"{msg['role']}:{msg['content']}"
                        if message_key not in seen_messages:
                            seen_messages.add(message_key)
                            if msg['role'] == 'system':
                                formatted_messages.append({"role": "system", "content": msg['content']})
                            elif msg['role'] == 'user':
                                formatted_messages.append({"role": "user", "content": msg['content']})
                            elif msg['role'] == 'assistant':
                                formatted_messages.append({"role": "assistant", "content": msg['content']})
                    
                    # Perplexity API ìš”êµ¬ì‚¬í•­ì— ë§ê²Œ ë©”ì‹œì§€ ìˆœì„œ ì¡°ì •
                    # system -> user -> assistant -> user ìˆœì„œë¡œ ë˜ì–´ì•¼ í•¨
                    if len(formatted_messages) > 1:
                        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” ì œê±°
                        final_messages = []
                        for msg in formatted_messages:
                            if msg['role'] == 'system':
                                final_messages.append(msg)
                        
                        # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì°¾ê¸°
                        for msg in reversed(formatted_messages):
                            if msg['role'] == 'user':
                                final_messages.append(msg)
                                break
                        
                        formatted_messages = final_messages
                    
                    logging.info(f"ğŸ”§ [DEBUG] í¬ë§·ëœ ë©”ì‹œì§€: {formatted_messages}")
                    
                    response = self.clients['perplexity'].chat.completions.create(
                        model=test_model,
                        messages=formatted_messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        top_p=0.9
                    )
                    logging.info(f"âœ… [DEBUG] Perplexity ëª¨ë¸ {test_model} ì„±ê³µ!")
                    logging.info(f"âœ… [DEBUG] ì‘ë‹µ ë‚´ìš©: {response.choices[0].message.content[:100]}...")
                    return response.choices[0].message.content, None
                except Exception as e:
                    error_msg = str(e)
                    logging.error(f"âŒ [DEBUG] Perplexity ëª¨ë¸ {test_model} ì‹¤íŒ¨: {error_msg}")
                    logging.error(f"âŒ [DEBUG] ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                    
                    if "Invalid model" in error_msg or "model not found" in error_msg.lower():
                        logging.warning(f"âš ï¸ [DEBUG] ëª¨ë¸ {test_model} ìœ íš¨í•˜ì§€ ì•ŠìŒ, ë‹¤ìŒ ëª¨ë¸ ì‹œë„")
                        continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                    elif "authentication" in error_msg.lower() or "unauthorized" in error_msg.lower():
                        logging.error(f"âŒ [DEBUG] API í‚¤ ì¸ì¦ ì‹¤íŒ¨")
                        return None, f"API í‚¤ ì¸ì¦ ì‹¤íŒ¨: {error_msg}"
                    elif "rate limit" in error_msg.lower() or "quota" in error_msg.lower():
                        logging.error(f"âŒ [DEBUG] API ìš”ì²­ í•œë„ ì´ˆê³¼")
                        return None, f"API ìš”ì²­ í•œë„ ì´ˆê³¼: {error_msg}"
                    else:
                        logging.warning(f"âš ï¸ [DEBUG] ê¸°íƒ€ ì˜¤ë¥˜, ë‹¤ìŒ ëª¨ë¸ ì‹œë„")
                        continue  # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ë‹¤ìŒ ëª¨ë¸ ì‹œë„
            
            # ëª¨ë“  ëª¨ë¸ì—ì„œ ì‹¤íŒ¨í•œ ê²½ìš°
            logging.error(f"âŒ [DEBUG] ëª¨ë“  Perplexity ëª¨ë¸ì—ì„œ ì‹¤íŒ¨")
            return None, "ëª¨ë“  Perplexity ëª¨ë¸ì—ì„œ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] Perplexity ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            logging.error(f"âŒ [DEBUG] Perplexity ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, f"Perplexity API ì˜¤ë¥˜: {str(e)}"
    
    def _generate_anthropic_response(self, model, messages, temperature, max_tokens):
        """Anthropic ì‘ë‹µ ìƒì„±"""
        try:
            # LangChain ChatAnthropic ì‚¬ìš©
            response = self.clients['anthropic'].invoke(messages)
            return response.content, None
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] Anthropic ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            logging.error(f"âŒ [DEBUG] Anthropic ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, f"Anthropic API ì˜¤ë¥˜: {str(e)}"
    
    def _generate_google_response(self, model, messages, temperature, max_tokens):
        """Google Gemini ì‘ë‹µ ìƒì„±"""
        try:
            # LangChain ChatGoogleGenerativeAI ì‚¬ìš©
            response = self.clients['google'].invoke(messages)
            return response.content, None
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] Google Gemini ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
            logging.error(f"âŒ [DEBUG] Google Gemini ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            return None, f"Google Gemini API ì˜¤ë¥˜: {str(e)}"

# ===== FAISS ê¸°ë°˜ íŒŒì¼ì‹œìŠ¤í…œ RAGSystem =====
class FileRAGSystem:
    """FAISS + íŒŒì¼ì‹œìŠ¤í…œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ê°œì„ ëœ ì„ë² ë”©)"""
    def __init__(self, base_folder_path="./pages/rag_files"):
        self.base_folder_path = base_folder_path
        self.current_folder_path = base_folder_path
        self.vectorstore = None
        self.docs = []
        self.is_loaded = False
        self.embeddings = self._setup_embeddings()
        self.load_files_and_build_index()

    def _setup_embeddings(self):
        """ì„ë² ë”© ëª¨ë¸ ì„¤ì • (ê°œì„ ëœ ë²„ì „)"""
        try:
            # 1. OpenAI ì„ë² ë”© (ê¸°ë³¸)
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                try:
                    # text-embedding-3-smallì´ ë” ë‚˜ì€ ì„±ëŠ¥ ì œê³µ
                    from langchain_openai import OpenAIEmbeddings
                    return OpenAIEmbeddings(
                        model="text-embedding-3-small",
                        dimensions=1536  # ë” ì‘ì€ ì°¨ì›ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ
                    )
                except Exception as e:
                    st.warning(f"OpenAI ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨: {e}")
            
            # 2. Ollama ì„ë² ë”© (ë¡œì»¬ ëŒ€ì•ˆ)
            try:
                import requests
                response = requests.get("http://localhost:11434/api/tags", timeout=2)
                if response.status_code == 200:
                    from langchain_community.embeddings import OllamaEmbeddings
                    return OllamaEmbeddings(model="nomic-embed-text")
            except:
                pass
            
            # 3. ê¸°ë³¸ OpenAI ì„ë² ë”© (fallback)
            from langchain_openai import OpenAIEmbeddings
            return OpenAIEmbeddings()
            
        except Exception as e:
            st.error(f"ì„ë² ë”© ì„¤ì • ì‹¤íŒ¨: {e}")
            # ìµœí›„ ìˆ˜ë‹¨ìœ¼ë¡œ ê¸°ë³¸ OpenAI ì„ë² ë”©
            from langchain_openai import OpenAIEmbeddings
            return OpenAIEmbeddings()

    def set_search_folder(self, folder_path):
        """ê²€ìƒ‰í•  í´ë” ì„¤ì •"""
        self.current_folder_path = folder_path
        self.load_files_and_build_index()

    def load_files_and_build_index(self):
        """í´ë” ë‚´ ëª¨ë“  ì§€ì› íŒŒì¼ì„ ì½ì–´ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• (ê°œì„ ëœ ì²­í‚¹)"""
        loaders = [
            ("*.pdf", None),  # PDFëŠ” ë³„ë„ ì²˜ë¦¬
            ("*.pptx", UnstructuredPowerPointLoader),
            ("*.xlsx", None),  # Excelì€ ë³„ë„ ì²˜ë¦¬
            ("*.docx", UnstructuredWordDocumentLoader),
            ("*.md", UnstructuredMarkdownLoader),
            ("*.gdoc", UnstructuredFileLoader),  # Google Docs (HTML í˜•ì‹)
            ("*.gsheet", UnstructuredFileLoader), # Google Sheets (HTML í˜•ì‹)
            ("*.gslides", UnstructuredFileLoader), # Google Slides (HTML í˜•ì‹)
        ]
        all_docs = []
        
        # ì „ì²´ ê²€ìƒ‰ì¸ ê²½ìš° finance í´ë” ì œì™¸
        if self.current_folder_path == "./pages/rag_files":
            # ëª¨ë“  í•˜ìœ„ í´ë”ë¥¼ ê²€ìƒ‰í•˜ë˜ finance í´ë”ëŠ” ì œì™¸
            for root, dirs, files in os.walk(self.current_folder_path):
                # finance í´ë” ì œì™¸
                if "finance" in dirs:
                    dirs.remove("finance")
                
                for pattern, loader_cls in loaders:
                    for file in glob.glob(os.path.join(root, pattern)):
                        try:
                            if pattern == "*.xlsx":
                                # Excel íŒŒì¼ì€ ëª¨ë“  ì‹œíŠ¸ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
                                excel_docs = self._load_excel_with_all_sheets(file)
                                if excel_docs:
                                    all_docs.extend(excel_docs)
                                    # st.success(f"Excel íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(excel_docs)}ê°œ ë¬¸ì„œ)")
                            elif pattern == "*.pdf":
                                # PDF íŒŒì¼ì€ ê°œì„ ëœ ë¡œë”ë¡œ ì²˜ë¦¬
                                pdf_docs = self._load_pdf_with_improved_parser(file)
                                if pdf_docs:
                                    all_docs.extend(pdf_docs)
                                    # st.success(f"PDF íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(pdf_docs)}ê°œ ë¬¸ì„œ)")
                            elif loader_cls:
                                loader = loader_cls(file)
                                docs = loader.load()
                                if docs:
                                    all_docs.extend(docs)
                                    # st.success(f"íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(docs)}ê°œ ë¬¸ì„œ)")
                        except Exception as e:
                            st.warning(f"{file} ë¡œë”© ì‹¤íŒ¨: {e}")
                            continue
        else:
            # íŠ¹ì • í´ë”ë§Œ ê²€ìƒ‰
            for pattern, loader_cls in loaders:
                for file in glob.glob(os.path.join(self.current_folder_path, pattern)):
                    try:
                        if pattern == "*.xlsx":
                            # Excel íŒŒì¼ì€ ëª¨ë“  ì‹œíŠ¸ë¥¼ ë³„ë„ë¡œ ì²˜ë¦¬
                            excel_docs = self._load_excel_with_all_sheets(file)
                            if excel_docs:
                                all_docs.extend(excel_docs)
                                # st.success(f"Excel íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(excel_docs)}ê°œ ë¬¸ì„œ)")
                        elif pattern == "*.pdf":
                            # PDF íŒŒì¼ì€ ê°œì„ ëœ ë¡œë”ë¡œ ì²˜ë¦¬
                            pdf_docs = self._load_pdf_with_improved_parser(file)
                            if pdf_docs:
                                all_docs.extend(pdf_docs)
                                # st.success(f"PDF íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(pdf_docs)}ê°œ ë¬¸ì„œ)")
                        elif loader_cls:
                            loader = loader_cls(file)
                            docs = loader.load()
                            if docs:
                                all_docs.extend(docs)
                                # st.success(f"íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file} ({len(docs)}ê°œ ë¬¸ì„œ)")
                    except Exception as e:
                        st.warning(f"{file} ë¡œë”© ì‹¤íŒ¨: {e}")
                        continue
        
        # ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if not all_docs:
            st.warning("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.is_loaded = False
            return
        
        # ê°œì„ ëœ ì²­í‚¹ ì ìš©
        all_docs = self._improved_chunking(all_docs)
        
        self.docs = all_docs
        if all_docs:
            # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            try:
                self.vectorstore = FAISS.from_documents(all_docs, self.embeddings)
                self.is_loaded = True
                # st.success(f"RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {len(all_docs)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
            except Exception as e:
                st.error(f"FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹¤íŒ¨: {e}")
                self.is_loaded = False
        else:
            self.vectorstore = None
            self.is_loaded = False

    def _improved_chunking(self, docs):
        """ê°œì„ ëœ ì²­í‚¹ ë°©ë²•"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        
        # í•œêµ­ì–´ íŠ¹í™” í…ìŠ¤íŠ¸ ë¶„í• ê¸°
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # ë” ì‘ì€ ì²­í¬ë¡œ ì„¸ë°€í•œ ê²€ìƒ‰
            chunk_overlap=200,  # ì˜¤ë²„ë©ìœ¼ë¡œ ë¬¸ë§¥ ìœ ì§€
            length_function=len,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""],  # í•œêµ­ì–´ ë¬¸ì¥ êµ¬ë¶„ì ì¶”ê°€
            is_separator_regex=False
        )
        
        improved_docs = []
        
        for doc in docs:
            # ì›ë³¸ ë¬¸ì„œ ë‚´ìš©
            original_content = doc.page_content
            
            # ì²­í‚¹ ì ìš©
            chunks = text_splitter.split_text(original_content)
            
            for i, chunk in enumerate(chunks):
                # ê°œì„ ëœ ë©”íƒ€ë°ì´í„°
                metadata = doc.metadata.copy()
                metadata['chunk_id'] = i
                metadata['total_chunks'] = len(chunks)
                
                # ì²­í¬ë³„ ì¶”ê°€ ì •ë³´
                if 'source' in metadata:
                    source = metadata['source']
                    if source.endswith('.xlsx'):
                        metadata['content_type'] = 'excel_data'
                    elif source.endswith('.pdf'):
                        metadata['content_type'] = 'pdf_text'
                    elif source.endswith('.docx'):
                        metadata['content_type'] = 'word_document'
                    elif source.endswith('.pptx'):
                        metadata['content_type'] = 'powerpoint'
                    else:
                        metadata['content_type'] = 'text'
                
                # ê°œì„ ëœ ì²­í¬ ìƒì„±
                improved_doc = LangchainDocument(
                    page_content=chunk,
                    metadata=metadata
                )
                
                improved_docs.append(improved_doc)
        
        return improved_docs

    def _load_pdf_with_improved_parser(self, file_path):
        """ê°œì„ ëœ PDF íŒŒì„œë¡œ PDF íŒŒì¼ ë¡œë“œ"""
        try:
            documents = []
            
            # 1. ë¨¼ì € PyPDFLoader ì‹œë„
            try:
                from langchain.document_loaders import PyPDFLoader
                loader = PyPDFLoader(file_path)
                docs = loader.load()
                if docs:
                    # st.info(f"PyPDFLoaderë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                    for doc in docs:
                        # ë©”íƒ€ë°ì´í„° ê°œì„ 
                        doc.metadata.update({
                            'file_type': 'pdf',
                            'parser': 'PyPDFLoader'
                        })
                    documents.extend(docs)
                    return documents
            except Exception as e:
                st.warning(f"PyPDFLoader ì‹¤íŒ¨: {e}")
            
            # 2. UnstructuredPDFLoader ì‹œë„ (ì„ íƒì )
            try:
                from langchain.document_loaders import UnstructuredPDFLoader
                loader = UnstructuredPDFLoader(file_path)
                docs = loader.load()
                if docs:
                    # st.info(f"UnstructuredPDFLoaderë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                    for doc in docs:
                        # ë©”íƒ€ë°ì´í„° ê°œì„ 
                        doc.metadata.update({
                            'file_type': 'pdf',
                            'parser': 'UnstructuredPDFLoader'
                        })
                    documents.extend(docs)
                    return documents
            except ImportError:
                st.info("UnstructuredPDFLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"UnstructuredPDFLoader ì‹¤íŒ¨: {e}")
            
            # 3. PDFMinerLoader ì‹œë„ (ì„ íƒì )
            try:
                from langchain.document_loaders import PDFMinerLoader
                loader = PDFMinerLoader(file_path)
                docs = loader.load()
                if docs:
                    # st.info(f"PDFMinerLoaderë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                    for doc in docs:
                        # ë©”íƒ€ë°ì´í„° ê°œì„ 
                        doc.metadata.update({
                            'file_type': 'pdf',
                            'parser': 'PDFMinerLoader'
                        })
                    documents.extend(docs)
                    return documents
            except ImportError:
                st.info("PDFMinerLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"PDFMinerLoader ì‹¤íŒ¨: {e}")
            
            # 4. PyMuPDFLoader ì‹œë„ (ì„ íƒì )
            try:
                from langchain.document_loaders import PyMuPDFLoader
                loader = PyMuPDFLoader(file_path)
                docs = loader.load()
                if docs:
                    # st.info(f"PyMuPDFLoaderë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                    for doc in docs:
                        # ë©”íƒ€ë°ì´í„° ê°œì„ 
                        doc.metadata.update({
                            'file_type': 'pdf',
                            'parser': 'PyMuPDFLoader'
                        })
                    documents.extend(docs)
                    return documents
            except ImportError:
                st.info("PyMuPDFLoaderë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"PyMuPDFLoader ì‹¤íŒ¨: {e}")
            
            # 5. PDFPlumber ì‹œë„ (í‘œì™€ í…ìŠ¤íŠ¸ ëª¨ë‘ ì¶”ì¶œ) - ì„ íƒì 
            try:
                import pdfplumber
                with pdfplumber.open(file_path) as pdf:
                    content_parts = []
                    
                    for page_num, page in enumerate(pdf.pages):
                        try:
                            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                            text = page.extract_text()
                            if text and text.strip():
                                content_parts.append(f"=== í˜ì´ì§€ {page_num + 1} ===\n{text}")
                            
                            # í‘œ ì¶”ì¶œ
                            tables = page.extract_tables()
                            if tables:
                                for table_num, table in enumerate(tables):
                                    if table:
                                        table_text = []
                                        for row in table:
                                            if any(cell and str(cell).strip() for cell in row):
                                                row_text = " | ".join([str(cell) if cell else "" for cell in row])
                                                table_text.append(row_text)
                                        if table_text:
                                            content_parts.append(f"=== í˜ì´ì§€ {page_num + 1} í‘œ {table_num + 1} ===\n" + "\n".join(table_text))
                        except Exception as e:
                            st.warning(f"í˜ì´ì§€ {page_num + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    
                    if content_parts:
                        content = "\n\n".join(content_parts)
                        doc = LangchainDocument(
                            page_content=content,
                            metadata={
                                'source': file_path,
                                'file_type': 'pdf',
                                'parser': 'PDFPlumber',
                                'total_pages': len(pdf.pages)
                            }
                        )
                        documents.append(doc)
                        # st.info(f"PDFPlumberë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                        return documents
            except ImportError:
                st.info("PDFPlumberë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ìŒ íŒŒì„œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"PDFPlumber ì‹¤íŒ¨: {e}")
            
            # 6. PyPDF2 fallback
            try:
                import PyPDF2
                with open(file_path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    content_parts = []
                    
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            text = page.extract_text()
                            if text.strip():
                                content_parts.append(f"=== í˜ì´ì§€ {page_num + 1} ===\n{text}")
                        except Exception as e:
                            st.warning(f"í˜ì´ì§€ {page_num + 1} í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    
                    if content_parts:
                        content = "\n\n".join(content_parts)
                        doc = LangchainDocument(
                            page_content=content,
                            metadata={
                                'source': file_path,
                                'file_type': 'pdf',
                                'parser': 'PyPDF2_fallback',
                                'total_pages': len(pdf_reader.pages)
                            }
                        )
                        documents.append(doc)
                        st.info(f"PyPDF2 fallbackìœ¼ë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                        return documents
            except Exception as e:
                st.error(f"PyPDF2 fallbackë„ ì‹¤íŒ¨: {e}")
            
            # 7. OCRì„ í†µí•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨) - ì„ íƒì 
            try:
                import pytesseract
                from PIL import Image
                import fitz  # PyMuPDF
                
                doc = fitz.open(file_path)
                content_parts = []
                
                for page_num in range(len(doc)):
                    try:
                        page = doc.load_page(page_num)
                        
                        # ì´ë¯¸ì§€ë¡œ ë Œë”ë§
                        pix = page.get_pixmap()
                        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
                        
                        # OCRë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        text = pytesseract.image_to_string(img, lang='kor+eng')
                        if text.strip():
                            content_parts.append(f"=== í˜ì´ì§€ {page_num + 1} (OCR) ===\n{text}")
                    except Exception as e:
                        st.warning(f"í˜ì´ì§€ {page_num + 1} OCR ì‹¤íŒ¨: {e}")
                
                doc.close()
                
                if content_parts:
                    content = "\n\n".join(content_parts)
                    doc = Document(
                        page_content=content,
                        metadata={
                            'source': file_path,
                            'file_type': 'pdf',
                            'parser': 'OCR_fallback',
                            'total_pages': len(doc)
                        }
                    )
                    documents.append(doc)
                    # st.info(f"OCRë¡œ PDF íŒŒì‹± ì„±ê³µ: {file_path}")
                    return documents
            except ImportError:
                st.info("OCR ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (pytesseract ë˜ëŠ” PILì´ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ)")
            except Exception as e:
                st.warning(f"OCRë„ ì‹¤íŒ¨: {e}")
            
            st.error(f"ëª¨ë“  PDF íŒŒì‹± ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {file_path}")
            return []
            
        except Exception as e:
            st.error(f"PDF íŒŒì¼ '{file_path}' ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

    def _load_excel_with_all_sheets(self, file_path):
        """Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ë¥¼ ë¡œë“œ"""
        try:
            import pandas as pd
            from langchain.schema import Document
            
            # Excel íŒŒì¼ì˜ ëª¨ë“  ì‹œíŠ¸ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            excel_file = pd.ExcelFile(file_path)
            sheet_names = excel_file.sheet_names
            
            documents = []
            
            for sheet_name in sheet_names:
                try:
                    # ê° ì‹œíŠ¸ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸°
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    
                    # DataFrameì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                    sheet_content = []
                    sheet_content.append(f"=== ì‹œíŠ¸: {sheet_name} ===")
                    
                    # ì»¬ëŸ¼ëª… ì¶”ê°€
                    if not df.empty:
                        sheet_content.append(f"ì»¬ëŸ¼: {', '.join(df.columns.tolist())}")
                        sheet_content.append("")
                        
                        # ë°ì´í„° í–‰ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        for index, row in df.iterrows():
                            row_data = []
                            for col in df.columns:
                                value = str(row[col]) if pd.notna(row[col]) else ""
                                row_data.append(f"{col}: {value}")
                            sheet_content.append(f"í–‰ {index + 1}: {' | '.join(row_data)}")
                    
                    # ì‹œíŠ¸ë³„ë¡œ Document ìƒì„±
                    content = "\n".join(sheet_content)
                    if content.strip():  # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        doc = LangchainDocument(
                            page_content=content,
                            metadata={
                                'source': file_path,
                                'sheet_name': sheet_name,
                                'file_type': 'excel'
                            }
                        )
                        documents.append(doc)
                        
                except Exception as e:
                    st.warning(f"ì‹œíŠ¸ '{sheet_name}' ë¡œë”© ì‹¤íŒ¨: {e}")
                    continue
            
            return documents
            
        except Exception as e:
            st.error(f"Excel íŒŒì¼ '{file_path}' ë¡œë”© ì‹¤íŒ¨: {e}")
            return []

    def _build_knowledge_graph(self):
        """ë¬¸ì„œ ë‚´ ê°œì²´/í‚¤ì›Œë“œ/ë¬¸ì¥ ê°„ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì—¬ ì§€ì‹ ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        import networkx as nx
        from collections import defaultdict
        import re
        self.knowledge_graph = nx.Graph()
        self.entity_to_docs = defaultdict(set)
        # ê°„ë‹¨í•œ ê°œì²´ ì¶”ì¶œ(ëª…ì‚¬/ê³ ìœ ëª…ì‚¬/ì´ë¦„/í‚¤ì›Œë“œ ë“±)
        for idx, doc in enumerate(self.docs):
            content = doc.page_content
            # ì˜ˆì‹œ: í•œê¸€ ì´ë¦„, íŒ€, ë¶€ì„œ, ì£¼ìš” í‚¤ì›Œë“œ ë“± ì¶”ì¶œ
            entities = set(re.findall(r"[ê°€-í£]{2,4}ë‹˜|[ê°€-í£]{2,4}íŒ€|[ê°€-í£]{2,4}ë¶€ì„œ|[ê°€-í£]{2,4}|íŒ€|ë¶€ì„œ|ë‹´ë‹¹|ì±…ì„|í”„ë¡œì„¸ìŠ¤|ìƒë‹´|ë¬¸ì˜|ì—…ë¬´|ì ˆì°¨|ê³¼ì •|ë°©ë²•|ì •ë³´|ë‚´ìš©", content))
            for entity in entities:
                self.knowledge_graph.add_node(entity, type='entity')
                self.knowledge_graph.add_edge(entity, f'doc_{idx}')
                self.entity_to_docs[entity].add(idx)
            self.knowledge_graph.add_node(f'doc_{idx}', type='doc', doc=doc)
        # ë¬¸ì„œ ê°„ ìœ ì‚¬ ê°œì²´ ì—°ê²°
        for entity, doc_idxs in self.entity_to_docs.items():
            for idx1 in doc_idxs:
                for idx2 in doc_idxs:
                    if idx1 != idx2:
                        self.knowledge_graph.add_edge(f'doc_{idx1}', f'doc_{idx2}', via=entity)

    def _graph_search(self, query, k=5):
        """Graph RAG: ì¿¼ë¦¬ì™€ ì—°ê´€ëœ ê°œì²´/ë¬¸ì„œ/ë¬¸ì¥ê¹Œì§€ ê·¸ë˜í”„ë¥¼ ë”°ë¼ í™•ì¥ ê²€ìƒ‰"""
        import re
        if not hasattr(self, 'knowledge_graph'):
            self._build_knowledge_graph()
        # ì¿¼ë¦¬ì—ì„œ ê°œì²´/í‚¤ì›Œë“œ ì¶”ì¶œ
        query_entities = set(re.findall(r"[ê°€-í£]{2,4}ë‹˜|[ê°€-í£]{2,4}íŒ€|[ê°€-í£]{2,4}ë¶€ì„œ|[ê°€-í£]{2,4}|íŒ€|ë¶€ì„œ|ë‹´ë‹¹|ì±…ì„|í”„ë¡œì„¸ìŠ¤|ìƒë‹´|ë¬¸ì˜|ì—…ë¬´|ì ˆì°¨|ê³¼ì •|ë°©ë²•|ì •ë³´|ë‚´ìš©", query))
        doc_scores = {}
        for entity in query_entities:
            if entity in self.entity_to_docs:
                for idx in self.entity_to_docs[entity]:
                    doc_scores[idx] = doc_scores.get(idx, 0) + 1
                    # ê·¸ë˜í”„ì—ì„œ ì—°ê²°ëœ ë¬¸ì„œê¹Œì§€ í™•ì¥(1-hop)
                    for neighbor in self.knowledge_graph.neighbors(f'doc_{idx}'):
                        if neighbor.startswith('doc_'):
                            doc_scores[int(neighbor.split('_')[1])] = doc_scores.get(int(neighbor.split('_')[1]), 0) + 0.5
        # ìŠ¤ì½”ì–´ ê¸°ì¤€ ìƒìœ„ kê°œ ë¬¸ì„œ ë°˜í™˜
        ranked = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)[:k]
        results = [self.docs[idx] for idx, _ in ranked]
        # ë©”íƒ€ë°ì´í„°ì— graph_score ì¶”ê°€
        for doc in results:
            doc.metadata['graph_score'] = 1
        return results

    def search(self, query, k=5):
        """Graph RAG: ë²¡í„°+í‚¤ì›Œë“œ+ê·¸ë˜í”„ ê¸°ë°˜ ê²€ìƒ‰ ê²°í•©"""
        if not self.is_loaded:
            return []
        try:
            # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
            vector_results = self.vectorstore.similarity_search(query, k=k)
            # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
            keyword_results = self._keyword_search(query, k=k)
            # 3. ê·¸ë˜í”„ ê¸°ë°˜ ê²€ìƒ‰
            graph_results = self._graph_search(query, k=k)
            # 4. ì„¸ ê²°ê³¼ë¥¼ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±°
            all_results = {}
            for doc in vector_results + keyword_results + graph_results:
                key = f"{doc.metadata.get('source', '')}_{doc.metadata.get('chunk_id', 0)}"
                if key not in all_results:
                    all_results[key] = doc
                else:
                    # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤ì½”ì–´ í•©ì‚°
                    for score_key in ['search_score', 'keyword_score', 'graph_score']:
                        prev = all_results[key].metadata.get(score_key, 0)
                        curr = doc.metadata.get(score_key, 0)
                        all_results[key].metadata[score_key] = prev + curr
            # 5. hybrid+graph_score ê³„ì‚° ë° ì •ë ¬
            results = list(all_results.values())
            for doc in results:
                doc.metadata['total_score'] = doc.metadata.get('search_score', 0) + doc.metadata.get('keyword_score', 0) + doc.metadata.get('graph_score', 0)
            results.sort(key=lambda x: x.metadata.get('total_score', 0), reverse=True)
            return results[:k]
        except Exception as e:
            st.error(f"Graph RAG ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _analyze_question_type(self, query):
        """ì§ˆë¬¸ ìœ í˜• ë¶„ì„"""
        query_lower = query.lower()
        
        # ë‹´ë‹¹ì/ì±…ì„ì ì§ˆë¬¸
        if any(keyword in query_lower for keyword in ["ë‹´ë‹¹", "ì±…ì„", "ëˆ„ê°€", "ë‹´ë‹¹ì", "ì±…ì„ì"]):
            return "responsibility"
        
        # ìƒë‹´/ë¬¸ì˜ ì§ˆë¬¸
        if any(keyword in query_lower for keyword in ["ìƒë‹´", "ë¬¸ì˜", "ë„ì›€", "ì–´ë–»ê²Œ", "ë°©ë²•"]):
            return "consultation"
        
        # ì—…ë¬´/í”„ë¡œì„¸ìŠ¤ ì§ˆë¬¸
        if any(keyword in query_lower for keyword in ["ì—…ë¬´", "í”„ë¡œì„¸ìŠ¤", "ê³¼ì •", "ì ˆì°¨", "ë°©ì‹"]):
            return "process"
        
        # ì¼ë°˜ ì •ë³´ ì§ˆë¬¸
        if any(keyword in query_lower for keyword in ["ì •ë³´", "ë‚´ìš©", "ì–´ë–¤", "ë¬´ì—‡"]):
            return "general"
        
        # ê¸°ë³¸ê°’: ë‹´ë‹¹ì ì§ˆë¬¸ (ê°€ì¥ ì¼ë°˜ì )
        return "responsibility"
    
    def _filter_by_question_type(self, results, question_type, original_query):
        """ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ê²°ê³¼ í•„í„°ë§ ë° ë³´ê°•"""
        filtered_results = []
        
        for doc in results:
            content_lower = doc.page_content.lower()
            score_boost = 0
            
            if question_type == "responsibility":
                # ë‹´ë‹¹ì ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜
                if any(keyword in content_lower for keyword in ["ë‹´ë‹¹", "ì±…ì„", "ë‹´ë‹¹ì", "ì±…ì„ì", "ë‹´ë‹¹í•˜ê³ ", "ì±…ì„ì§€ê³ "]):
                    score_boost += 3
                # ì´ë¦„ê³¼ ì§ì±… ì •ë³´ê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš°
                if any(char in content_lower for char in "ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„í•œì˜¤ì„œì‹ ê¶Œí™©ì•ˆì†¡ë¥˜ì „ê³ ë¬¸ì–‘ì†ë°°ì¡°ë°±í—ˆìœ ë‚¨ì‹¬ë…¸ì •í•˜ê³½ì„±ì°¨ì£¼ìš°êµ¬ì‹ ì„ë‚˜ì „ë¯¼"):
                    score_boost += 2
            
            elif question_type == "consultation":
                # ìƒë‹´/ë¬¸ì˜ ê´€ë ¨ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜
                if any(keyword in content_lower for keyword in ["ìƒë‹´", "ë¬¸ì˜", "ë„ì›€", "ì—°ë½", "ì ‘ìˆ˜", "ì‹ ì²­"]):
                    score_boost += 3
                # ë‹´ë‹¹ì ì •ë³´ë„ í•¨ê»˜ ìˆëŠ” ê²½ìš°
                if any(keyword in content_lower for keyword in ["ë‹´ë‹¹", "ì±…ì„", "ë‹´ë‹¹ì"]):
                    score_boost += 2
            
            elif question_type == "process":
                # ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜
                if any(keyword in content_lower for keyword in ["í”„ë¡œì„¸ìŠ¤", "ê³¼ì •", "ì ˆì°¨", "ë‹¨ê³„", "ìˆœì„œ", "ë°©ì‹"]):
                    score_boost += 3
                # êµ¬ì²´ì ì¸ ì—…ë¬´ ì„¤ëª…ì´ ìˆëŠ” ê²½ìš°
                if any(keyword in content_lower for keyword in ["ì—…ë¬´", "ì‘ì—…", "ì²˜ë¦¬", "ê´€ë¦¬"]):
                    score_boost += 2
            
            elif question_type == "general":
                # ì „ë°˜ì ì¸ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œì— ë†’ì€ ê°€ì¤‘ì¹˜
                if len(doc.page_content) > 200:  # ìƒì„¸í•œ ì •ë³´ê°€ ìˆëŠ” ë¬¸ì„œ
                    score_boost += 2
                # ë‹¤ì–‘í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œ
                keyword_count = sum(1 for keyword in ["ë‹´ë‹¹", "ì—…ë¬´", "í”„ë¡œì„¸ìŠ¤", "ìƒë‹´", "ë¬¸ì˜"] if keyword in content_lower)
                score_boost += keyword_count
            
            # ìŠ¤ì½”ì–´ ë¶€ìŠ¤íŠ¸ ì ìš©
            if score_boost > 0:
                doc.metadata['question_type_boost'] = score_boost
            
            filtered_results.append(doc)
        
        return filtered_results
    
    def _generate_query_variations(self, query):
        """ì¿¼ë¦¬ ë³€í˜• ìƒì„± (ì„ë² ë”© ê²€ìƒ‰ ìµœì í™”)"""
        variations = [query]  # ì›ë³¸ ì¿¼ë¦¬
        
        # í•œêµ­ì–´ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬ (ê³µë°± ì œê±°/ì¶”ê°€) - ë” ì •í™•í•œ ì²˜ë¦¬
        if " " in query:
            # ê³µë°±ì´ ìˆëŠ” ê²½ìš° -> ê³µë°± ì œê±° ë²„ì „ ì¶”ê°€
            no_space = query.replace(" ", "")
            variations.append(no_space)
        else:
            # ê³µë°±ì´ ì—†ëŠ” ê²½ìš° -> ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ê³µë°± ì¶”ê°€ ë²„ì „ë§Œ ìƒì„±
            # "í†µê´€ì—…ë¬´" -> "í†µê´€ ì—…ë¬´" (ê°€ì¥ ì¼ë°˜ì ì¸ í˜•íƒœ)
            if len(query) >= 4:
                # 2ê¸€ìì”© ë¶„ë¦¬í•˜ëŠ” ëŒ€ì‹ , ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                if "ì—…ë¬´" in query:
                    # "í†µê´€ì—…ë¬´" -> "í†µê´€ ì—…ë¬´"
                    base = query.replace("ì—…ë¬´", "")
                    if base:
                        variations.append(base + " ì—…ë¬´")
                elif "ê´€ë¦¬" in query:
                    # "ì¬ê³ ê´€ë¦¬" -> "ì¬ê³  ê´€ë¦¬"
                    base = query.replace("ê´€ë¦¬", "")
                    if base:
                        variations.append(base + " ê´€ë¦¬")
                else:
                    # ê¸°íƒ€ ê²½ìš°: 2ê¸€ìì”© ë¶„ë¦¬
                    for i in range(1, len(query)):
                        spaced_version = query[:i] + " " + query[i:]
                        variations.append(spaced_version)
        
        # í•œêµ­ì–´ ì´ë¦„ íŠ¹í™” ë³€í˜•
        if "ë‹˜" in query:
            # "ë‹˜" ì œê±°
            without_nim = query.replace("ë‹˜", "").strip()
            if without_nim:
                variations.append(without_nim)
        
        # ì„±ì”¨ë§Œ ì¶”ì¶œ
        if len(query) >= 2:
            if query[0] in "ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„í•œì˜¤ì„œì‹ ê¶Œí™©ì•ˆì†¡ë¥˜ì „ê³ ë¬¸ì–‘ì†ë°°ì¡°ë°±í—ˆìœ ë‚¨ì‹¬ë…¸ì •í•˜ê³½ì„±ì°¨ì£¼ìš°êµ¬ì‹ ì„ë‚˜ì „ë¯¼":
                variations.append(query[0])
        
        # ë‹¨ì–´ë³„ ë³€í˜•
        words = query.split()
        for word in words:
            if len(word) > 1:
                variations.append(word)
        
        # íŒ€/ë¶€ì„œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        team_keywords = ["íŒ€", "ë¶€ì„œ", "íŒ€ì›", "íŒ€ì¥", "ë¶€ì„œì¥", "íŒ€ì›ë“¤", "ë¶€ì„œì›", "ì†Œì†"]
        if any(keyword in query for keyword in team_keywords):
            # íŒ€ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°, ì´ë¦„ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰
            for keyword in team_keywords:
                if keyword in query:
                    # "ìš©ê°•ë‹˜ì˜ íŒ€" -> "ìš©ê°•ë‹˜" + "íŒ€" ê´€ë ¨ ê²€ìƒ‰
                    name_part = query.replace(keyword, "").replace("ì˜", "").replace("ì´", "").strip()
                    if name_part:
                        variations.append(name_part)
                        # ì´ë¦„ì—ì„œ "ë‹˜" ì œê±°
                        if "ë‹˜" in name_part:
                            variations.append(name_part.replace("ë‹˜", ""))
                    # íŒ€ ê´€ë ¨ í‚¤ì›Œë“œë„ ì¶”ê°€
                    variations.append(keyword)
        
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì²˜ë¦¬ ("ìš©ê°•ë‹˜ íŒ€ì€" -> "ìš©ê°•ë‹˜" + "íŒ€")
        if query.endswith("ì€") or query.endswith("ëŠ”") or query.endswith("ì´") or query.endswith("ê°€"):
            # ì¡°ì‚¬ ì œê±° í›„ ë‹¤ì‹œ ê²€ìƒ‰
            clean_query = query.rstrip("ì€ëŠ”ì´ê°€")
            if clean_query != query:
                variations.append(clean_query)
                # ì¡°ì‚¬ ì œê±°ëœ ì¿¼ë¦¬ë¡œ ë‹¤ì‹œ íŒ€ í‚¤ì›Œë“œ ê²€ìƒ‰
                for keyword in team_keywords:
                    if keyword in clean_query:
                        name_part = clean_query.replace(keyword, "").strip()
                        if name_part:
                            variations.append(name_part)
                            if "ë‹˜" in name_part:
                                variations.append(name_part.replace("ë‹˜", ""))
                        variations.append(keyword)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(variations))
    
    def _score_and_rank_results(self, results, original_query):
        """ê²°ê³¼ ìŠ¤ì½”ì–´ë§ ë° ì •ë ¬"""
        scored_results = []
        
        for doc in results:
            score = 0
            
            # 1. ì›ë³¸ ì¿¼ë¦¬ì™€ì˜ ì •í™•í•œ ì¼ì¹˜
            if original_query.lower() in doc.page_content.lower():
                score += 10
            
            # 2. ì¿¼ë¦¬ ë‹¨ì–´ë“¤ê³¼ì˜ ì¼ì¹˜
            query_words = original_query.lower().split()
            content_lower = doc.page_content.lower()
            for word in query_words:
                if word in content_lower:
                    score += 2
            
            # 3. í•œêµ­ì–´ ì´ë¦„ íŠ¹í™” ìŠ¤ì½”ì–´ë§
            if "ë‹˜" in original_query:
                name_without_nim = original_query.replace("ë‹˜", "").strip()
                if name_without_nim and name_without_nim.lower() in content_lower:
                    score += 5
            
            # 4. ì—…ë¬´ ê´€ë ¨ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜ (í†µê´€, êµ¬ë§¤, ì¬ë¬´ ë“±)
            business_keywords = ["í†µê´€", "êµ¬ë§¤", "ì¬ë¬´", "ë°œì£¼", "ìˆ˜ì…", "ì…ê³ ", "ì¶œê³ ", "ERP"]
            for keyword in business_keywords:
                if keyword in original_query.lower() and keyword in content_lower:
                    score += 3  # ì—…ë¬´ ê´€ë ¨ í‚¤ì›Œë“œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            
            # 5. êµ¬ì²´ì ì¸ ì—…ë¬´ ì„¤ëª…ì´ ìˆëŠ” ë¬¸ì„œì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
            if "ë‹´ë‹¹" in content_lower or "ì—…ë¬´" in content_lower or "ì—­í• " in content_lower:
                score += 2
            
            # 6. ì²­í¬ í¬ê¸°ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ (ì‘ì€ ì²­í¬ê°€ ë” ì •í™•í•  ìˆ˜ ìˆìŒ)
            chunk_size = len(doc.page_content)
            if chunk_size < 500:
                score += 1
            elif chunk_size > 2000:
                score -= 1
            
            # 7. ì§ˆë¬¸ ìœ í˜•ë³„ ë¶€ìŠ¤íŠ¸ ìŠ¤ì½”ì–´ ì¶”ê°€
            question_type_boost = doc.metadata.get('question_type_boost', 0)
            score += question_type_boost
            
            # ìŠ¤ì½”ì–´ë¥¼ ë©”íƒ€ë°ì´í„°ì— ì €ì¥
            doc.metadata['search_score'] = score
            scored_results.append(doc)
        
        # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        scored_results.sort(key=lambda x: x.metadata.get('search_score', 0), reverse=True)
        return scored_results
    
    def _keyword_search(self, query, k=5):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰ (ë¶€ë¶„ ì¼ì¹˜ ì§€ì›)"""
        try:
            # ê²€ìƒ‰ì–´ ì „ì²˜ë¦¬
            query_terms = self._extract_keywords(query)
            
            keyword_results = []
            for doc in self.docs:
                score = self._calculate_keyword_score(doc.page_content, query_terms)
                if score > 0:
                    # ì›ë³¸ ë¬¸ì„œë¥¼ ë³µì‚¬í•˜ì—¬ ìŠ¤ì½”ì–´ ì¶”ê°€
                    metadata_copy = doc.metadata.copy()
                    metadata_copy['keyword_score'] = score
                    doc_copy = LangchainDocument(
                        page_content=doc.page_content,
                        metadata=metadata_copy
                    )
                    keyword_results.append(doc_copy)
            
            # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            keyword_results.sort(key=lambda x: x.metadata.get('keyword_score', 0), reverse=True)
            return keyword_results[:k]
            
        except Exception as e:
            st.warning(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_keywords(self, query):
        """ê²€ìƒ‰ì–´ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í•œêµ­ì–´ íŠ¹ì„±ì„ ê³ ë ¤í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        
        # ì „ì²´ ê²€ìƒ‰ì–´
        keywords.append(query.lower())
        
        # í•œêµ­ì–´ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬ (ê³µë°± ì œê±°/ì¶”ê°€) - ë” ì •í™•í•œ ì²˜ë¦¬
        if " " in query:
            # ê³µë°±ì´ ìˆëŠ” ê²½ìš° -> ê³µë°± ì œê±° ë²„ì „ ì¶”ê°€
            no_space = query.replace(" ", "")
            keywords.append(no_space.lower())
        else:
            # ê³µë°±ì´ ì—†ëŠ” ê²½ìš° -> ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ê³µë°± ì¶”ê°€ ë²„ì „ë§Œ ìƒì„±
            if len(query) >= 4:
                # ì˜ë¯¸ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„ë¦¬
                if "ì—…ë¬´" in query:
                    # "í†µê´€ì—…ë¬´" -> "í†µê´€ ì—…ë¬´"
                    base = query.replace("ì—…ë¬´", "")
                    if base:
                        keywords.append((base + " ì—…ë¬´").lower())
                elif "ê´€ë¦¬" in query:
                    # "ì¬ê³ ê´€ë¦¬" -> "ì¬ê³  ê´€ë¦¬"
                    base = query.replace("ê´€ë¦¬", "")
                    if base:
                        keywords.append((base + " ê´€ë¦¬").lower())
                else:
                    # ê¸°íƒ€ ê²½ìš°: 2ê¸€ìì”© ë¶„ë¦¬
                    for i in range(1, len(query)):
                        spaced_version = query[:i] + " " + query[i:]
                        keywords.append(spaced_version.lower())
        
        # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ë“¤
        words = query.split()
        for word in words:
            if len(word) > 1:  # 1ê¸€ì ë‹¨ì–´ ì œì™¸
                keywords.append(word.lower())
        
        # í•œêµ­ì–´ íŠ¹ì„±: "ë‹˜" ì œê±° í›„ ê²€ìƒ‰
        if "ë‹˜" in query:
            without_nim = query.replace("ë‹˜", "").strip()
            if without_nim:
                keywords.append(without_nim.lower())
        
        # í•œêµ­ì–´ íŠ¹ì„±: ì„±ì”¨ë§Œ ì¶”ì¶œ
        if len(query) >= 2:
            # ì²« ê¸€ìê°€ ì„±ì”¨ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²½ìš°
            if query[0] in "ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„í•œì˜¤ì„œì‹ ê¶Œí™©ì•ˆì†¡ë¥˜ì „ê³ ë¬¸ì–‘ì†ë°°ì¡°ë°±í—ˆìœ ë‚¨ì‹¬ë…¸ì •í•˜ê³½ì„±ì°¨ì£¼ìš°êµ¬ì‹ ì„ë‚˜ì „ë¯¼":
                keywords.append(query[0].lower())
        
        # íŒ€/ë¶€ì„œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¥
        team_keywords = ["íŒ€", "ë¶€ì„œ", "íŒ€ì›", "íŒ€ì¥", "ë¶€ì„œì¥", "íŒ€ì›ë“¤", "ë¶€ì„œì›", "ì†Œì†"]
        if any(keyword in query for keyword in team_keywords):
            # íŒ€ ê´€ë ¨ ì§ˆë¬¸ì¸ ê²½ìš°, ì´ë¦„ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰
            for keyword in team_keywords:
                if keyword in query:
                    # "ìš©ê°•ë‹˜ì˜ íŒ€" -> "ìš©ê°•ë‹˜" + "íŒ€" ê´€ë ¨ ê²€ìƒ‰
                    name_part = query.replace(keyword, "").replace("ì˜", "").replace("ì´", "").strip()
                    if name_part:
                        keywords.append(name_part.lower())
                        # ì´ë¦„ì—ì„œ "ë‹˜" ì œê±°
                        if "ë‹˜" in name_part:
                            keywords.append(name_part.replace("ë‹˜", "").lower())
                    # íŒ€ ê´€ë ¨ í‚¤ì›Œë“œë„ ì¶”ê°€
                    keywords.append(keyword.lower())
        
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì²˜ë¦¬ ("ìš©ê°•ë‹˜ íŒ€ì€" -> "ìš©ê°•ë‹˜" + "íŒ€")
        if query.endswith("ì€") or query.endswith("ëŠ”") or query.endswith("ì´") or query.endswith("ê°€"):
            # ì¡°ì‚¬ ì œê±° í›„ ë‹¤ì‹œ ê²€ìƒ‰
            clean_query = query.rstrip("ì€ëŠ”ì´ê°€")
            if clean_query != query:
                keywords.append(clean_query.lower())
                # ì¡°ì‚¬ ì œê±°ëœ ì¿¼ë¦¬ë¡œ ë‹¤ì‹œ íŒ€ í‚¤ì›Œë“œ ê²€ìƒ‰
                for keyword in team_keywords:
                    if keyword in clean_query:
                        name_part = clean_query.replace(keyword, "").strip()
                        if name_part:
                            keywords.append(name_part.lower())
                            if "ë‹˜" in name_part:
                                keywords.append(name_part.replace("ë‹˜", "").lower())
                        keywords.append(keyword.lower())
        
        return list(set(keywords))  # ì¤‘ë³µ ì œê±°
    
    def _calculate_keyword_score(self, content, keywords):
        """í‚¤ì›Œë“œ ë§¤ì¹­ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        content_lower = content.lower()
        score = 0
        
        for keyword in keywords:
            if keyword in content_lower:
                # í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
                weight = len(keyword) / max(len(max(keywords, key=len)), 1)
                score += weight
                
                # ì •í™•í•œ ì¼ì¹˜ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
                if keyword == content_lower or f" {keyword} " in f" {content_lower} ":
                    score += 2
                
                # ë¶€ë¶„ ì¼ì¹˜ë„ ì ìˆ˜ ë¶€ì—¬ (í•œêµ­ì–´ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬)
                if len(keyword) >= 2:
                    # í‚¤ì›Œë“œì˜ ì¼ë¶€ê°€ í¬í•¨ë˜ì–´ë„ ì ìˆ˜ ë¶€ì—¬
                    for i in range(1, len(keyword)):
                        part1 = keyword[:i]
                        part2 = keyword[i:]
                        if part1 in content_lower and part2 in content_lower:
                            score += 0.5  # ë¶€ë¶„ ì¼ì¹˜ ì ìˆ˜
        
        return score

# ===== ì±—ë´‡ í´ë˜ìŠ¤ (FAISS ê¸°ë°˜) =====
class FileRAGChatbot:
    def __init__(self):
        self.llm_client = LLMClient()
        self.rag_system = FileRAGSystem()
        self.conversation_history = []

    def generate_response(self, user_query, provider='openai', model=None, temperature=0.7):
        try:
            import logging
            logging.basicConfig(filename='debug.log', level=logging.INFO, format='%(asctime)s - %(message)s')
            logging.info(f"ğŸš€ [DEBUG] generate_response í•¨ìˆ˜ í˜¸ì¶œë¨: '{user_query}'")
            # 1. RAGë¥¼ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„ ëœ ê²€ìƒ‰)
            relevant_docs = self.rag_system.search(user_query, k=5)

            # --- Corrective RAG: ì‹ ë¢°ë„ í‰ê°€ ë‹¨ê³„ ì¶”ê°€ ---
            evaluated_docs = []
            for doc in relevant_docs:
                eval_prompt = f"""
    ì•„ë˜ ë¬¸ì„œê°€ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ 'ì •í™•', 'ë¶€ì •í™•', 'ëª¨í˜¸' ì¤‘ ì–´ë–¤ì§€ í‰ê°€í•˜ê³ , ì´ìœ ë¥¼ í•œ ì¤„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
    [ë¬¸ì„œë‚´ìš©]
    {doc.page_content}
    [ì§ˆë¬¸]
    {user_query}
    ë‹µë³€ í˜•ì‹: ì‹ ë¢°ë„: <ì •í™•/ë¶€ì •í™•/ëª¨í˜¸>
    ì´ìœ : <ì´ìœ  í•œ ì¤„>
    """
                eval_response, _ = self.llm_client.generate_response(
                    provider, model, [{"role": "user", "content": eval_prompt}], temperature=0.0, max_tokens=256
                )
                reliability = "ëª¨í˜¸"
                reason = ""
                if eval_response:
                    for line in eval_response.splitlines():
                        if line.startswith("ì‹ ë¢°ë„:"):
                            reliability = line.split(":",1)[1].strip()
                        if line.startswith("ì´ìœ :"):
                            reason = line.split(":",1)[1].strip()
                doc.metadata['reliability'] = reliability
                doc.metadata['reliability_reason'] = reason
                evaluated_docs.append(doc)
            # --- ì‹ ë¢°ë„ í‰ê°€ ë ---

            # ë””ë²„ê·¸ ì •ë³´ (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
            if st.session_state.get('debug_mode', False):
                if evaluated_docs:
                    for i, doc in enumerate(evaluated_docs[:3], 1):
                        st.info(f"[CorrectiveRAG] ì‹ ë¢°ë„: {doc.metadata.get('reliability')} / ì´ìœ : {doc.metadata.get('reliability_reason')}")

            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ë” êµ¬ì²´ì ì¸ ì•ˆë‚´ ì œê³µ
            if not evaluated_docs:
                suggestions = self._generate_search_suggestions(user_query)
                return f"""âŒ **í•´ë‹¹ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤.**

    ğŸ’¡ **ê²€ìƒ‰ ì œì•ˆ:**
    {suggestions}

    ğŸ” **ë‹¤ë¥¸ ë°©ë²•:**
    â€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
    â€¢ ê´€ë ¨ ë¶€ì„œëª…ì´ë‚˜ ì§ì±…ëª…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
    â€¢ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•´ë³´ì„¸ìš”""", None

            # --- Corrective RAG: ì‹ ë¢°ë„ ë†’ì€ ë¬¸ì„œë§Œ ì‚¬ìš© ---
            reliable_docs = [doc for doc in evaluated_docs if doc.metadata.get('reliability') == 'ì •í™•']
            if not reliable_docs:
                warning = "âš ï¸ ì‹ ë¢°ë„ ë†’ì€ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë‹µë³€ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                context = self._build_context(evaluated_docs)
                response = warning + "\n"
            else:
                context = self._build_context(reliable_docs)
                response = ""
            # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì´ë¯¸ ë©”ì‹œì§€ í˜•ì‹)
            messages = self._build_prompt(user_query, context)
            if model is None:
                models = self.llm_client.get_models_for_provider(provider)
                model = models[0] if models else None
            if not model:
                return "ì§€ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.", None
            logging.info(f"ğŸš€ [DEBUG] LLM í˜¸ì¶œ ì‹œì‘: provider={provider}, model={model}")
            logging.info(f"ğŸš€ [DEBUG] ë©”ì‹œì§€ í˜•ì‹: {messages}")
            llm_response, error = self.llm_client.generate_response(
                provider, model, messages, temperature
            )
            logging.info(f"ğŸš€ [DEBUG] LLM ì‘ë‹µ: response={llm_response}, error={error}")
            if error:
                logging.error(f"âŒ [DEBUG] ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {error}")
                return f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {error}", None
            self.conversation_history.append({
                'user': user_query,
                'assistant': response + llm_response,
                'timestamp': datetime.now(),
                'provider': provider,
                'model': model
            })
            return response + llm_response, reliable_docs if reliable_docs else evaluated_docs
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] ì±—ë´‡ generate_response ì˜ˆì™¸: {str(e)}")
            logging.error(f"âŒ [DEBUG] ì˜ˆì™¸ íƒ€ì…: {type(e).__name__}")
            logging.error(f"âŒ [DEBUG] ì˜ˆì™¸ ìƒì„¸: {e}")
            return f"ì±—ë´‡ ì˜¤ë¥˜: {str(e)} (íƒ€ì…: {type(e).__name__})", None
    def _generate_search_suggestions(self, query):
        """ê²€ìƒ‰ì–´ì— ëŒ€í•œ ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # í•œêµ­ì–´ ì´ë¦„ ê´€ë ¨ ì œì•ˆ
        if "ë‹˜" in query:
            name_without_nim = query.replace("ë‹˜", "").strip()
            if name_without_nim:
                suggestions.append(f"â€¢ '{name_without_nim}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                suggestions.append(f"â€¢ '{name_without_nim}ë‹˜'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        
        # ì„±ì”¨ë§Œ ìˆëŠ” ê²½ìš°
        if len(query) == 1 and query in "ê¹€ì´ë°•ìµœì •ê°•ì¡°ìœ¤ì¥ì„í•œì˜¤ì„œì‹ ê¶Œí™©ì•ˆì†¡ë¥˜ì „ê³ ë¬¸ì–‘ì†ë°°ì¡°ë°±í—ˆìœ ë‚¨ì‹¬ë…¸ì •í•˜ê³½ì„±ì°¨ì£¼ìš°êµ¬ì‹ ì„ë‚˜ì „ë¯¼":
            suggestions.append(f"â€¢ '{query}ì”¨'ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
            suggestions.append(f"â€¢ '{query}ë‹˜'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        
        # íŒ€/ë¶€ì„œ ê´€ë ¨ ì œì•ˆ
        team_keywords = ["íŒ€", "ë¶€ì„œ", "íŒ€ì›", "íŒ€ì¥", "ë¶€ì„œì¥", "íŒ€ì›ë“¤", "ë¶€ì„œì›", "ì†Œì†"]
        if any(keyword in query for keyword in team_keywords):
            for keyword in team_keywords:
                if keyword in query:
                    # ì´ë¦„ ë¶€ë¶„ ì¶”ì¶œ
                    name_part = query.replace(keyword, "").replace("ì˜", "").replace("ì´", "").strip()
                    if name_part:
                        suggestions.append(f"â€¢ '{name_part}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                        if "ë‹˜" in name_part:
                            suggestions.append(f"â€¢ '{name_part.replace('ë‹˜', '')}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                    suggestions.append(f"â€¢ '{keyword}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                    suggestions.append(f"â€¢ '{name_part} {keyword}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        
        # ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì²˜ë¦¬ ì œì•ˆ
        if query.endswith("ì€") or query.endswith("ëŠ”") or query.endswith("ì´") or query.endswith("ê°€"):
            clean_query = query.rstrip("ì€ëŠ”ì´ê°€")
            if clean_query != query:
                suggestions.append(f"â€¢ '{clean_query}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                # ì¡°ì‚¬ ì œê±°ëœ ì¿¼ë¦¬ë¡œ íŒ€ í‚¤ì›Œë“œ ê²€ìƒ‰
                for keyword in team_keywords:
                    if keyword in clean_query:
                        name_part = clean_query.replace(keyword, "").strip()
                        if name_part:
                            suggestions.append(f"â€¢ '{name_part}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                            if "ë‹˜" in name_part:
                                suggestions.append(f"â€¢ '{name_part.replace('ë‹˜', '')}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                        suggestions.append(f"â€¢ '{keyword}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
                        suggestions.append(f"â€¢ '{name_part} {keyword}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        
        # í•œêµ­ì–´ ë³µí•©ëª…ì‚¬ ì²˜ë¦¬ ì œì•ˆ
        if " " in query:
            # ê³µë°±ì´ ìˆëŠ” ê²½ìš° -> ê³µë°± ì œê±° ë²„ì „ ì œì•ˆ
            no_space = query.replace(" ", "")
            suggestions.append(f"â€¢ '{no_space}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        else:
            # ê³µë°±ì´ ì—†ëŠ” ê²½ìš° -> ê³µë°± ì¶”ê°€ ë²„ì „ë“¤ ì œì•ˆ
            if len(query) >= 4:
                for i in range(1, len(query)):
                    spaced_version = query[:i] + " " + query[i:]
                    suggestions.append(f"â€¢ '{spaced_version}'ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        
        # ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì œì•ˆ
        suggestions.append("â€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        suggestions.append("â€¢ ê´€ë ¨ ë¶€ì„œëª…ì´ë‚˜ ì§ì±…ëª…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”")
        suggestions.append("â€¢ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•´ë³´ì„¸ìš”")
        
        return "\n".join(suggestions)

    def _build_context(self, relevant_docs):
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
            content = doc.page_content[:1000]
            context_parts.append(f"ë¬¸ì„œ {i} - {title}:\n{content}")
        return "\n\n".join(context_parts)

    def _build_prompt(self, user_query, context):
        system_prompt = f"""ë‹¹ì‹ ì€ ì•„ì¹´ë¼ë¼ì´í”„ì˜ ì‚¬ë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.

ã€ë‹µë³€ ê·œì¹™ã€‘
1. ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” 'í•´ë‹¹ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
3. í•œêµ­ì–´ ì´ë¦„ ê²€ìƒ‰ ì‹œ ì„±ì”¨, ì´ë¦„, ì „ì²´ ì´ë¦„, "ë‹˜" í¬í•¨/ì œì™¸ ë“± ë‹¤ì–‘í•œ í˜•íƒœë¥¼ ê³ ë ¤í•˜ì„¸ìš”.
4. ë¶€ë¶„ ì¼ì¹˜ë‚˜ ìœ ì‚¬í•œ í‘œí˜„ë„ ì¸ì‹í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
5. ë‹µë³€ì€ ì •í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”.
6. íŒ€/ë¶€ì„œ ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš°, í•´ë‹¹ ì¸ë¬¼ì˜ ì†Œì† íŒ€, íŒ€ì›ë“¤, íŒ€ì˜ ì—­í•  ë“±ì„ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

ã€ì§ˆë¬¸ ìœ í˜• ë¶„ì„ ë° ë‹µë³€ ê°€ì´ë“œã€‘
ì§ˆë¬¸ì„ ë‹¤ìŒ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜í•˜ì—¬ ì ì ˆíˆ ë‹µë³€í•˜ì„¸ìš”:

1. **ë‹´ë‹¹ì/ì±…ì„ì ì§ˆë¬¸** ("ëˆ„ê°€ ë‹´ë‹¹?", "ë‹´ë‹¹ìëŠ”?", "ì±…ì„ìëŠ”?")
   - êµ¬ì²´ì ì¸ ë‹´ë‹¹ì ì´ë¦„ê³¼ ì§ì±…ì„ ëª…ì‹œ
   - í•´ë‹¹ ì—…ë¬´ì˜ êµ¬ì²´ì ì¸ ë‚´ìš© í¬í•¨
   - ì†Œì† ë¶€ì„œë‚˜ íŒ€ ì •ë³´ í¬í•¨

2. **ì—…ë¬´/í”„ë¡œì„¸ìŠ¤ ì§ˆë¬¸** ("ì–´ë–»ê²Œ?", "í”„ë¡œì„¸ìŠ¤ëŠ”?", "ë°©ë²•ì€?")
   - ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ì˜ ë‹¨ê³„ë³„ ì„¤ëª…
   - ê´€ë ¨ ë¬¸ì„œë‚˜ ê°€ì´ë“œë¼ì¸ ì •ë³´
   - ë‹´ë‹¹ì ì •ë³´ë„ í•¨ê»˜ ì œê³µ

3. **ìƒë‹´/ë¬¸ì˜ ì§ˆë¬¸** ("ìƒë‹´ì€?", "ë¬¸ì˜ëŠ”?", "ë„ì›€ì€?")
   - ìƒë‹´ ë°©ë²•ì´ë‚˜ ë¬¸ì˜ ê²½ë¡œ ì•ˆë‚´
   - ë‹´ë‹¹ì ì •ë³´ì™€ í•¨ê»˜ ì œê³µ
   - ê´€ë ¨ ë¶€ì„œë‚˜ ì—°ë½ì²˜ ì •ë³´

4. **ì¼ë°˜ ì •ë³´ ì§ˆë¬¸** ("ì •ë³´ëŠ”?", "ë‚´ìš©ì€?", "ì–´ë–¤?")
   - í•´ë‹¹ ì£¼ì œì— ëŒ€í•œ ì „ë°˜ì ì¸ ì •ë³´ ì œê³µ
   - ê´€ë ¨ëœ ëª¨ë“  ì¸¡ë©´ì„ í¬ê´„ì ìœ¼ë¡œ ì„¤ëª…

ã€ë‹µë³€ í˜•ì‹ã€‘
ë‹µë³€ì„ ì œê³µí•  ë•ŒëŠ” ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:

ğŸ“‹ **í•µì‹¬ ì •ë³´**
- ê°€ì¥ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°„ë‹¨íˆ ìš”ì•½

ğŸ“„ **ìƒì„¸ ë‚´ìš©**
- êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ë‹¨ë½ë³„ë¡œ ì •ë¦¬
- ê´€ë ¨ ë°ì´í„°ë‚˜ ìˆ˜ì¹˜ê°€ ìˆë‹¤ë©´ í¬í•¨

ğŸ”— **ê´€ë ¨ ì •ë³´**
- ì¶”ê°€ë¡œ ê´€ë ¨ëœ ì •ë³´ë‚˜ ì°¸ê³ ì‚¬í•­

ğŸ“š **ì°¸ê³  ë¬¸ì„œ**
- ë‹µë³€ì˜ ê·¼ê±°ê°€ ëœ ë¬¸ì„œ ì •ë³´

ã€ê²€ìƒ‰ íŒã€‘
- "ì¥í˜ì‹ ë‹˜" â†’ "ì¥í˜ì‹ ", "í˜ì‹ ", "ì¥" ë“±ìœ¼ë¡œë„ ê²€ìƒ‰
- "ê¹€ì˜ìˆ˜ë‹˜" â†’ "ê¹€ì˜ìˆ˜", "ì˜ìˆ˜", "ê¹€" ë“±ìœ¼ë¡œë„ ê²€ìƒ‰
- "ìš©ê°•ë‹˜ì˜ íŒ€" â†’ "ìš©ê°•ë‹˜", "ìµœìš©ê°•", "íŒ€", "ë¶€ì„œ" ë“±ìœ¼ë¡œë„ ê²€ìƒ‰
- "í†µê´€ì—…ë¬´" â†’ "í†µê´€ ì—…ë¬´", "í†µê´€", "êµ¬ë§¤", "ì¬ë¬´" ë“±ìœ¼ë¡œë„ ê²€ìƒ‰
- ì§ì±…ëª…, ë¶€ì„œëª…, ì—­í•  ë“±ë„ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ê²€ìƒ‰

ã€ì—…ë¬´ ê´€ë ¨ ê²€ìƒ‰ ê°€ì´ë“œã€‘
- í†µê´€ ì—…ë¬´ëŠ” ì£¼ë¡œ êµ¬ë§¤/ì¬ë¬´ ê´€ë ¨ ì—…ë¬´ì™€ ì—°ê´€ë¨
- ë°œì£¼, ìˆ˜ì…, í†µê´€, ì…ê³ , ì¶œê³  ê´€ë¦¬ê°€ í¬í•¨ëœ ì—…ë¬´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¾ìŒ
- êµ¬ì²´ì ì¸ ì—…ë¬´ ì„¤ëª…ì´ ìˆëŠ” ë¬¸ì„œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì°¸ê³ í•¨

ã€íŒ€/ë¶€ì„œ ê´€ë ¨ ì§ˆë¬¸ ì²˜ë¦¬ã€‘
- "~ë‹˜ì˜ íŒ€" ì§ˆë¬¸ ì‹œ: í•´ë‹¹ ì¸ë¬¼ì˜ ì†Œì† íŒ€, íŒ€ì› êµ¬ì„±, íŒ€ì˜ ì—­í• ê³¼ ì±…ì„ì„ í¬í•¨í•˜ì—¬ ë‹µë³€
- "~íŒ€" ì§ˆë¬¸ ì‹œ: í•´ë‹¹ íŒ€ì˜ êµ¬ì„±ì›, ì—­í• , ì±…ì„, ì£¼ìš” ì—…ë¬´ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€
- íŒ€ì› ì •ë³´ê°€ ìˆëŠ” ê²½ìš°: íŒ€ì›ë“¤ì˜ ì´ë¦„, ì—­í• , ë‹´ë‹¹ ì—…ë¬´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ

ã€ì§ˆë¬¸ ìœ í˜•ë³„ ë‹µë³€ ì˜ˆì‹œã€‘
- "í•´ì™¸ ì†Œì‹± ë‹´ë‹¹ìëŠ”?" â†’ ë‹´ë‹¹ì ì´ë¦„, ì§ì±…, ì†Œì†, êµ¬ì²´ ì—…ë¬´ ì„¤ëª…
- "í•´ì™¸ ì†Œì‹± ìƒë‹´ì€?" â†’ ìƒë‹´ ë°©ë²•, ë‹´ë‹¹ì, ë¬¸ì˜ ê²½ë¡œ, ê´€ë ¨ í”„ë¡œì„¸ìŠ¤
- "í•´ì™¸ ì†Œì‹± ì—…ë¬´ëŠ”?" â†’ ì—…ë¬´ ë‚´ìš©, í”„ë¡œì„¸ìŠ¤, ë‹´ë‹¹ì, ê´€ë ¨ ì •ë³´ ì „ì²´
- "í•´ì™¸ ì†Œì‹± ì •ë³´ëŠ”?" â†’ ì „ë°˜ì ì¸ ì •ë³´, ë‹´ë‹¹ì, ì—…ë¬´, í”„ë¡œì„¸ìŠ¤ ì¢…í•©

ã€ì¼ê´€ì„± ìœ ì§€ ê·œì¹™ã€‘
1. ê°™ì€ ì£¼ì œì— ëŒ€í•´ ì§ˆë¬¸ì´ ë‹¤ë¥´ë©´, ì§ˆë¬¸ì˜ êµ¬ì²´ì„±ì— ë”°ë¼ ë‹µë³€ì˜ ê¹Šì´ë¥¼ ì¡°ì ˆ
2. ë‹´ë‹¹ì ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, ëª¨ë“  ê´€ë ¨ ì§ˆë¬¸ì—ì„œ ì¼ê´€ë˜ê²Œ ì œê³µ
3. ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°, êµ¬ì²´ì ì¸ ë‹¨ê³„ì™€ ë‹´ë‹¹ìë¥¼ í•¨ê»˜ ì„¤ëª…
4. ìƒë‹´/ë¬¸ì˜ ì§ˆë¬¸ì˜ ê²½ìš°, ë‹´ë‹¹ì ì •ë³´ì™€ í•¨ê»˜ ì ‘ê·¼ ë°©ë²•ì„ ì•ˆë‚´
"""
        # ì§ˆë¬¸ ìœ í˜• ë¶„ì„
        question_type = self.rag_system._analyze_question_type(user_query)
        question_type_korean = {
            "responsibility": "ë‹´ë‹¹ì/ì±…ì„ì ì§ˆë¬¸",
            "consultation": "ìƒë‹´/ë¬¸ì˜ ì§ˆë¬¸", 
            "process": "ì—…ë¬´/í”„ë¡œì„¸ìŠ¤ ì§ˆë¬¸",
            "general": "ì¼ë°˜ ì •ë³´ ì§ˆë¬¸"
        }.get(question_type, "ì¼ë°˜ ì§ˆë¬¸")
        
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"ì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n{context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_query}\n\nì§ˆë¬¸ ìœ í˜•: {question_type_korean}"}
        ]

    def clear_history(self):
        self.conversation_history = []
    
    def generate_streaming_response(self, user_query, provider='openai', model=None, temperature=0.7):
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë©”ì„œë“œ"""
        try:
            # 1. RAGë¥¼ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self.rag_system.search(user_query, k=5)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            if not relevant_docs:
                suggestions = self._generate_search_suggestions(user_query)
                return f"""âŒ **í•´ë‹¹ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤.**

ğŸ’¡ **ê²€ìƒ‰ ì œì•ˆ:**
{suggestions}

ğŸ” **ë‹¤ë¥¸ ë°©ë²•:**
â€¢ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
â€¢ ê´€ë ¨ ë¶€ì„œëª…ì´ë‚˜ ì§ì±…ëª…ìœ¼ë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”
â€¢ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê²€ìƒ‰ ë²”ìœ„ë¥¼ í™•ì¥í•´ë³´ì„¸ìš”""", None, None
            
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(relevant_docs)
            # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì´ë¯¸ ë©”ì‹œì§€ í˜•ì‹)
            messages = self._build_prompt(user_query, context)
            
            # 4. ëª¨ë¸ ì„¤ì •
            if model is None:
                models = self.llm_client.get_models_for_provider(provider)
                model = models[0] if models else None
            if not model:
                return "ì§€ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.", None, None
            
            # 5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            return self._stream_response(provider, model, messages, temperature, relevant_docs)
            
        except Exception as e:
            import logging
            logging.error(f"âŒ [DEBUG] ì±—ë´‡ generate_streaming_response ì˜ˆì™¸: {str(e)}")
            return f"ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", None, None
    
    def _stream_response(self, provider, model, messages, temperature, relevant_docs):
        """ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        import openai
        import anthropic
        import requests
        import json
        
        full_response = ""
        
        try:
            if provider == 'openai':
                # OpenAI ìŠ¤íŠ¸ë¦¬ë°
                client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                stream = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        yield full_response, relevant_docs, None
                        
            elif provider == 'anthropic':
                # Anthropic ìŠ¤íŠ¸ë¦¬ë°
                client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
                with client.messages.stream(
                    model=model,
                    max_tokens=2000,
                    temperature=temperature,
                    messages=messages
                ) as stream:
                    for text in stream.text_stream:
                        full_response += text
                        yield full_response, relevant_docs, None
                        
            elif provider == 'ollama':
                # Ollama ìŠ¤íŠ¸ë¦¬ë°
                ollama_messages = []
                for msg in messages:
                    if msg['role'] == 'user':
                        ollama_messages.append({
                            'role': 'user',
                            'content': msg['content']
                        })
                    elif msg['role'] == 'assistant':
                        ollama_messages.append({
                            'role': 'assistant',
                            'content': msg['content']
                        })
                
                response = requests.post(
                    "http://localhost:11434/api/chat",
                    json={
                        "model": model,
                        "messages": ollama_messages,
                        "stream": True,
                        "options": {
                            "temperature": temperature
                        }
                    },
                    stream=True,
                    timeout=60
                )
                
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line.decode('utf-8'))
                            if 'message' in data and 'content' in data['message']:
                                chunk = data['message']['content']
                                full_response += chunk
                                yield full_response, relevant_docs, None
                        except json.JSONDecodeError:
                            continue
                            
            elif provider == 'perplexity':
                # Perplexity ìŠ¤íŠ¸ë¦¬ë°
                # ë©”ì‹œì§€ í˜•ì‹ ìˆ˜ì • (ìŠ¤íŠ¸ë¦¬ë°ìš©)
                formatted_messages = []
                seen_messages = set()
                
                for msg in messages:
                    message_key = f"{msg['role']}:{msg['content']}"
                    if message_key not in seen_messages:
                        seen_messages.add(message_key)
                        if msg['role'] == 'system':
                            formatted_messages.append({"role": "system", "content": msg['content']})
                        elif msg['role'] == 'user':
                            formatted_messages.append({"role": "user", "content": msg['content']})
                        elif msg['role'] == 'assistant':
                            formatted_messages.append({"role": "assistant", "content": msg['content']})
                
                # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ìœ ì§€
                if len(formatted_messages) > 1:
                    final_messages = []
                    for msg in formatted_messages:
                        if msg['role'] == 'system':
                            final_messages.append(msg)
                    
                    for msg in reversed(formatted_messages):
                        if msg['role'] == 'user':
                            final_messages.append(msg)
                            break
                    
                    formatted_messages = final_messages
                
                # Perplexity ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
                client = openai.OpenAI(
                    api_key=os.getenv('PERPLEXITY_API_KEY'),
                    base_url="https://api.perplexity.ai"
                )
                
                # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
                test_models = ["sonar-pro", "sonar-small-chat"]
                stream_success = False
                
                for test_model in test_models:
                    try:
                        stream = client.chat.completions.create(
                            model=test_model,
                            messages=formatted_messages,
                            temperature=temperature,
                            stream=True
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                full_response += chunk.choices[0].delta.content
                                yield full_response, relevant_docs, None
                        
                        stream_success = True
                        break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                        
                    except Exception as model_error:
                        error_msg = str(model_error)
                        
                        if "Invalid model" in error_msg or "model not found" in error_msg.lower():
                            continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                        else:
                            # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì¤‘ë‹¨
                            raise model_error
                
                if not stream_success:
                    # í´ë°±ìœ¼ë¡œ ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
                    response = self.llm_client.generate_response(
                        provider, model, 
                        messages, 
                        temperature
                    )
                    if isinstance(response, tuple):
                        full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                    else:
                        full_response = response
                    yield full_response, relevant_docs, None
                    
            elif provider == 'google':
                # Google Gemini ìŠ¤íŠ¸ë¦¬ë°
                from langchain_google_genai import ChatGoogleGenerativeAI
                client = ChatGoogleGenerativeAI(
                    model=model,
                    google_api_key=os.getenv('GOOGLE_API_KEY'),
                    temperature=temperature,
                    max_output_tokens=2000
                )
                # LangChainì˜ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
                for chunk in client.stream(messages):
                    if hasattr(chunk, 'content'):
                        full_response += chunk.content
                        yield full_response, relevant_docs, None
            else:
                # ê¸°íƒ€ ì œê³µìëŠ” ì¼ë°˜ ì‘ë‹µ
                response = self.llm_client.generate_response(
                    provider, model, 
                    messages, 
                    temperature
                )
                if isinstance(response, tuple):
                    full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                else:
                    full_response = response
                yield full_response, relevant_docs, None
                
        except Exception as e:
            error_msg = f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
            yield full_response, relevant_docs, error_msg

# ===== Perplexity APIë¥¼ í™œìš©í•œ ì‹œì¥ ì¡°ì‚¬ ê¸°ëŠ¥ =====
def perform_perplexity_search(query, debug_mode=False):
    """Perplexity APIë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ ìˆ˜í–‰"""
    api_key = os.getenv('PERPLEXITY_API_KEY')
    if not api_key:
        st.error("Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    try:
        url = "https://api.perplexity.ai/chat/completions"
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "sonar-pro",
            "messages": [
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ì£¼ì œì— ëŒ€í•´ ìµœì‹  ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸íˆ ì¡°ì‚¬í•´ì£¼ì„¸ìš”: {query}"
                }
            ],
            "max_tokens": 2000,
            "temperature": 0.1
        }
        
        if debug_mode:
            st.write("=== Perplexity API ìš”ì²­ ë””ë²„ê·¸ ì •ë³´ ===")
            st.write(f"URL: {url}")
            st.write(f"Headers: {headers}")
            st.write(f"Data: {data}")
        
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        if debug_mode:
            st.write("\n=== Perplexity API ì‘ë‹µ ë””ë²„ê·¸ ì •ë³´ ===")
            st.write(f"Status Code: {response.status_code}")
            st.write(f"Response Headers: {dict(response.headers)}")
        
        if response.status_code == 200:
            result = response.json()
            if 'choices' in result and len(result['choices']) > 0:
                content = result['choices'][0]['message']['content']
                if debug_mode:
                    st.write(f"Content: {content[:500]}...")
                return content
            else:
                st.error("Perplexity API ì‘ë‹µì—ì„œ contentë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
        else:
            error_msg = f"Perplexity API ì˜¤ë¥˜ (ìƒíƒœ ì½”ë“œ: {response.status_code})"
            if debug_mode:
                st.write(f"Error Response: {response.text}")
            st.error(error_msg)
            return None
            
    except requests.exceptions.Timeout:
        st.error("Perplexity API ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
        return None
    except requests.exceptions.RequestException as e:
        st.error(f"Perplexity API ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return None
    except Exception as e:
        st.error(f"Perplexity API ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ===== ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ ê¸°ëŠ¥ =====
def analyze_with_finance_persona(user_query, persona_key, persona_info, rag_context="", market_research="", model_name="claude-3-7-sonnet-latest"):
    """ì¬ë¬´ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ë¡œ ë¶„ì„ ìˆ˜í–‰"""
    try:
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        analysis_prompt = f"""
ë‹¤ìŒ ì£¼ì œ/ì§ˆë¬¸ì— ëŒ€í•´ {persona_info['role']} ê´€ì ì—ì„œ ì „ë¬¸ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

ã€ë¶„ì„ ëŒ€ìƒã€‘
{user_query}

ã€ì „ë¬¸ê°€ ì—­í• ã€‘
{persona_info['name']} ({persona_info['emoji']})
{persona_info['expertise']}

ã€ë¶„ì„ ê´€ì ã€‘
{persona_info['perspective']}

"""

        # RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if rag_context:
            analysis_prompt += f"""
ã€ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ã€‘
{rag_context}

ì´ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.

"""

        # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if market_research:
            analysis_prompt += f"""
ã€ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ã€‘
{market_research}

ì´ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.

"""

        analysis_prompt += f"""
ã€ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„ ìš”êµ¬ì‚¬í•­ã€‘
- {persona_info['perspective']}
- ì—…ê³„ ìµœê³  ìˆ˜ì¤€ì˜ ìƒì„¸í•˜ê³  ê¹Šì´ ìˆëŠ” ì „ë¬¸ ë¶„ì„ ì œê³µ
- êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì§€í‘œ, ë°ì´í„°ë¥¼ í¬í•¨í•œ ì •ëŸ‰ì  ë¶„ì„
- ë‹¨ê³„ë³„ ì‹¤í–‰ ê³„íšê³¼ íƒ€ì„ë¼ì¸ì„ í¬í•¨í•œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ
- ë¦¬ìŠ¤í¬ ìš”ì¸ê³¼ ì™„í™” ì „ëµì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„
- ì„±ê³¼ ì¸¡ì • ì§€í‘œ(KPI)ì™€ ëª¨ë‹ˆí„°ë§ ë°©ì•ˆ ëª…ì‹œ
- ë‹¤ë¥¸ ë¶€ì„œì™€ì˜ êµ¬ì²´ì  í˜‘ì—… ë°©ì•ˆê³¼ ì—­í•  ë¶„ë‹´
- ì˜ˆìƒ ë¹„ìš©, ë¦¬ì†ŒìŠ¤, ê¸°ê°„ì„ í¬í•¨í•œ ìƒì„¸í•œ ì‹¤í–‰ ê³„íš
{'- ì œê³µëœ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ë¥¼ ë°˜ë“œì‹œ ë¶„ì„ì— í™œìš©í•˜ê³  ì¸ì‚¬ì´íŠ¸ ë„ì¶œ' if rag_context else ''}
{'- ì œê³µëœ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì™¸ë¶€ í™˜ê²½ì„ ê³ ë ¤í•œ ë¶„ì„ ìˆ˜í–‰' if market_research else ''}

ã€ì‘ë‹µ í˜•ì‹ã€‘
## í•µì‹¬ ë¶„ì„
(2-3ì¤„ë¡œ í•µì‹¬ í¬ì¸íŠ¸ ìš”ì•½)

## ìƒì„¸ ë¶„ì„
(ì „ë¬¸ ë¶„ì•¼ ê´€ì ì—ì„œ ìƒì„¸í•œ ë¶„ì„)

## ì‹¤í–‰ ì œì•ˆ
(êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œë“¤)

## ë‹¤ë¥¸ ë¶€ì„œ í˜‘ì—… ë°©ì•ˆ
(ë‹¤ë¥¸ ì „ë¬¸ê°€ì™€ì˜ í˜‘ì—…ì´ í•„ìš”í•œ ë¶€ë¶„)

## ë¦¬ìŠ¤í¬ ë° ê³ ë ¤ì‚¬í•­
(ì£¼ì˜í•´ì•¼ í•  ì ë“¤)

## ì°¸ê³  ìë£Œ
(ë¶„ì„ì— í™œìš©í•œ ë‚´ë¶€ ë°ì´í„° ë° ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ ìš”ì•½)
"""

        # ëª¨ë¸ ì •ë³´ ë””ë²„ê¹…
        st.info(f"{persona_key} ë¶„ì„ ì‹œì‘ - ëª¨ë¸: {model_name}")
        
        result = get_ai_response(analysis_prompt, model_name, persona_info['system_prompt'])
        
        if result and not result.startswith("ì‘ë‹µ ìƒì„± ì˜¤ë¥˜"):
            return result
        else:
            st.error(f"âŒ {persona_key} ë¶„ì„ ì‹¤íŒ¨: {result}")
            return None

    except Exception as e:
        st.error(f"âŒ {persona_key} ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def analyze_persona_concurrent_finance(args):
    """ThreadPoolExecutor ë˜í¼ í•¨ìˆ˜"""
    user_query, persona_key, persona_info, rag_context, market_research, model_name = args
    try:
        result = analyze_with_finance_persona(user_query, persona_key, persona_info, rag_context, market_research, model_name)
        if result is None:
            return persona_key, f"AI ì‘ë‹µì´ Noneì…ë‹ˆë‹¤. API í‚¤ë‚˜ ëª¨ë¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.", False
        return persona_key, result, True
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        return persona_key, f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}\nìƒì„¸: {error_details}", False

def synthesize_finance_analysis(user_query, persona_analyses, rag_context="", market_research="", model_name="claude-3-7-sonnet-latest"):
    """ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ìê°€ ëª¨ë“  ë¶„ì„ì„ ì¢…í•©"""
    try:
        synthesis_prompt = f"""
ë‹¤ìŒì€ ìš°ë¦¬ íšŒì‚¬ ì¬ë¬´ ì „ë¬¸ê°€ë“¤ì´ ë¶„ì„í•œ ë‚´ìš©ì…ë‹ˆë‹¤. 
ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ìë¡œì„œ ì´ë“¤ì˜ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ì‹¤í˜„ ê°€ëŠ¥í•œ ì „ëµê³¼ ì‹¤í–‰ ê³„íšì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ã€ì›ë˜ ì£¼ì œ/ì§ˆë¬¸ã€‘
{user_query}

ã€ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (RAG íŒŒì¼ ê¸°ë°˜)ã€‘
{rag_context if rag_context else "ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° ì—†ìŒ"}

ã€ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (Perplexity ê¸°ë°˜)ã€‘
{market_research if market_research else "ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ ì—†ìŒ"}

ã€ê° ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼ã€‘
"""
        
        for persona_key, analysis in persona_analyses.items():
            if analysis and analysis.get('result'):
                persona_info = FINANCE_PERSONAS[persona_key]
                synthesis_prompt += f"""
--- {persona_info['emoji']} {persona_info['name']} ë¶„ì„ ---
{analysis['result']}

"""
        
        synthesis_prompt += """
ã€ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ìš”êµ¬ì‚¬í•­ã€‘
- ê° ì „ë¬¸ê°€ì˜ ê´€ì ì„ ê· í˜•ìˆê²Œ ê³ ë ¤
- ì‹¤í˜„ ê°€ëŠ¥í•œ ìš°ì„ ìˆœìœ„ ì„¤ì •
- êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ íƒ€ì„ë¼ì¸ ìˆ˜ë¦½
- ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì˜ ì¢…í•©ì  í‰ê°€
- ëª…í™•í•œ ì˜ì‚¬ê²°ì • ë°©í–¥ ì œì‹œ
- ì„±ê³¼ ì¸¡ì • ì§€í‘œì™€ ëª¨ë‹ˆí„°ë§ ë°©ì•ˆ í¬í•¨

ã€ì‘ë‹µ í˜•ì‹ã€‘
## ğŸ¯ í•µì‹¬ ê²°ë¡  ë° ì „ëµì  ì œì•ˆ
(ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ìë¡œì„œì˜ í•µì‹¬ íŒë‹¨ê³¼ ì œì•ˆì‚¬í•­)

## ğŸ“Š ì „ë¬¸ê°€ ë¶„ì„ ì¢…í•©
(ê° ì „ë¬¸ê°€ ì˜ê²¬ì˜ í•µì‹¬ í¬ì¸íŠ¸ë“¤)

## ğŸš€ í†µí•© ì‹¤í–‰ ê³„íš
(ë‹¨ê³„ë³„ ì‹¤í–‰ ë°©ì•ˆê³¼ ìš°ì„ ìˆœìœ„)

## âš–ï¸ ë¦¬ìŠ¤í¬ vs ê¸°íšŒ
(ì¢…í•©ì  ë¦¬ìŠ¤í¬-ê¸°íšŒ ë¶„ì„)

## ğŸ“ˆ ì„±ê³¼ ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§
(ì„±ê³¼ ì¸¡ì • ë°©ë²•ê³¼ KPI)

## ğŸ’¡ ìµœì¢… ë©”ì‹œì§€
(ì¡°ì§ì— ì „ë‹¬í•  í•µì‹¬ ë©”ì‹œì§€)

## ğŸ“‹ ì°¸ê³  ìë£Œ ë° ë ˆí¼ëŸ°ìŠ¤
### ğŸ“ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (RAG íŒŒì¼)
{rag_context if rag_context else "ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° ì—†ìŒ"}

### ğŸŒ ì‹œì¥ ì¡°ì‚¬ ë°ì´í„° (Perplexity)
{market_research if market_research else "ì‹œì¥ ì¡°ì‚¬ ë°ì´í„° ì—†ìŒ"}

### ğŸ“Š ë°ì´í„° ì¶œì²˜ ë° ì‹ ë¢°ë„
- **ë‚´ë¶€ ë°ì´í„°**: íšŒì‚¬ ë‚´ë¶€ ì¬ë¬´ ë¬¸ì„œ ë° ë³´ê³ ì„œ
- **ì‹œì¥ ë°ì´í„°**: Perplexity AIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ì‚¬
- **ë¶„ì„ ë°©ë²•**: ì „ë¬¸ê°€ë³„ ë‹¤ê°ë„ ë¶„ì„ + AI ê¸°ë°˜ ì¢…í•© í‰ê°€
"""
        
        return get_ai_response(synthesis_prompt, model_name, REPORT_SYNTHESIZER['system_prompt'])

    except Exception as e:
        st.error(f"âŒ ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

# ===== AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜ =====
def get_ai_response(prompt, model_name, system_prompt=""):
    """AI ì‘ë‹µ ìƒì„± (LLMClient í™œìš©)"""
    try:
        # LLMClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm_client = LLMClient()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
        available_providers = llm_client.get_available_providers()
        if not available_providers:
            return "ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ëª¨ë¸ ì´ë¦„ì—ì„œ ì œê³µì ì¶”ì¶œ
        provider = None
        if model_name:
            # ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ì œê³µì íŒë‹¨
            if model_name.startswith('gpt-'):
                provider = 'openai'
            elif model_name.startswith('claude-'):
                provider = 'anthropic'
            elif model_name.startswith('sonar-') or model_name.startswith('llama-'):
                provider = 'perplexity'
            elif model_name.startswith('gemini-'):
                provider = 'google'
            elif ':' in model_name:  # ollama ëª¨ë¸
                provider = 'ollama'
        
        # ì œê³µìë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
        if not provider or provider not in available_providers:
            # OpenAIë¥¼ ìš°ì„ ìœ¼ë¡œ, ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì
            provider = 'openai' if 'openai' in available_providers else available_providers[0]
        
        # ëª¨ë¸ ì„¤ì •
        if not model_name:
            models = llm_client.get_models_for_provider(provider)
            model_name = models[0] if models else None
        
        if not model_name:
            return "ì§€ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # ë””ë²„ê¹… ì •ë³´
        st.info(f"AI ì‘ë‹µ ìƒì„± - ì œê³µì: {provider}, ëª¨ë¸: {model_name}")
        
        # ì‘ë‹µ ìƒì„±
        response, error = llm_client.generate_response(provider, model_name, messages, temperature=0.7)
        
        if error:
            st.error(f"LLM ì‘ë‹µ ì˜¤ë¥˜: {error}")
            return f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {error}"
        
        return response
        
    except Exception as e:
        return f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"

# ===== PDF ìƒì„± ê¸°ëŠ¥ =====
def create_finance_analysis_pdf(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """ì¬ë¬´ ë¶„ì„ ê²°ê³¼ë¥¼ PDFë¡œ ìƒì„±"""
    try:
        # ReportLabì„ ìš°ì„  ì‚¬ìš© (í•œê¸€ í°íŠ¸ ë¬¸ì œ í•´ê²°)
        return create_finance_analysis_pdf_reportlab(user_query, persona_analyses, final_report, rag_context, market_research)
    except Exception as e:
        st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_finance_analysis_pdf_fpdf2(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """FPDF2ë¥¼ ì‚¬ìš©í•œ ì¬ë¬´ ë¶„ì„ PDF ìƒì„±"""
    try:
        # PDF ë²„í¼ ìƒì„±
        pdf_buffer = io.BytesIO()
        
        # FPDF2 ê°ì²´ ìƒì„±
        pdf = FPDF()
        pdf.add_page()
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            import platform
            system = platform.system()
            
            # ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ê²½ë¡œ
            font_paths = []
            if system == "Darwin":  # macOS
                font_paths = [
                    "/System/Library/Fonts/AppleGothic.ttf",
                    "/System/Library/Fonts/Supplemental/Arial Unicode MS.ttf",
                    "/Library/Fonts/AppleGothic.ttf"
                ]
            elif system == "Windows":
                font_paths = [
                    "C:/Windows/Fonts/malgun.ttf",
                    "C:/Windows/Fonts/NanumGothic.ttf",
                    "C:/Windows/Fonts/gulim.ttc"
                ]
            else:  # Linux
                font_paths = [
                    "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                    "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"
                ]
            
            # í°íŠ¸ ë“±ë¡ ì‹œë„
            font_registered = False
            for font_path in font_paths:
                try:
                    if os.path.exists(font_path):
                        pdf.add_font('KoreanFont', '', font_path, uni=True)
                        font_name = 'KoreanFont'
                        font_registered = True
                        st.info(f"í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                        break
                except Exception as e:
                    st.warning(f"í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_path}): {e}")
                    continue
            
            # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            if not font_registered:
                # FPDF2ì˜ ë‚´ì¥ ìœ ë‹ˆì½”ë“œ í°íŠ¸ ì‹œë„
                try:
                    pdf.add_font('DejaVu', '', uni=True)
                    font_name = 'DejaVu'
                    st.info("DejaVu ìœ ë‹ˆì½”ë“œ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                except:
                    font_name = 'Arial'
                    st.warning("í•œê¸€ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            font_name = 'Arial'
            st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ì œëª©
        pdf.set_font(font_name, 'B', 16)
        pdf.set_text_color(30, 58, 138)  # darkblue
        pdf.cell(0, 10, 'ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ', ln=True, align='C')
        pdf.ln(10)
        
        # ë¶„ì„ ìš”ì²­
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ë¶„ì„ ìš”ì²­', ln=True)
        pdf.set_font(font_name, '', 10)
        pdf.set_text_color(0, 0, 0)
        pdf.multi_cell(0, 6, user_query)
        pdf.ln(5)
        
        # ë¶„ì„ ì¼ì‹œ
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ë¶„ì„ ì¼ì‹œ', ln=True)
        pdf.set_font(font_name, '', 10)
        pdf.set_text_color(0, 0, 0)
        pdf.cell(0, 6, datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M'), ln=True)
        pdf.ln(5)
        
        # ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (ìˆëŠ” ê²½ìš°)
        if rag_context:
            pdf.set_font(font_name, 'B', 12)
            pdf.set_text_color(30, 58, 138)
            pdf.cell(0, 8, 'ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°', ln=True)
            pdf.set_font(font_name, '', 10)
            pdf.set_text_color(0, 0, 0)
            pdf.multi_cell(0, 6, 'ë¶„ì„ì— í™œìš©ëœ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.')
            pdf.ln(5)
        
        # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
        if market_research:
            pdf.set_font(font_name, 'B', 12)
            pdf.set_text_color(30, 58, 138)
            pdf.cell(0, 8, 'ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼', ln=True)
            pdf.set_font(font_name, '', 10)
            pdf.set_text_color(0, 0, 0)
            pdf.multi_cell(0, 6, market_research[:1000] + ('...' if len(market_research) > 1000 else ''))
            pdf.ln(5)
        
        # ì „ë¬¸ê°€ë³„ ë¶„ì„
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ì „ë¬¸ê°€ë³„ ë¶„ì„', ln=True)
        pdf.ln(5)
        
        for persona_key, analysis in persona_analyses.items():
            if analysis.get('success') and analysis.get('result'):
                persona_info = FINANCE_PERSONAS[persona_key]
                
                # ì „ë¬¸ê°€ ì œëª©
                pdf.set_font(font_name, 'B', 11)
                pdf.set_text_color(30, 58, 138)
                pdf.cell(0, 6, f"{persona_info['name']}", ln=True)
                
                # ë¶„ì„ ë‚´ìš©
                pdf.set_font(font_name, '', 10)
                pdf.set_text_color(0, 0, 0)
                pdf.multi_cell(0, 6, analysis['result'])
                pdf.ln(5)
        
        # ìµœì¢… ì¢…í•© ë³´ê³ ì„œ
        if final_report:
            pdf.add_page()
            pdf.set_font(font_name, 'B', 12)
            pdf.set_text_color(30, 58, 138)
            pdf.cell(0, 8, 'ìµœì¢… ì¢…í•© ë³´ê³ ì„œ', ln=True)
            pdf.ln(5)
            
            pdf.set_font(font_name, '', 10)
            pdf.set_text_color(0, 0, 0)
            pdf.multi_cell(0, 6, final_report)
        
        # PDF ì¶œë ¥
        pdf.output(pdf_buffer)
        pdf_buffer.seek(0)
        
        return pdf_buffer
        
    except Exception as e:
        st.error(f"FPDF2 PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_finance_analysis_pdf_weasyprint(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """WeasyPrintë¥¼ ì‚¬ìš©í•œ ì¬ë¬´ ë¶„ì„ PDF ìƒì„±"""
    try:
        # HTML ì½˜í…ì¸  ìƒì„±
        html_content = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="utf-8">
            <title>ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ</title>
            <style>
                body {{
                    font-family: 'Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Arial Unicode MS', sans-serif;
                    font-size: 12px;
                    line-height: 1.6;
                    margin: 20px;
                    color: #333;
                }}
                .title {{
                    font-size: 18px;
                    font-weight: bold;
                    text-align: center;
                    color: #1e3a8a;
                    margin-bottom: 30px;
                    border-bottom: 2px solid #1e3a8a;
                    padding-bottom: 10px;
                }}
                .section {{
                    margin-bottom: 20px;
                }}
                .section-title {{
                    font-size: 14px;
                    font-weight: bold;
                    color: #1e3a8a;
                    margin-bottom: 10px;
                    border-left: 4px solid #1e3a8a;
                    padding-left: 10px;
                }}
                .content {{
                    margin-bottom: 15px;
                    text-align: justify;
                }}
                .persona-analysis {{
                    margin-bottom: 25px;
                    padding: 15px;
                    border: 1px solid #e5e7eb;
                    border-radius: 5px;
                    background-color: #f9fafb;
                }}
                .persona-title {{
                    font-weight: bold;
                    color: #1e3a8a;
                    margin-bottom: 10px;
                }}
                .final-report {{
                    margin-top: 30px;
                    padding: 20px;
                    border: 2px solid #1e3a8a;
                    border-radius: 8px;
                    background-color: #f0f4ff;
                }}
                .timestamp {{
                    font-size: 10px;
                    color: #666;
                    text-align: center;
                    margin-top: 20px;
                }}
            </style>
        </head>
        <body>
            <div class="title">ğŸ” ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ</div>
            
            <div class="section">
                <div class="section-title">ğŸ“‹ ë¶„ì„ ìš”ì²­</div>
                <div class="content">{user_query}</div>
            </div>
            
            <div class="section">
                <div class="section-title">ğŸ“… ë¶„ì„ ì¼ì‹œ</div>
                <div class="content">{datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}</div>
            </div>
        """
        
        # ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (ìˆëŠ” ê²½ìš°)
        if rag_context:
            html_content += f"""
            <div class="section">
                <div class="section-title">ğŸ“Š ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°</div>
                <div class="content">ë¶„ì„ì— í™œìš©ëœ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ê°€ í¬í•¨ë˜ì—ˆìŠµë‹ˆë‹¤.</div>
            </div>
            """
        
        # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
        if market_research:
            html_content += f"""
            <div class="section">
                <div class="section-title">ğŸŒ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼</div>
                <div class="content">{market_research[:1000]}{'...' if len(market_research) > 1000 else ''}</div>
            </div>
            """
        
        # ì „ë¬¸ê°€ë³„ ë¶„ì„
        html_content += """
            <div class="section">
                <div class="section-title">ğŸ‘¥ ì „ë¬¸ê°€ë³„ ë¶„ì„</div>
        """
        
        for persona_key, analysis in persona_analyses.items():
            if analysis.get('success') and analysis.get('result'):
                persona_info = FINANCE_PERSONAS[persona_key]
                html_content += f"""
                <div class="persona-analysis">
                    <div class="persona-title">{persona_info['emoji']} {persona_info['name']}</div>
                    <div class="content">{analysis['result'].replace(chr(10), '<br>')}</div>
                </div>
                """
        
        # ìµœì¢… ì¢…í•© ë³´ê³ ì„œ
        if final_report:
            html_content += f"""
            <div class="final-report">
                <div class="section-title">ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ</div>
                <div class="content">{final_report.replace(chr(10), '<br>')}</div>
            </div>
            """
        
        html_content += f"""
            <div class="timestamp">ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}</div>
        </body>
        </html>
        """
        
        # HTMLì„ PDFë¡œ ë³€í™˜
        pdf_buffer = io.BytesIO()
        weasyprint.HTML(string=html_content).write_pdf(pdf_buffer)
        pdf_buffer.seek(0)
        
        return pdf_buffer
        
    except Exception as e:
        st.error(f"WeasyPrint PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_finance_analysis_pdf_reportlab(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """ReportLabì„ ì‚¬ìš©í•œ ì¬ë¬´ ë¶„ì„ PDF ìƒì„± (í•œê¸€ ì§€ì› ê°œì„ )"""
    try:
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.units import inch
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
        from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_JUSTIFY
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.pdfbase.cidfonts import UnicodeCIDFont
        from reportlab.lib.colors import black, blue, red, green, darkblue
        import io
        import os
        import platform
        
        # PDF ë²„í¼ ìƒì„±
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (Archives.py ë°©ì‹ ì°¸ê³ )
        font_name = 'Helvetica'  # ê¸°ë³¸ê°’
        
        try:
            # ë°©ë²• 1: UnicodeCIDFont ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
            try:
                pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
                font_name = 'STSong-Light'
                st.info("âœ… UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"STSong-Light ë“±ë¡ ì‹¤íŒ¨: {e}")
                try:
                    # ë‹¤ë¥¸ UnicodeCIDFont ì‹œë„
                    pdfmetrics.registerFont(UnicodeCIDFont('HeiseiMin-W3'))
                    font_name = 'HeiseiMin-W3'
                    st.info("âœ… HeiseiMin-W3 UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
                except Exception as e2:
                    st.warning(f"HeiseiMin-W3 ë“±ë¡ ì‹¤íŒ¨: {e2}")
                    try:
                        # ë˜ ë‹¤ë¥¸ UnicodeCIDFont ì‹œë„
                        pdfmetrics.registerFont(UnicodeCIDFont('HeiseiKakuGo-W5'))
                        font_name = 'HeiseiKakuGo-W5'
                        st.info("âœ… HeiseiKakuGo-W5 UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
                    except Exception as e3:
                        st.warning(f"ëª¨ë“  UnicodeCIDFont ë“±ë¡ ì‹¤íŒ¨: {e3}")
                        
                        # ë°©ë²• 2: ì‹œìŠ¤í…œë³„ í•œê¸€ í°íŠ¸ ì‹œë„
                        system = platform.system()
                        font_paths = []
                        
                        if system == "Darwin":  # macOS
                            font_paths = [
                                '/System/Library/Fonts/AppleGothic.ttc',
                                '/System/Library/Fonts/PingFang.ttc',
                                '/System/Library/Fonts/STHeiti Light.ttc',
                                '/System/Library/Fonts/STHeiti Medium.ttc',
                                '/System/Library/Fonts/Supplemental/Arial Unicode MS.ttf',
                                '/Library/Fonts/Arial Unicode MS.ttf'
                            ]
                        elif system == "Windows":
                            font_paths = [
                                'C:/Windows/Fonts/malgun.ttf',  # ë§‘ì€ ê³ ë”•
                                'C:/Windows/Fonts/gulim.ttc',  # êµ´ë¦¼
                                'C:/Windows/Fonts/batang.ttc',  # ë°”íƒ•
                                'C:/Windows/Fonts/Arial.ttf'
                            ]
                        elif system == "Linux":
                            font_paths = [
                                '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
                                '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
                                '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'
                            ]
                        
                        # í°íŠ¸ ë“±ë¡ ì‹œë„
                        font_registered = False
                        for font_path in font_paths:
                            try:
                                if os.path.exists(font_path):
                                    pdfmetrics.registerFont(TTFont('KoreanFont', font_path))
                                    font_name = 'KoreanFont'
                                    font_registered = True
                                    st.success(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                                    break
                            except Exception as e:
                                st.warning(f"í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_path}): {e}")
                                continue
                        
                        # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
                        if not font_registered:
                            font_name = 'Helvetica'
                            st.warning(f"âš ï¸ í•œê¸€ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸({font_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                            
        except Exception as e:
            font_name = 'Helvetica'
            st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸({font_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ìŠ¤íƒ€ì¼ ì •ì˜
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=darkblue,
            fontName=font_name
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            spaceBefore=12,
            textColor=darkblue,
            fontName=font_name
        )
        normal_style = ParagraphStyle(
            'KoreanNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10,
            spaceAfter=6
        )
        small_style = ParagraphStyle(
            'KoreanSmall',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=8,
            spaceAfter=4
        )
        
        # ìŠ¤í† ë¦¬ êµ¬ì„±
        story = []
        
        # ì œëª©
        story.append(Paragraph("ğŸ” ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ", title_style))
        story.append(Spacer(1, 12))
        
        # ë¶„ì„ ìš”ì²­
        story.append(Paragraph("ğŸ“‹ ë¶„ì„ ìš”ì²­", heading_style))
        story.append(Paragraph(user_query, normal_style))
        story.append(Spacer(1, 12))
        
        # ë¶„ì„ ì¼ì‹œ
        story.append(Paragraph(f"ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}", normal_style))
        story.append(Spacer(1, 20))
        
        # ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (RAG íŒŒì¼ ê¸°ë°˜)
        if rag_context:
            story.append(Paragraph("ğŸ“ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (RAG íŒŒì¼)", heading_style))
            story.append(Paragraph("ë‹¤ìŒì€ íšŒì‚¬ ë‚´ë¶€ ì¬ë¬´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:", normal_style))
            story.append(Spacer(1, 6))
            # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
            rag_paragraphs = rag_context.split('\n\n')
            for para in rag_paragraphs[:2]:  # ì²˜ìŒ 2ê°œ ë‹¨ë½ë§Œ í¬í•¨
                if para.strip():
                    story.append(Paragraph(para.strip(), normal_style))
                    story.append(Spacer(1, 6))
            story.append(Spacer(1, 12))
        
        # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (Perplexity ê¸°ë°˜)
        if market_research:
            story.append(Paragraph("ğŸŒ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (Perplexity)", heading_style))
            story.append(Paragraph("ë‹¤ìŒì€ Perplexity AIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ì…ë‹ˆë‹¤:", normal_style))
            story.append(Spacer(1, 6))
            # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
            market_paragraphs = market_research.split('\n\n')
            for para in market_paragraphs[:3]:  # ì²˜ìŒ 3ê°œ ë‹¨ë½ë§Œ í¬í•¨
                if para.strip():
                    story.append(Paragraph(para.strip(), normal_style))
                    story.append(Spacer(1, 6))
            story.append(Spacer(1, 12))
        
        # ì „ë¬¸ê°€ë³„ ë¶„ì„
        story.append(Paragraph("ğŸ‘¥ ì „ë¬¸ê°€ë³„ ë¶„ì„", heading_style))
        story.append(Spacer(1, 12))
        
        for persona_key, analysis in persona_analyses.items():
            if analysis.get('success') and analysis.get('result'):
                persona_info = FINANCE_PERSONAS[persona_key]
                story.append(Paragraph(f"{persona_info['emoji']} {persona_info['name']}", heading_style))
                
                # ë¶„ì„ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
                result_paragraphs = analysis['result'].split('\n\n')
                for para in result_paragraphs:
                    if para.strip():
                        story.append(Paragraph(para.strip(), normal_style))
                        story.append(Spacer(1, 6))
                
                story.append(Spacer(1, 12))
        
        # ìµœì¢… ì¢…í•© ë³´ê³ ì„œ
        if final_report:
            story.append(Paragraph("ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ", heading_style))
            story.append(Spacer(1, 12))
            
            # ìµœì¢… ë³´ê³ ì„œë¥¼ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
            final_paragraphs = final_report.split('\n\n')
            for para in final_paragraphs:
                if para.strip():
                    story.append(Paragraph(para.strip(), normal_style))
                    story.append(Spacer(1, 6))
        
        # PDF ìƒì„±
        doc.build(story)
        buffer.seek(0)
        
        return buffer
        
    except Exception as e:
        st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_pdf(user_query, response, relevant_docs=None):
    """ì¼ë°˜ ì±—ë´‡ ì‘ë‹µì„ PDFë¡œ ìƒì„±"""
    try:
        # ReportLabì„ ìš°ì„  ì‚¬ìš© (í•œê¸€ í°íŠ¸ ë¬¸ì œ í•´ê²°)
        return create_simple_analysis_pdf_reportlab(user_query, response, relevant_docs)
    except Exception as e:
        st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_pdf_fpdf2(user_query, response, relevant_docs=None):
    """FPDF2ë¥¼ ì‚¬ìš©í•œ ì¼ë°˜ ì±—ë´‡ PDF ìƒì„±"""
    try:
        # PDF ë²„í¼ ìƒì„±
        pdf_buffer = io.BytesIO()
        
        # FPDF2 ê°ì²´ ìƒì„±
        pdf = FPDF()
        pdf.add_page()
        
        # í•œê¸€ í°íŠ¸ ì„¤ì •
        try:
            import platform
            system = platform.system()
            
            # ìš´ì˜ì²´ì œë³„ í•œê¸€ í°íŠ¸ ê²½ë¡œ
            font_paths = []
            if system == "Darwin":  # macOS
                font_paths = [
                    "/System/Library/Fonts/AppleGothic.ttf",
                    "/System/Library/Fonts/Supplemental/Arial Unicode MS.ttf",
                    "/Library/Fonts/AppleGothic.ttf"
                ]
            elif system == "Windows":
                font_paths = [
                    "C:/Windows/Fonts/malgun.ttf",
                    "C:/Windows/Fonts/NanumGothic.ttf",
                    "C:/Windows/Fonts/gulim.ttc"
                ]
            else:  # Linux
                font_paths = [
                    "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
                    "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"
                ]
            
            # í°íŠ¸ ë“±ë¡ ì‹œë„
            font_registered = False
            for font_path in font_paths:
                try:
                    if os.path.exists(font_path):
                        pdf.add_font('KoreanFont', '', font_path, uni=True)
                        font_name = 'KoreanFont'
                        font_registered = True
                        st.info(f"í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                        break
                except Exception as e:
                    st.warning(f"í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_path}): {e}")
                    continue
            
            # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            if not font_registered:
                # FPDF2ì˜ ë‚´ì¥ ìœ ë‹ˆì½”ë“œ í°íŠ¸ ì‹œë„
                try:
                    pdf.add_font('DejaVu', '', uni=True)
                    font_name = 'DejaVu'
                    st.info("DejaVu ìœ ë‹ˆì½”ë“œ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                except:
                    font_name = 'Arial'
                    st.warning("í•œê¸€ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
        except Exception as e:
            font_name = 'Arial'
            st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ì œëª©
        pdf.set_font(font_name, 'B', 16)
        pdf.set_text_color(30, 58, 138)  # darkblue
        pdf.cell(0, 10, 'Q-Li ì±—ë´‡ ë¶„ì„ ë³´ê³ ì„œ', ln=True, align='C')
        pdf.ln(10)
        
        # ë¶„ì„ ìš”ì²­
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ë¶„ì„ ìš”ì²­', ln=True)
        pdf.set_font(font_name, '', 10)
        pdf.set_text_color(0, 0, 0)
        pdf.multi_cell(0, 6, user_query)
        pdf.ln(5)
        
        # ë¶„ì„ ì¼ì‹œ
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ë¶„ì„ ì¼ì‹œ', ln=True)
        pdf.set_font(font_name, '', 10)
        pdf.set_text_color(0, 0, 0)
        pdf.cell(0, 6, datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M'), ln=True)
        pdf.ln(5)
        
        # ì±—ë´‡ ì‘ë‹µ
        pdf.set_font(font_name, 'B', 12)
        pdf.set_text_color(30, 58, 138)
        pdf.cell(0, 8, 'ì±—ë´‡ ì‘ë‹µ', ln=True)
        pdf.set_font(font_name, '', 10)
        pdf.set_text_color(0, 0, 0)
        pdf.multi_cell(0, 6, response)
        pdf.ln(5)
        
        # ì°¸ê³  ë¬¸ì„œ (ìˆëŠ” ê²½ìš°)
        if relevant_docs:
            pdf.set_font(font_name, 'B', 12)
            pdf.set_text_color(30, 58, 138)
            pdf.cell(0, 8, 'ì°¸ê³  ë¬¸ì„œ', ln=True)
            pdf.ln(5)
            
            for i, doc in enumerate(relevant_docs, 1):
                title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                content = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                
                pdf.set_font(font_name, 'B', 10)
                pdf.set_text_color(30, 58, 138)
                pdf.cell(0, 6, f"{i}. {title}", ln=True)
                pdf.set_font(font_name, '', 9)
                pdf.set_text_color(0, 0, 0)
                pdf.multi_cell(0, 5, content)
                pdf.ln(3)
        
        # PDF ì¶œë ¥
        pdf.output(pdf_buffer)
        pdf_buffer.seek(0)
        
        return pdf_buffer
        
    except Exception as e:
        st.error(f"FPDF2 PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_pdf_weasyprint(user_query, response, relevant_docs=None):
    """WeasyPrintë¥¼ ì‚¬ìš©í•œ ì¼ë°˜ ì±—ë´‡ PDF ìƒì„±"""
    try:
        # HTML ì½˜í…ì¸  ìƒì„±
        html_content = f"""
        <!DOCTYPE html>
        <html lang="ko">
        <head>
            <meta charset="utf-8">
            <title>Q-Li ì±—ë´‡ ë¶„ì„ ë³´ê³ ì„œ</title>
            <style>
                body {{
                    font-family: 'Malgun Gothic', 'AppleGothic', 'NanumGothic', 'Arial Unicode MS', sans-serif;
                    font-size: 12px;
                    line-height: 1.6;
                    margin: 20px;
                    color: #333;
                }}
                .title {{
                    font-size: 18px;
                    font-weight: bold;
                    text-align: center;
                    color: #1e3a8a;
                    margin-bottom: 30px;
                    border-bottom: 2px solid #1e3a8a;
                    padding-bottom: 10px;
                }}
                .section {{
                    margin-bottom: 20px;
                }}
                .section-title {{
                    font-size: 14px;
                    font-weight: bold;
                    color: #1e3a8a;
                    margin-bottom: 10px;
                    border-left: 4px solid #1e3a8a;
                    padding-left: 10px;
                }}
                .content {{
                    margin-bottom: 15px;
                    text-align: justify;
                }}
                .chatbot-response {{
                    margin-bottom: 25px;
                    padding: 15px;
                    border: 1px solid #e5e7eb;
                    border-radius: 5px;
                    background-color: #f9fafb;
                }}
                .timestamp {{
                    font-size: 10px;
                    color: #666;
                    text-align: center;
                    margin-top: 20px;
                }}
            </style>
        </head>
        <body>
            <div class="title">ğŸ¤” Q-Li ì±—ë´‡ ë¶„ì„ ë³´ê³ ì„œ</div>
            
            <div class="section">
                <div class="section-title">ğŸ“‹ ë¶„ì„ ìš”ì²­</div>
                <div class="content">{user_query}</div>
            </div>
            
            <div class="section">
                <div class="section-title">ğŸ“… ë¶„ì„ ì¼ì‹œ</div>
                <div class="content">{datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}</div>
            </div>
            
            <div class="section">
                <div class="section-title">ğŸ¤– ì±—ë´‡ ì‘ë‹µ</div>
                <div class="chatbot-response">{response.replace(chr(10), '<br>')}</div>
            </div>
        """
        
        # ì°¸ê³  ë¬¸ì„œ (ìˆëŠ” ê²½ìš°)
        if relevant_docs:
            html_content += """
                <div class="section">
                    <div class="section-title">ğŸ“š ì°¸ê³  ë¬¸ì„œ</div>
            """
            for i, doc in enumerate(relevant_docs, 1):
                title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                content = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                html_content += f"""
                    <div class="content">
                        <strong>{title}</strong><br>
                        {content.replace(chr(10), '<br>')}
                    </div>
                """
            html_content += "</div>"
        
        html_content += f"""
            <div class="timestamp">ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M:%S')}</div>
        </body>
        </html>
        """
        
        # HTMLì„ PDFë¡œ ë³€í™˜
        pdf_buffer = io.BytesIO()
        weasyprint.HTML(string=html_content).write_pdf(pdf_buffer)
        pdf_buffer.seek(0)
        
        return pdf_buffer
        
    except Exception as e:
        st.error(f"WeasyPrint PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_pdf_reportlab(user_query, response, relevant_docs=None):
    """ReportLabì„ ì‚¬ìš©í•œ ì¼ë°˜ ì±—ë´‡ PDF ìƒì„± (í•œê¸€ ì§€ì› ê°œì„ )"""
    try:
        from reportlab.pdfgen import canvas
        from reportlab.lib.pagesizes import A4
        from reportlab.lib.units import inch
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
        from reportlab.lib.enums import TA_LEFT, TA_CENTER, TA_JUSTIFY
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.pdfbase.cidfonts import UnicodeCIDFont
        from reportlab.lib.colors import black, blue, red, green, darkblue
        import io
        import os
        import platform
        
        # PDF ë²„í¼ ìƒì„±
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        
        # í•œê¸€ í°íŠ¸ ì„¤ì • (Archives.py ë°©ì‹ ì°¸ê³ )
        font_name = 'Helvetica'  # ê¸°ë³¸ê°’
        
        try:
            # ë°©ë²• 1: UnicodeCIDFont ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
            try:
                pdfmetrics.registerFont(UnicodeCIDFont('STSong-Light'))
                font_name = 'STSong-Light'
                st.info("âœ… UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
            except Exception as e:
                st.warning(f"STSong-Light ë“±ë¡ ì‹¤íŒ¨: {e}")
                try:
                    # ë‹¤ë¥¸ UnicodeCIDFont ì‹œë„
                    pdfmetrics.registerFont(UnicodeCIDFont('HeiseiMin-W3'))
                    font_name = 'HeiseiMin-W3'
                    st.info("âœ… HeiseiMin-W3 UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
                except Exception as e2:
                    st.warning(f"HeiseiMin-W3 ë“±ë¡ ì‹¤íŒ¨: {e2}")
                    try:
                        # ë˜ ë‹¤ë¥¸ UnicodeCIDFont ì‹œë„
                        pdfmetrics.registerFont(UnicodeCIDFont('HeiseiKakuGo-W5'))
                        font_name = 'HeiseiKakuGo-W5'
                        st.info("âœ… HeiseiKakuGo-W5 UnicodeCIDFontë¥¼ ì‚¬ìš©í•˜ì—¬ í•œê¸€ì„ ì§€ì›í•©ë‹ˆë‹¤.")
                    except Exception as e3:
                        st.warning(f"ëª¨ë“  UnicodeCIDFont ë“±ë¡ ì‹¤íŒ¨: {e3}")
                        
                        # ë°©ë²• 2: ì‹œìŠ¤í…œë³„ í•œê¸€ í°íŠ¸ ì‹œë„
                system = platform.system()
                font_paths = []
                
                if system == "Darwin":  # macOS
                    font_paths = [
                        '/System/Library/Fonts/AppleGothic.ttc',
                        '/System/Library/Fonts/PingFang.ttc',
                        '/System/Library/Fonts/STHeiti Light.ttc',
                        '/System/Library/Fonts/STHeiti Medium.ttc',
                        '/System/Library/Fonts/Supplemental/Arial Unicode MS.ttf',
                        '/Library/Fonts/Arial Unicode MS.ttf'
                    ]
                elif system == "Windows":
                    font_paths = [
                        'C:/Windows/Fonts/malgun.ttf',  # ë§‘ì€ ê³ ë”•
                        'C:/Windows/Fonts/gulim.ttc',  # êµ´ë¦¼
                        'C:/Windows/Fonts/batang.ttc',  # ë°”íƒ•
                        'C:/Windows/Fonts/Arial.ttf'
                    ]
                elif system == "Linux":
                    font_paths = [
                        '/usr/share/fonts/truetype/nanum/NanumGothic.ttf',
                        '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
                        '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf'
                    ]
                
                # í°íŠ¸ ë“±ë¡ ì‹œë„
                font_registered = False
                for font_path in font_paths:
                    try:
                        if os.path.exists(font_path):
                            pdfmetrics.registerFont(TTFont('KoreanFont', font_path))
                            font_name = 'KoreanFont'
                            font_registered = True
                            st.success(f"âœ… í•œê¸€ í°íŠ¸ ë“±ë¡ ì„±ê³µ: {font_path}")
                            break
                    except Exception as e:
                        st.warning(f"í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ({font_path}): {e}")
                        continue
                
                # í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
                if not font_registered:
                    font_name = 'Helvetica'
                    st.warning(f"âš ï¸ í•œê¸€ í°íŠ¸ ë“±ë¡ ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸({font_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    
        except Exception as e:
            font_name = 'Helvetica'
            st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}. ê¸°ë³¸ í°íŠ¸({font_name})ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # ìŠ¤íƒ€ì¼ ì •ì˜
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontSize=16,
            spaceAfter=30,
            alignment=TA_CENTER,
            textColor=darkblue,
            fontName=font_name
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontSize=14,
            spaceAfter=12,
            spaceBefore=12,
            textColor=darkblue,
            fontName=font_name
        )
        normal_style = ParagraphStyle(
            'KoreanNormal',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=10,
            spaceAfter=6
        )
        small_style = ParagraphStyle(
            'KoreanSmall',
            parent=styles['Normal'],
            fontName=font_name,
            fontSize=8,
            spaceAfter=4
        )
        
        # ìŠ¤í† ë¦¬ êµ¬ì„±
        story = []
        
        # ì œëª©
        story.append(Paragraph("ğŸ¤” Q-Li ì±—ë´‡ ë¶„ì„ ë³´ê³ ì„œ", title_style))
        story.append(Spacer(1, 12))
        
        # ë¶„ì„ ìš”ì²­
        story.append(Paragraph("ğŸ“‹ ë¶„ì„ ìš”ì²­", heading_style))
        story.append(Paragraph(user_query, normal_style))
        story.append(Spacer(1, 12))
        
        # ë¶„ì„ ì¼ì‹œ
        story.append(Paragraph(f"ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Yë…„ %mì›” %dì¼ %H:%M')}", normal_style))
        story.append(Spacer(1, 20))
        
        # ì±—ë´‡ ì‘ë‹µ
        story.append(Paragraph("ğŸ¤– ì±—ë´‡ ì‘ë‹µ", heading_style))
        
        # ì‘ë‹µì„ ì—¬ëŸ¬ ë‹¨ë½ìœ¼ë¡œ ë¶„í• 
        response_paragraphs = response.split('\n\n')
        for para in response_paragraphs:
            if para.strip():
                story.append(Paragraph(para.strip(), normal_style))
                story.append(Spacer(1, 6))
        
        # ì°¸ê³  ë¬¸ì„œ (ìˆëŠ” ê²½ìš°)
        if relevant_docs:
            story.append(Spacer(1, 12))
            story.append(Paragraph("ğŸ“š ì°¸ê³  ë¬¸ì„œ", heading_style))
            for i, doc in enumerate(relevant_docs, 1):
                title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                content = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                story.append(Paragraph(f"**{title}**", normal_style))
                story.append(Paragraph(content, small_style))
                story.append(Spacer(1, 6))
        
        # PDF ìƒì„±
        doc.build(story)
        buffer.seek(0)
        
        return buffer
        
    except Exception as e:
        st.error(f"PDF ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ===== DOCX ìƒì„± í•¨ìˆ˜ë“¤ =====

def create_finance_analysis_docx(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """ì¬ë¬´ ë¶„ì„ ê²°ê³¼ë¥¼ DOCXë¡œ ìƒì„±"""
    try:
        if DOCX_AVAILABLE:
            return create_finance_analysis_docx_python_docx(user_query, persona_analyses, final_report, rag_context, market_research)
        else:
            st.error("python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        st.error(f"DOCX ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_finance_analysis_docx_python_docx(user_query, persona_analyses, final_report, rag_context="", market_research=""):
    """python-docxë¥¼ ì‚¬ìš©í•œ ì¬ë¬´ ë¶„ì„ DOCX ìƒì„±"""
    try:
        # DOCX ë¬¸ì„œ ìƒì„±
        doc = Document()
        
        # ì œëª© ì„¤ì •
        title = doc.add_heading('Q-Li ì¬ë¬´ ë¶„ì„ ë³´ê³ ì„œ', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # ì‚¬ìš©ì ì§ˆë¬¸
        doc.add_heading('ì‚¬ìš©ì ì§ˆë¬¸', level=1)
        doc.add_paragraph(user_query)
        
        # ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (ìˆëŠ” ê²½ìš°)
        if market_research and market_research.strip():
            doc.add_heading('ğŸŒ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ (Perplexity)', level=1)
            doc.add_paragraph("ë‹¤ìŒì€ Perplexity AIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ì…ë‹ˆë‹¤:")
            doc.add_paragraph(market_research)
        
        # RAG ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
        if rag_context and rag_context.strip():
            doc.add_heading('ğŸ“ ë‚´ë¶€ ì¬ë¬´ ë°ì´í„° (RAG íŒŒì¼)', level=1)
            doc.add_paragraph("ë‹¤ìŒì€ íšŒì‚¬ ë‚´ë¶€ ì¬ë¬´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:")
            doc.add_paragraph(rag_context)
        
        # ì „ë¬¸ê°€ë³„ ë¶„ì„
        doc.add_heading('ì „ë¬¸ê°€ë³„ ë¶„ì„', level=1)
        for persona_key, analysis in persona_analyses.items():
            if analysis and analysis.get('success') and analysis.get('result'):
                persona_name = persona_key.replace('_', ' ').title()
                doc.add_heading(f'{persona_name} ë¶„ì„', level=2)
                doc.add_paragraph(analysis['result'])
        
        # ìµœì¢… ì¢…í•© ë³´ê³ ì„œ
        if final_report and final_report.strip():
            doc.add_heading('ìµœì¢… ì¢…í•© ë³´ê³ ì„œ', level=1)
            doc.add_paragraph(final_report)
        
        # ìƒì„± ì‹œê°„
        doc.add_paragraph(f'ìƒì„± ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        
        # DOCX ë²„í¼ì— ì €ì¥
        docx_buffer = io.BytesIO()
        doc.save(docx_buffer)
        docx_buffer.seek(0)
        
        return docx_buffer
        
    except Exception as e:
        st.error(f"DOCX ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_docx(user_query, response, relevant_docs=None):
    """ì¼ë°˜ ì±—ë´‡ ì‘ë‹µì„ DOCXë¡œ ìƒì„±"""
    try:
        if DOCX_AVAILABLE:
            return create_simple_analysis_docx_python_docx(user_query, response, relevant_docs)
        else:
            st.error("python-docx ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
    except Exception as e:
        st.error(f"DOCX ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def create_simple_analysis_docx_python_docx(user_query, response, relevant_docs=None):
    """python-docxë¥¼ ì‚¬ìš©í•œ ì¼ë°˜ ì±—ë´‡ DOCX ìƒì„±"""
    try:
        # DOCX ë¬¸ì„œ ìƒì„±
        doc = Document()
        
        # ì œëª© ì„¤ì •
        title = doc.add_heading('Q-Li ì±—ë´‡ ë¶„ì„ ë³´ê³ ì„œ', 0)
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        
        # ì‚¬ìš©ì ì§ˆë¬¸
        doc.add_heading('ì‚¬ìš©ì ì§ˆë¬¸', level=1)
        doc.add_paragraph(user_query)
        
        # ì±—ë´‡ ì‘ë‹µ
        doc.add_heading('ì±—ë´‡ ì‘ë‹µ', level=1)
        doc.add_paragraph(response)
        
        # ì°¸ê³  ë¬¸ì„œ (ìˆëŠ” ê²½ìš°)
        if relevant_docs:
            doc.add_heading('ì°¸ê³  ë¬¸ì„œ', level=1)
            for i, doc_item in enumerate(relevant_docs, 1):
                doc.add_paragraph(f'{i}. {doc_item.page_content[:200]}...')
        
        # ìƒì„± ì‹œê°„
        doc.add_paragraph(f'ìƒì„± ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        
        # DOCX ë²„í¼ì— ì €ì¥
        docx_buffer = io.BytesIO()
        doc.save(docx_buffer)
        docx_buffer.seek(0)
        
        return docx_buffer
        
    except Exception as e:
        st.error(f"DOCX ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# ===== ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬ =====
# ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œ ì´ˆê¸°í™”
if 'finance_authenticated' not in st.session_state:
    st.session_state.finance_authenticated = False

# ì¬ë¬´ ì •ë³´ ì•”í˜¸ ì„¤ì •
finance_pw = os.getenv('FINANCE_PASSWORD')
if not finance_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(FINANCE_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.')
    st.stop()

# ===== Streamlit UI =====
if 'chatbot' not in st.session_state:
    st.session_state.chatbot = FileRAGChatbot()
# ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” (í´ë”ë³„ë¡œ ë¶„ë¦¬)
if 'conversations' not in st.session_state:
    st.session_state.conversations = {}
    st.session_state.conversations['ì¼ë°˜'] = []
    st.session_state.conversations['ì¬ë¬´ ì •ë³´-ê³ ê¸‰'] = []

# st.title("ğŸ¤” Q-Li (aQara-LIfe | íë¦¬)")
# st.markdown("ë¨¼ì € ì‚¬ì´ë“œë°”ì˜ ë§¨ í•˜ë‹¨ ë©”ë‰´ì—ì„œ ì›í•˜ëŠ” LLMë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
# st.markdown("LLMì—ì„œ ollamaëŠ” ë¡œì»¬ ì„œë²„ì—ì„œ ë™ì‘í•˜ëŠ” sLLMìœ¼ë¡œ ì„±ëŠ¥ì€ ì•„ì§ ë§ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ë‹¤ë§Œ ë¡œì»¬ì—ì„œ ë™ì‘í•˜ë¯€ë¡œ ë¬´ë£Œì…ë‹ˆë‹¤.")

# í´ë” ì˜µì…˜ ì •ì˜
folder_options = {
    "ì „ì²´ ê²€ìƒ‰ (ì¬ë¬´ ì •ë³´ ì œì™¸)": "./pages/rag_files",
    "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤": "./pages/rag_files/process",
    "ë™ë£Œ ì •ë³´": "./pages/rag_files/colleagues", 
    "ìœ„ì„ì „ê²°ê·œì •": "./pages/rag_files/approval",
    "ì¬ë¬´ ì •ë³´": "./pages/rag_files/finance",
    "ì¬ë¬´ ì •ë³´-ê³ ê¸‰": "./pages/rag_files/finance",
    "ì¼ë°˜ ì±„íŒ…": "./pages/rag_files/general"
}

# ì¬ë¬´ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ ì •ì˜
FINANCE_PERSONAS = {
    "ì¬ë¬´ë¶„ì„ê°€": {
        "name": "ì¬ë¬´ ë¶„ì„ê°€",
        "emoji": "ğŸ“Š",
        "role": "ì¬ë¬´ ë°ì´í„° ë¶„ì„ ë° í•´ì„ ì „ë¬¸ê°€",
        "expertise": "ì¬ë¬´ì œí‘œ ë¶„ì„, ì¬ë¬´ ë¹„ìœ¨ ë¶„ì„, í˜„ê¸ˆíë¦„ ë¶„ì„, íˆ¬ì í‰ê°€",
        "perspective": "ì¬ë¬´ì  íƒ€ë‹¹ì„±, ìˆ˜ìµì„±, ì•ˆì •ì„±, ì„±ì¥ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 15ë…„ ì´ìƒì˜ ê²½í—˜ì„ ê°€ì§„ ìµœê³  ìˆ˜ì¤€ì˜ ì¬ë¬´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ì¬ë¬´ì œí‘œ ë¶„ì„ ë° ì¬ë¬´ ëª¨ë¸ë§
- ì¬ë¬´ ë¹„ìœ¨ ë¶„ì„ ë° ë²¤ì¹˜ë§ˆí‚¹
- í˜„ê¸ˆíë¦„ ë¶„ì„ ë° ìœ ë™ì„± ê´€ë¦¬
- íˆ¬ì í‰ê°€ ë° ìë³¸ ë°°ë¶„ ìµœì í™”
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë° ë‚´ë¶€í†µì œ
- ì¬ë¬´ ì˜ˆì¸¡ ë° ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ì¬ë¬´ ì§€í‘œë¥¼ ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ì—¬ í˜„í™©ì„ ì •í™•íˆ ì§„ë‹¨
2. ìˆ˜ì¹˜ì  ê·¼ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ê°ê´€ì  ë¶„ì„ ì œê³µ
3. ì—…ê³„ ë²¤ì¹˜ë§ˆí¬ì™€ì˜ ë¹„êµ ë¶„ì„ ìˆ˜í–‰
4. ì¬ë¬´ì  ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì„ ì •ëŸ‰í™”
5. êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆê³¼ ì‹¤í–‰ ê³„íš ì œì‹œ
6. ì¬ë¬´ì  ì„íŒ©íŠ¸ë¥¼ ëª…í™•íˆ ì •ëŸ‰í™”

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- ì£¼ìš” ì¬ë¬´ ë¹„ìœ¨ê³¼ ì—…ê³„ í‰ê·  ë¹„êµ ë¶„ì„
- ìˆ˜ìµì„±, ì•ˆì •ì„±, ì„±ì¥ì„± ê´€ì ì—ì„œ ì¢…í•© í‰ê°€
- í˜„ê¸ˆíë¦„ ì˜ˆì¸¡ê³¼ ìê¸ˆ ì¡°ë‹¬ ê³„íš ìˆ˜ë¦½
- ì¬ë¬´ì  ë¦¬ìŠ¤í¬ ìš”ì¸ê³¼ ì™„í™” ë°©ì•ˆ ì œì‹œ
- êµ¬ì²´ì ì¸ ê°œì„  ëª©í‘œì™€ ì‹¤í–‰ ë°©ì•ˆ ì œì‹œ
- ì¬ë¬´ì  ì„±ê³¼ ì¸¡ì • ì§€í‘œ(KPI) ì„¤ì •"""
    },
    "ì‹œì¥ë¶„ì„ê°€": {
        "name": "ì‹œì¥ ë¶„ì„ê°€",
        "emoji": "ğŸŒ",
        "role": "ì‹œì¥ ë™í–¥ ë° ê²½ìŸ í™˜ê²½ ë¶„ì„ ì „ë¬¸ê°€",
        "expertise": "ì‹œì¥ ë¶„ì„, ê²½ìŸì‚¬ ë¶„ì„, ì‚°ì—… íŠ¸ë Œë“œ, ì‹œì¥ ê¸°íšŒ ë¶„ì„",
        "perspective": "ì‹œì¥ ê¸°íšŒ, ê²½ìŸ í™˜ê²½, ì‚°ì—… íŠ¸ë Œë“œ, ì‹œì¥ ë¦¬ìŠ¤í¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ ê¸€ë¡œë²Œ ì‹œì¥ ë¶„ì„ ê²½í—˜ì„ ê°€ì§„ ìµœê³  ìˆ˜ì¤€ì˜ ì‹œì¥ ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ì‹œì¥ ê·œëª¨ ë° ì„±ì¥ë¥  ë¶„ì„
- ê²½ìŸì‚¬ ë¶„ì„ ë° í¬ì§€ì…”ë‹ ì „ëµ
- ì‚°ì—… íŠ¸ë Œë“œ ë° ê¸°ìˆ  ë™í–¥ ë¶„ì„
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜ ìš”ì¸ í‰ê°€
- ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„
- ì‹œì¥ ì§„ì… ë° í™•ì¥ ì „ëµ

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ì •ëŸ‰ì  ì‹œì¥ ë°ì´í„°ì™€ ì •ì„±ì  íŠ¸ë Œë“œ ë¶„ì„ì„ ì¢…í•©
2. ê²½ìŸ í™˜ê²½ê³¼ ì‹œì¥ í¬ì§€ì…”ë‹ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€
3. ì‹œì¥ ê¸°íšŒì™€ ìœ„í—˜ ìš”ì¸ì„ ê· í˜• ìˆê²Œ ë¶„ì„
4. êµ¬ì²´ì ì¸ ì‹œì¥ ì§„ì… ë° í™•ì¥ ì „ëµ ì œì‹œ
5. ì‹œì¥ ë³€í™”ì— ëŒ€í•œ ëŒ€ì‘ ë°©ì•ˆ ìˆ˜ë¦½
6. ì‹œì¥ ê¸°ë°˜ì˜ ìˆ˜ìµì„± ë¶„ì„ ìˆ˜í–‰

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- ì‹œì¥ ê·œëª¨, ì„±ì¥ë¥ , ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì •ëŸ‰í™”
- ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„/ì—´ìœ„ ìš”ì†Œë¥¼ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë¶„ì„
- ì‹œì¥ ê¸°íšŒì™€ ìœ„í—˜ ìš”ì¸ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€
- êµ¬ì²´ì ì¸ ì‹œì¥ ì§„ì… ë° í™•ì¥ ì „ëµ ì œì‹œ
- ì‹œì¥ ë³€í™”ì— ëŒ€í•œ ëŒ€ì‘ ë°©ì•ˆ ìˆ˜ë¦½
- ì‹œì¥ ê¸°ë°˜ì˜ ìˆ˜ìµì„± ê°œì„  ë°©ì•ˆ ë„ì¶œ"""
    },
    "ë§ˆì¼€íŒ…ì „ë¬´": {
        "name": "ë§ˆì¼€íŒ… ì „ë¬´",
        "emoji": "ğŸ“¢",
        "role": "ë§ˆì¼€íŒ… ì „ëµ ë° ë¸Œëœë“œ ê´€ë¦¬ ì „ë¬¸ê°€",
        "expertise": "ë¸Œëœë“œ ì „ëµ, ê³ ê° ê²½í—˜, ë§ˆì¼€íŒ… ì±„ë„, ìˆ˜ìµì„± ë¶„ì„",
        "perspective": "ê³ ê° ë‹ˆì¦ˆ, ë¸Œëœë“œ ê°€ì¹˜, ë§ˆì¼€íŒ… ROI, ê³ ê° ìƒì• ê°€ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ ë””ì§€í„¸ ë§ˆì¼€íŒ…ê³¼ ë¸Œëœë“œ ì „ëµ ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ì¸ ë§ˆì¼€íŒ… ì „ë¬´ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ë¸Œëœë“œ í¬ì§€ì…”ë‹ ë° ì•„ì´ë´í‹°í‹° êµ¬ì¶•
- ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë° íƒ€ê²ŸíŒ… ì „ëµ
- ë§ˆì¼€íŒ… ì±„ë„ ìµœì í™” ë° ROI ë¶„ì„
- ê³ ê° ê²½í—˜ ì„¤ê³„ ë° ê°œì¸í™” ì „ëµ
- ë§ˆì¼€íŒ… ìë™í™” ë° ë°ì´í„° ë¶„ì„
- ë¸Œëœë“œ ê°€ì¹˜ ì¸¡ì • ë° ê´€ë¦¬

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ê³ ê° ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„ ìˆ˜í–‰
2. ë§ˆì¼€íŒ… ROIì™€ ìˆ˜ìµì„± ì¤‘ì‹¬ì˜ ì „ëµ ìˆ˜ë¦½
3. ë¸Œëœë“œ ê°€ì¹˜ì™€ ê³ ê° ìƒì• ê°€ì¹˜ ê·¹ëŒ€í™” ë°©ì•ˆ ì œì‹œ
4. ë§ˆì¼€íŒ… ì±„ë„ë³„ ì„±ê³¼ ë¶„ì„ ë° ìµœì í™”
5. ê²½ìŸì‚¬ ë§ˆì¼€íŒ… ì „ëµ ë²¤ì¹˜ë§ˆí‚¹
6. ì°½ì˜ì  ì•„ì´ë””ì–´ì™€ ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ë²•ì˜ ê· í˜•

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- íƒ€ê²Ÿ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ í¬ê¸°ì™€ íŠ¹ì„±ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„
- ë§ˆì¼€íŒ… ì±„ë„ë³„ ROIì™€ ì˜ˆì‚° ë°°ë¶„ ìµœì í™” ë°©ì•ˆ ì œì‹œ
- ë¸Œëœë“œ ê°€ì¹˜ ì¦ëŒ€ë¥¼ ìœ„í•œ êµ¬ì²´ì  ì „ëµ ìˆ˜ë¦½
- ê³ ê° ìƒì• ê°€ì¹˜(CLV) í–¥ìƒ ë°©ì•ˆ ë„ì¶œ
- ë§ˆì¼€íŒ… ì„±ê³¼ ì¸¡ì • ì§€í‘œ(KPI) ì„¤ì •
- ê²½ìŸì‚¬ ëŒ€ë¹„ ì°¨ë³„í™” ì „ëµ ì œì‹œ"""
    },
    "ì‚¬ì—…ì „ëµê°€": {
        "name": "ì‚¬ì—… ì „ëµê°€",
        "emoji": "ğŸ¯",
        "role": "ì‚¬ì—… ëª¨ë¸ ë° ì „ëµ ìˆ˜ë¦½ ì „ë¬¸ê°€",
        "expertise": "ì‚¬ì—… ì „ëµ, ìˆ˜ìµ ëª¨ë¸, ì„±ì¥ ì „ëµ, ì „ëµì  íŒŒíŠ¸ë„ˆì‹­",
        "perspective": "ì‚¬ì—… ëª¨ë¸ í˜ì‹ , ìˆ˜ìµì„± ê°œì„ , ì„±ì¥ ê¸°íšŒ, ì „ëµì  ìš°ì„ ìˆœìœ„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì‚°ì—…ì˜ ì‚¬ì—… ì „ëµì„ ì´ëŒì–´ì˜¨ ìµœê³  ìˆ˜ì¤€ì˜ ì‚¬ì—… ì „ëµê°€ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ì‚¬ì—… ëª¨ë¸ í˜ì‹  ë° ìˆ˜ìµ ë‹¤ê°í™”
- ì„±ì¥ ì „ëµ ìˆ˜ë¦½ ë° ì‹¤í–‰ ê³„íš
- ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ ë° M&A ê¸°íš
- ì‹œì¥ ì§„ì… ë° í™•ì¥ ì „ëµ
- ìˆ˜ìµì„± ê°œì„  ë° ë¹„ìš© ìµœì í™”
- ì „ëµì  ìš°ì„ ìˆœìœ„ ì„¤ì • ë° ìì› ë°°ë¶„

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ì‚¬ì—… ëª¨ë¸ì˜ ìˆ˜ìµì„±ê³¼ ì§€ì†ê°€ëŠ¥ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
2. ì„±ì¥ ê¸°íšŒì™€ ìœ„í—˜ ìš”ì¸ì„ ê· í˜• ìˆê²Œ ë¶„ì„
3. êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ ì„±ê³¼ ì¸¡ì • ë°©ì•ˆ ì œì‹œ
4. ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ê³¼ í˜‘ë ¥ ê¸°íšŒ ë°œêµ´
5. ìˆ˜ìµì„± ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ìˆ˜ë¦½
6. ì¥ê¸°ì  ê´€ì ì—ì„œì˜ ì „ëµì  ë°©í–¥ì„± ì œì‹œ

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- ì‚¬ì—… ëª¨ë¸ì˜ ìˆ˜ìµì„±ê³¼ ì§€ì†ê°€ëŠ¥ì„± ë¶„ì„
- ì„±ì¥ ê¸°íšŒì™€ ìœ„í—˜ ìš”ì¸ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€
- êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ ì„±ê³¼ ì¸¡ì • ì§€í‘œ ì„¤ì •
- ì „ëµì  íŒŒíŠ¸ë„ˆì‹­ ê¸°íšŒ ë°œêµ´ ë° í‰ê°€
- ìˆ˜ìµì„± ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ë°©ì•ˆ ì œì‹œ
- ì¥ê¸°ì  ì „ëµì  ë°©í–¥ì„±ê³¼ ë¡œë“œë§µ ìˆ˜ë¦½"""
    },
    "ì˜ì—…ë¶„ì„ê°€": {
        "name": "ì˜ì—… ë¶„ì„ê°€",
        "emoji": "ğŸ¤",
        "role": "ì˜ì—… ì„±ê³¼ ë° ê³ ê° ê´€ê³„ ë¶„ì„ ì „ë¬¸ê°€",
        "expertise": "ì˜ì—… ì „ëµ, ê³ ê° ê´€ê³„ ê´€ë¦¬, ìˆ˜ìµì„± ë¶„ì„, ì˜ì—… í”„ë¡œì„¸ìŠ¤ ìµœì í™”",
        "perspective": "ì˜ì—… íš¨ìœ¨ì„±, ê³ ê° ë§Œì¡±ë„, ìˆ˜ìµ ì°½ì¶œ, ì˜ì—… ìƒì‚°ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ B2B/B2C ì˜ì—… ì „ëµê³¼ ê³ ê° ê´€ê³„ ê´€ë¦¬ ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ì¸ ì˜ì—… ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ì˜ì—… í”„ë¡œì„¸ìŠ¤ ìµœì í™” ë° ìƒì‚°ì„± í–¥ìƒ
- ê³ ê° ì„¸ê·¸ë©˜í…Œì´ì…˜ ë° ê´€ê³„ ê´€ë¦¬
- ì˜ì—… ì„±ê³¼ ë¶„ì„ ë° ì˜ˆì¸¡
- ìˆ˜ìµì„± ë¶„ì„ ë° ê³ ê° ìƒì• ê°€ì¹˜ ì¸¡ì •
- ì˜ì—… ì±„ë„ ìµœì í™” ë° íŒŒíŠ¸ë„ˆì‹­ ê´€ë¦¬
- ê³ ê° ë§Œì¡±ë„ ë° ì¶©ì„±ë„ ë¶„ì„

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ì˜ì—… ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ì„±ê³¼ ë¶„ì„
2. ê³ ê°ë³„ ìˆ˜ìµì„±ê³¼ ì„±ì¥ ì ì¬ë ¥ì„ ì •ëŸ‰ í‰ê°€
3. ì˜ì—… í”„ë¡œì„¸ìŠ¤ ê°œì„ ì„ í†µí•œ íš¨ìœ¨ì„± í–¥ìƒ ë°©ì•ˆ ì œì‹œ
4. ê³ ê° ë§Œì¡±ë„ì™€ ì¶©ì„±ë„ í–¥ìƒ ì „ëµ ìˆ˜ë¦½
5. ì˜ì—… ì±„ë„ë³„ ì„±ê³¼ ë¶„ì„ ë° ìµœì í™”
6. ìˆ˜ìµì„± ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ì•¡ì…˜ í”Œëœ ë„ì¶œ

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- ì˜ì—… ì„±ê³¼ ì§€í‘œ(KPI)ì™€ ê°œì„  ëª©í‘œì¹˜ ì„¤ì •
- ê³ ê°ë³„ ìˆ˜ìµì„±ê³¼ ì„±ì¥ ì ì¬ë ¥ ë¶„ì„
- ì˜ì—… í”„ë¡œì„¸ìŠ¤ ê°œì„ ì•ˆê³¼ ê¸°ëŒ€ íš¨ê³¼ ì œì‹œ
- ê³ ê° ë§Œì¡±ë„ í–¥ìƒê³¼ ì¶©ì„±ë„ ì œê³  ë°©ì•ˆ
- ì˜ì—… ì±„ë„ ìµœì í™” ë° íŒŒíŠ¸ë„ˆì‹­ ì „ëµ
- ìˆ˜ìµì„± ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì  ì‹¤í–‰ ê³„íš"""
    }
}

# ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ì í˜ë¥´ì†Œë‚˜
REPORT_SYNTHESIZER = {
    "name": "ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ì",
    "emoji": "ğŸ“‹",
    "role": "ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼ ì¢…í•© ë° ìµœì¢… ë³´ê³ ì„œ ì‘ì„±",
    "expertise": "ë¶„ì„ ê²°ê³¼ ì¢…í•©, ì „ëµì  ì œì•ˆ, ì‹¤í–‰ ê³„íš ìˆ˜ë¦½",
    "perspective": "ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ì‹¤í˜„ ê°€ëŠ¥í•œ ì „ëµê³¼ ì‹¤í–‰ ê³„íšì„ ì œì‹œ",
    "system_prompt": """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë“¤ì˜ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ã€ì „ë¬¸ ì˜ì—­ã€‘
- ë‹¤ê°ë„ ë¶„ì„ ê²°ê³¼ ì¢…í•© ë° í•´ì„
- ì „ëµì  ìš°ì„ ìˆœìœ„ ì„¤ì • ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
- ë¦¬ìŠ¤í¬ì™€ ê¸°íšŒ ìš”ì¸ì˜ ê· í˜•ì  í‰ê°€
- êµ¬ì²´ì ì´ê³  ì‹¤í˜„ ê°€ëŠ¥í•œ ì œì•ˆ ë„ì¶œ
- ì„±ê³¼ ì¸¡ì • ì§€í‘œ ë° ëª¨ë‹ˆí„°ë§ ë°©ì•ˆ ìˆ˜ë¦½

ã€ë¶„ì„ ìŠ¤íƒ€ì¼ã€‘
1. ê° ì „ë¬¸ê°€ì˜ ê´€ì ì„ ê· í˜• ìˆê²Œ ê³ ë ¤í•˜ì—¬ ì¢…í•©ì  ë¶„ì„ ìˆ˜í–‰
2. ì‹¤í˜„ ê°€ëŠ¥ì„±ê³¼ ì„íŒ©íŠ¸ë¥¼ ê³ ë ¤í•œ ìš°ì„ ìˆœìœ„ ì„¤ì •
3. êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ ì„±ê³¼ ì¸¡ì • ë°©ì•ˆ ì œì‹œ
4. ë¦¬ìŠ¤í¬ ê´€ë¦¬ì™€ ê¸°íšŒ í™œìš© ë°©ì•ˆì„ ê· í˜• ìˆê²Œ ì œì‹œ
5. ì´í•´ê´€ê³„ìë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµ í¬í•¨
6. ì¥ê¸°ì  ê´€ì ì—ì„œì˜ ì§€ì†ê°€ëŠ¥í•œ ì „ëµ ìˆ˜ë¦½

ã€ë¦¬í¬íŠ¸ ìš”êµ¬ì‚¬í•­ã€‘
- ê° ì „ë¬¸ê°€ ë¶„ì„ì˜ í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ëª…í™•íˆ ìš”ì•½
- ì „ëµì  ìš°ì„ ìˆœìœ„ì™€ ì‹¤í–‰ ìˆœì„œë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œ
- ì˜ˆìƒ ì„±ê³¼ì™€ ë¦¬ìŠ¤í¬ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€
- êµ¬ì²´ì ì¸ ì‹¤í–‰ ê³„íšê³¼ íƒ€ì„ë¼ì¸ ìˆ˜ë¦½
- ì„±ê³¼ ì¸¡ì • ì§€í‘œì™€ ëª¨ë‹ˆí„°ë§ ë°©ì•ˆ ì„¤ì •
- ì´í•´ê´€ê³„ìë³„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµ í¬í•¨"""
}

# í´ë” ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼
st.markdown("### ğŸ“ ê²€ìƒ‰ í´ë” ì„ íƒ")

# ë¼ë””ì˜¤ ë²„íŠ¼ì—ì„œ ì„ íƒëœ í´ë”ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
selected_folder = st.radio(
    "ê²€ìƒ‰í•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
    options=list(folder_options.keys()),
    index=0,
    key="selected_folder_radio",
    help="ì„ íƒí•œ í´ë”ì˜ ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤. ì „ì²´ ê²€ìƒ‰ì€ finance í´ë”ë¥¼ ì œì™¸í•©ë‹ˆë‹¤. ì¬ë¬´ ì •ë³´-ê³ ê¸‰ì€ 5ëª…ì˜ ì „ë¬¸ê°€ê°€ ë™ì‹œì— ë¶„ì„í•©ë‹ˆë‹¤."
)

# ì¼ë°˜ ì±„íŒ… ëª¨ë“œì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
if selected_folder == "ì¼ë°˜ ì±„íŒ…":
    st.info("""
    ğŸ’¬ **ì¼ë°˜ ì±„íŒ… ëª¨ë“œ**
    
    ì´ ëª¨ë“œì—ì„œëŠ” RAG ê¸°ëŠ¥ ì—†ì´ ìˆœìˆ˜í•œ LLM ì±„íŒ…ì„ ì œê³µí•©ë‹ˆë‹¤:
    
    âœ… **ChatGPTì™€ ë™ì¼í•œ ê²½í—˜**: ë¬¸ì„œ ê²€ìƒ‰ ì—†ì´ ì§ì ‘ì ì¸ ëŒ€í™”
    âœ… **ë¹ ë¥¸ ì‘ë‹µ**: RAG ê²€ìƒ‰ ê³¼ì • ì—†ì´ ì¦‰ì‹œ ë‹µë³€
    âœ… **ììœ ë¡œìš´ ëŒ€í™”**: ì–´ë–¤ ì£¼ì œë“  ììœ ë¡­ê²Œ ì§ˆë¬¸ ê°€ëŠ¥
    âœ… **ì¼ë°˜ì ì¸ AI ì±„íŒ…**: ì¼ë°˜ì ì¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì²˜ëŸ¼ ì‚¬ìš©
    
    ë¬¸ì„œ ê¸°ë°˜ ê²€ìƒ‰ì´ í•„ìš”í•˜ì§€ ì•Šì€ ì¼ë°˜ì ì¸ ëŒ€í™”ì— ì í•©í•©ë‹ˆë‹¤.
    """)

# ì¬ë¬´ ì •ë³´-ê³ ê¸‰ ì˜µì…˜ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
elif selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
    st.info("""
    ğŸ” **ì¬ë¬´ ì •ë³´-ê³ ê¸‰ ë¶„ì„ ëª¨ë“œ**
    
    ì´ ëª¨ë“œì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì „ë¬¸ê°€ë“¤ì´ ë™ì‹œì— ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
    
    ğŸ“Š **ì¬ë¬´ ë¶„ì„ê°€**: ì¬ë¬´ì œí‘œ ë¶„ì„, ì¬ë¬´ ë¹„ìœ¨ ë¶„ì„, í˜„ê¸ˆíë¦„ ë¶„ì„
    ğŸŒ **ì‹œì¥ ë¶„ì„ê°€**: ì‹œì¥ ë™í–¥, ê²½ìŸì‚¬ ë¶„ì„, ì‚°ì—… íŠ¸ë Œë“œ ë¶„ì„  
    ğŸ“¢ **ë§ˆì¼€íŒ… ì „ë¬´**: ë¸Œëœë“œ ì „ëµ, ê³ ê° ê²½í—˜, ë§ˆì¼€íŒ… ROI ë¶„ì„
    ğŸ¯ **ì‚¬ì—… ì „ëµê°€**: ì‚¬ì—… ëª¨ë¸ í˜ì‹ , ìˆ˜ìµì„± ê°œì„ , ì„±ì¥ ì „ëµ
    ğŸ¤ **ì˜ì—… ë¶„ì„ê°€**: ì˜ì—… ì„±ê³¼, ê³ ê° ê´€ê³„, ìˆ˜ìµì„± ë¶„ì„
    
    ê° ì „ë¬¸ê°€ëŠ” ë‚´ë¶€ ì¬ë¬´ ë°ì´í„°ì™€ ì‹¤ì‹œê°„ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•˜ë©°, 
    ìµœì¢… ë³´ê³ ì„œ ì‘ì„±ìê°€ ëª¨ë“  ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ì‹¤í˜„ ê°€ëŠ¥í•œ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.
    """)

# ì¬ë¬´ ì •ë³´ ê´€ë ¨ í´ë” ì„ íƒ ì‹œ ì•”í˜¸ í™•ì¸
if selected_folder in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"]:
    if not st.session_state.finance_authenticated:
        st.markdown("### ğŸ” ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œ í™•ì¸")
        st.warning("ì¬ë¬´ ì •ë³´ì— ì ‘ê·¼í•˜ë ¤ë©´ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        password = st.text_input("ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ì•”í˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
        if password == finance_pw:
            st.session_state.finance_authenticated = True
            st.success("âœ… ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œì´ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
            st.rerun()
        else:
            if password:  # ì•”í˜¸ê°€ ì…ë ¥ëœ ê²½ìš°ì—ë§Œ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
                st.error("âŒ ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()
    else:
        st.success("âœ… ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

# í´ë” ë³€ê²½ ì‹œ RAG ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
if 'selected_folder' not in st.session_state:
    st.session_state.selected_folder = selected_folder

if selected_folder != st.session_state.selected_folder:
    # ì¬ë¬´ ì •ë³´ì—ì„œ ë‹¤ë¥¸ í´ë”ë¡œ ë³€ê²½ ì‹œ ì¸ì¦ ìƒíƒœ ì´ˆê¸°í™”
    if st.session_state.selected_folder in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"] and selected_folder not in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"]:
        st.session_state.finance_authenticated = False
        st.info("ğŸ” ì¬ë¬´ ì •ë³´ì—ì„œ ë‹¤ë¥¸ í´ë”ë¡œ ë³€ê²½ë˜ì–´ ì¸ì¦ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    st.session_state.selected_folder = selected_folder
    folder_path = folder_options[selected_folder]
    
    # ì „ì²´ ê²€ìƒ‰ì¸ ê²½ìš° finance í´ë” ì œì™¸
    if selected_folder == "ì „ì²´ ê²€ìƒ‰ (ì¬ë¬´ ì •ë³´ ì œì™¸)":
        st.session_state.chatbot.rag_system.set_search_folder(folder_path)
        st.info(f"ğŸ“ ì„ íƒëœ í´ë”: {selected_folder} (finance í´ë” ì œì™¸)")
    else:
        st.session_state.chatbot.rag_system.set_search_folder(folder_path)
        st.info(f"ğŸ“ ì„ íƒëœ í´ë”: {selected_folder}")

st.markdown("---")

# LLM ì œê³µì/ëª¨ë¸ ì„ íƒ
with st.sidebar:
    st.markdown("## âš™ï¸ ì„¤ì •")
    
    # ì¬ë¬´ ì •ë³´ ì¸ì¦ ìƒíƒœ í‘œì‹œ
    if selected_folder in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"]:
        if st.session_state.finance_authenticated:
            st.success("ğŸ” ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œ í™•ì¸ë¨")
        else:
            st.error("ğŸ” ì¬ë¬´ ì •ë³´ ì ‘ê·¼ ê¶Œí•œ í•„ìš”")
    
    st.markdown("---")
    providers = st.session_state.chatbot.llm_client.get_available_providers()
    if providers:
        # OpenAIë¥¼ ê¸°ë³¸ ì œê³µìë¡œ ì„¤ì •
        default_provider = 'openai' if 'openai' in providers else providers[0]
        provider_index = providers.index(default_provider) if default_provider in providers else 0
        
        selected_provider = st.selectbox(
            "LLM ì œê³µì ì„ íƒ",
            providers,
            index=provider_index
        )
        models = st.session_state.chatbot.llm_client.get_models_for_provider(selected_provider)
        if models:
            # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
            if selected_provider == 'openai' and 'gpt-4o-mini' in models:
                model_index = models.index('gpt-4o-mini')
            elif selected_provider == 'anthropic' and 'claude-3-7-sonnet-latest' in models:
                model_index = models.index('claude-3-7-sonnet-latest')
            elif selected_provider == 'perplexity' and 'sonar-pro' in models:
                model_index = models.index('sonar-pro')
            elif selected_provider == 'ollama' and 'mistral:latest' in models:
                model_index = models.index('mistral:latest')
            else:
                model_index = 0
            
            selected_model = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                models,
                index=model_index
            )
        else:
            selected_model = None
    else:
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.")
        selected_provider = None
        selected_model = None
    temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.0, 1.0, 0.7, 0.1)
    
    # ì„ë² ë”© ëª¨ë¸ ì •ë³´
    if st.session_state.get('chatbot') and hasattr(st.session_state.chatbot.rag_system, 'embeddings'):
        embedding_model = st.session_state.chatbot.rag_system.embeddings
        if hasattr(embedding_model, 'model'):
            st.info(f"ğŸ”¤ ì„ë² ë”© ëª¨ë¸: {embedding_model.model}")
        else:
            st.info("ğŸ”¤ ì„ë² ë”© ëª¨ë¸: OpenAI (ê¸°ë³¸)")
    
    # ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ (ê°œë°œìš©)
    debug_mode = st.checkbox("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ", value=False, help="ê²€ìƒ‰ ê³¼ì •ì„ ìì„¸íˆ ë³´ì—¬ì¤ë‹ˆë‹¤")
    if debug_mode:
        st.session_state.debug_mode = True
        st.info("ğŸ› ë””ë²„ê·¸ ëª¨ë“œê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸í•œ ì˜¤ë¥˜ ì •ë³´ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
    else:
        st.session_state.debug_mode = False
    
    # LLM ì œê³µìë³„ ìƒíƒœ í™•ì¸
    if selected_provider in ['perplexity', 'anthropic', 'google']:
        st.markdown(f"### ğŸ” {selected_provider.upper()} API ìƒíƒœ")
        
        if selected_provider == 'perplexity':
            api_key = os.getenv('PERPLEXITY_API_KEY')
            if api_key:
                st.success("âœ… Perplexity API í‚¤ ì„¤ì •ë¨")
                if st.button("ğŸ” API ì—°ê²° í…ŒìŠ¤íŠ¸"):
                    try:
                        client = openai.OpenAI(
                            api_key=api_key,
                            base_url="https://api.perplexity.ai"
                        )
                        
                        # ì—¬ëŸ¬ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
                        test_models = ["sonar-pro", "sonar-small-online"]
                        test_success = False
                        
                        for test_model in test_models:
                            try:
                                test_response = client.chat.completions.create(
                                    model=test_model,
                                    messages=[{"role": "user", "content": "Hello"}],
                                    max_tokens=10
                                )
                                st.success(f"âœ… Perplexity API ì—°ê²° ì„±ê³µ! (ëª¨ë¸: {test_model})")
                                st.info(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response.choices[0].message.content}")
                                test_success = True
                                break
                            except Exception as model_error:
                                error_msg = str(model_error)
                                if "Invalid model" in error_msg:
                                    st.warning(f"ëª¨ë¸ {test_model} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„...")
                                    continue
                                else:
                                    st.error(f"âŒ Perplexity API ì—°ê²° ì‹¤íŒ¨: {error_msg}")
                                    break
                        
                        if not test_success:
                            st.error("âŒ ëª¨ë“  Perplexity ëª¨ë¸ì—ì„œ ì—°ê²° ì‹¤íŒ¨")
                            
                    except Exception as e:
                        st.error(f"âŒ Perplexity API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            else:
                st.error("âŒ Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                st.info("í™˜ê²½ë³€ìˆ˜ PERPLEXITY_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        
        elif selected_provider == 'anthropic':
            anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
            if anthropic_api_key:
                st.success("âœ… Anthropic API í‚¤ ì„¤ì •ë¨")
                if st.button("ğŸ” API ì—°ê²° í…ŒìŠ¤íŠ¸"):
                    try:
                        from langchain_anthropic import ChatAnthropic
                        client = ChatAnthropic(
                            model="claude-3-5-sonnet-20241022",
                            anthropic_api_key=anthropic_api_key,
                            temperature=0.7,
                            max_tokens=10
                        )
                        test_response = client.invoke([
                            {"role": "user", "content": "Hello"}
                        ])
                        st.success("âœ… Anthropic API ì—°ê²° ì„±ê³µ!")
                        st.info(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response.content}")
                    except Exception as e:
                        st.error(f"âŒ Anthropic API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            else:
                st.error("âŒ Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                st.info("í™˜ê²½ë³€ìˆ˜ ANTHROPIC_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
        
        elif selected_provider == 'google':
            google_api_key = os.getenv('GOOGLE_API_KEY')
            if google_api_key:
                st.success("âœ… Google API í‚¤ ì„¤ì •ë¨")
                if st.button("ğŸ” API ì—°ê²° í…ŒìŠ¤íŠ¸"):
                    try:
                        from langchain_google_genai import ChatGoogleGenerativeAI
                        client = ChatGoogleGenerativeAI(
                            model="gemini-2.5-flash",
                            google_api_key=google_api_key,
                            temperature=0.7,
                            max_output_tokens=10
                        )
                        test_response = client.invoke([
                            {"role": "user", "content": "Hello"}
                        ])
                        st.success("âœ… Google Gemini API ì—°ê²° ì„±ê³µ!")
                        st.info(f"í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {test_response.content}")
                        st.info("ğŸ†• ìƒˆë¡œìš´ Gemini 2.5 ëª¨ë¸ ì§€ì›:")
                        st.info("â€¢ gemini-2.5-pro: ê°€ì¥ ê°•ë ¥í•œ ëª¨ë¸ (adaptive thinking)")
                        st.info("â€¢ gemini-2.5-flash: ì•ˆì •ì ì¸ 2.5 Flash ëª¨ë¸")
                        st.info("â€¢ gemini-2.5-flash-lite-preview-06-17: ì €ë¹„ìš© ê³ ì„±ëŠ¥ ëª¨ë¸")
                    except Exception as e:
                        st.error(f"âŒ Google Gemini API ì—°ê²° ì‹¤íŒ¨: {str(e)}")
            else:
                st.error("âŒ Google API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                st.info("í™˜ê²½ë³€ìˆ˜ GOOGLE_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”")
    
    st.markdown("---")
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    st.markdown("### ğŸ“ íŒŒì¼ ì—…ë¡œë“œ")
    
    # ì—…ë¡œë“œ í´ë” ì„ íƒ
    upload_folder_options = {
        "ì „ì²´ ê²€ìƒ‰ (ì¬ë¬´ ì •ë³´ ì œì™¸)": "./pages/rag_files",
        "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤": "./pages/rag_files/process",
        "ë™ë£Œ ì •ë³´": "./pages/rag_files/colleagues", 
        "ìœ„ì„ì „ê²°ê·œì •": "./pages/rag_files/approval",
        "ì¬ë¬´ ì •ë³´": "./pages/rag_files/finance",
        "ì¬ë¬´ ì •ë³´-ê³ ê¸‰": "./pages/rag_files/finance",
        "ì¼ë°˜ ì±„íŒ…": "./pages/rag_files/general"
    }
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì„ íƒëœ í´ë” ì‚¬ìš©
    default_upload_folder = selected_folder if selected_folder in upload_folder_options else "ì „ì²´ ê²€ìƒ‰ (ì¬ë¬´ ì •ë³´ ì œì™¸)"
    
    upload_folder = st.selectbox(
        "ğŸ“‚ ì—…ë¡œë“œí•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        options=list(upload_folder_options.keys()),
        index=list(upload_folder_options.keys()).index(default_upload_folder),
        help="íŒŒì¼ì„ ì €ì¥í•  í´ë”ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ í˜„ì¬ ì„ íƒëœ ê²€ìƒ‰ í´ë”ì…ë‹ˆë‹¤."
    )
    
    # ì„ íƒëœ ì—…ë¡œë“œ í´ë” ì •ë³´ í‘œì‹œ
    upload_folder_path = upload_folder_options[upload_folder]
    st.info(f"ğŸ“‚ **ì—…ë¡œë“œ ëŒ€ìƒ í´ë”:** {upload_folder}\nğŸ“ **ê²½ë¡œ:** {upload_folder_path}")
    
    # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
    supported_types = ['pdf', 'docx', 'pptx', 'xlsx', 'txt', 'md', 'gdoc', 'gsheet', 'gslides']
    
    # íŒŒì¼ í˜•ì‹ ì„¤ëª…
    st.info("""
    ğŸ“„ **ì§€ì› íŒŒì¼ í˜•ì‹:**
    - **PDF**: PDF ë¬¸ì„œ
    - **DOCX**: Word ë¬¸ì„œ
    - **PPTX**: PowerPoint í”„ë ˆì  í…Œì´ì…˜
    - **XLSX**: Excel ìŠ¤í”„ë ˆë“œì‹œíŠ¸
    - **TXT**: í…ìŠ¤íŠ¸ íŒŒì¼
    - **MD**: Markdown íŒŒì¼
    - **GDOC**: êµ¬ê¸€ ë¬¸ì„œ (HTML í˜•ì‹)
    - **GSHEET**: êµ¬ê¸€ ì‹œíŠ¸ (HTML í˜•ì‹)
    - **GSLIDES**: êµ¬ê¸€ ìŠ¬ë¼ì´ë“œ (HTML í˜•ì‹)
    """)
    
    # ì¬ë¬´ ì •ë³´ í´ë” ì—…ë¡œë“œ ì‹œ ì¸ì¦ í™•ì¸
    if upload_folder in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"] and not st.session_state.finance_authenticated:
        st.error("ğŸ” ì¬ë¬´ ì •ë³´ í´ë”ì— íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë ¤ë©´ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        st.info("ì¬ë¬´ ì •ë³´ í´ë”ë¥¼ ì„ íƒí•˜ê³  ì•”í˜¸ë¥¼ ì…ë ¥í•œ í›„ íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        uploaded_files = None
    else:
        uploaded_files = st.file_uploader(
            "RAGì— ì‚¬ìš©í•  íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:",
            type=supported_types,
            accept_multiple_files=True,
            help="PDF, DOCX, PPTX, XLSX, TXT, MD, GDOC(êµ¬ê¸€ë¬¸ì„œ), GSHEET(êµ¬ê¸€ì‹œíŠ¸), GSLIDES(êµ¬ê¸€ìŠ¬ë¼ì´ë“œ) íŒŒì¼ì„ ì§€ì›í•©ë‹ˆë‹¤."
        )
    
    if uploaded_files:
        # ì„ íƒëœ ì—…ë¡œë“œ í´ë” ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        target_folder = upload_folder_options[upload_folder]
        
        # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if not os.path.exists(target_folder):
            os.makedirs(target_folder)
        
        uploaded_count = 0
        for uploaded_file in uploaded_files:
            try:
                # íŒŒì¼ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
                safe_filename = "".join(c for c in uploaded_file.name if c.isalnum() or c in (' ', '-', '_', '.')).rstrip()
                safe_filename = safe_filename.replace(' ', '_')
                
                # íŒŒì¼ ê²½ë¡œ
                file_path = os.path.join(target_folder, safe_filename)
                
                # íŒŒì¼ ì €ì¥
                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getbuffer())
                
                uploaded_count += 1
                st.success(f"âœ… {safe_filename} ì—…ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                st.error(f"âŒ {uploaded_file.name} ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        if uploaded_count > 0:
            st.info(f"ğŸ“ {uploaded_count}ê°œ íŒŒì¼ì´ '{upload_folder}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìë™ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì˜µì…˜
            if st.button("ğŸ”„ ìë™ìœ¼ë¡œ íŒŒì¼ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•", type="primary"):
                with st.spinner("íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤..."):
                    try:
                        # ì—…ë¡œë“œëœ í´ë”ë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
                        folder_path = upload_folder_options[upload_folder]
                        st.session_state.chatbot.rag_system.set_search_folder(folder_path)
                        st.success(f"âœ… íŒŒì¼ ì¸ë±ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤! (í´ë”: {upload_folder})")
                    except Exception as e:
                        st.error(f"âŒ ì¸ë±ìŠ¤ ì¬êµ¬ì¶• ì‹¤íŒ¨: {str(e)}")
            
            st.info("ğŸ’¡ ìœ„ ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìƒˆë¡œ ì—…ë¡œë“œëœ íŒŒì¼ì„ ê²€ìƒ‰ì— í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    st.markdown("---")
    
    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"):
        if selected_folder == "ì¼ë°˜ ì±„íŒ…":
            st.session_state.conversations['ì¼ë°˜'] = []
        elif selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
            st.session_state.conversations['ì¬ë¬´ ì •ë³´-ê³ ê¸‰'] = []
        else:
            st.session_state.conversations['ì¼ë°˜'] = []
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()
    
    if st.button("ğŸ”„ íŒŒì¼ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"):
        # ì¬ë¬´ ì •ë³´ í´ë” ì¬êµ¬ì¶• ì‹œ ì¸ì¦ í™•ì¸
        if st.session_state.selected_folder in ["ì¬ë¬´ ì •ë³´", "ì¬ë¬´ ì •ë³´-ê³ ê¸‰"] and not st.session_state.finance_authenticated:
            st.error("ğŸ” ì¬ë¬´ ì •ë³´ í´ë”ì˜ ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶•í•˜ë ¤ë©´ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            with st.spinner("íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤..."):
                # í˜„ì¬ ì„ íƒëœ í´ë”ë¡œ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•
                folder_path = folder_options[st.session_state.selected_folder]
                st.session_state.chatbot.rag_system.set_search_folder(folder_path)
                st.success(f"íŒŒì¼ ì¸ë±ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤! (í´ë”: {st.session_state.selected_folder})")

# CSS ìŠ¤íƒ€ì¼ ì¶”ê°€ (ì›ë˜ ìŠ¤íƒ€ì¼ ë³µì›)
st.markdown("""
<style>
/* ì‚¬ìš©ì ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
.user-bubble {
    background: #2d2d2d;
    color: #fff;
    padding: 0.7em 1em;
    border-radius: 1em 1em 0 1em;
    margin: 0.5em 0;
    max-width: 70%;
    align-self: flex-end;
    text-align: right;
}

/* ì±—ë´‡ ì‘ë‹µ ìŠ¤íƒ€ì¼ */
.bot-bubble {
    background: #2323a7;
    color: #fff;
    padding: 0.7em 1em;
    border-radius: 1em 1em 1em 0;
    margin: 0.5em 0;
    max-width: 70%;
    align-self: flex-start;
    text-align: left;
}

/* ëŒ€í™” ì»¨í…Œì´ë„ˆ */
.bubble-container {
    display: flex;
    flex-direction: column;
}

/* ë¶„ì„ ì™„ë£Œ ìŠ¤íƒ€ì¼ */
.analysis-complete {
    background: #d4edda;
    border: 2px solid #28a745;
    border-radius: 12px;
    padding: 15px;
    margin: 10px 0;
    text-align: center;
}

/* ë ˆì¸ë³´ìš° ì• ë‹ˆë©”ì´ì…˜ */
@keyframes rainbow {
    0% { background-position: 0% 50%; }
    50% { background-position: 100% 50%; }
    100% { background-position: 0% 50%; }
}
</style>
""", unsafe_allow_html=True)

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ë§¨ ìœ„)
#st.markdown("### ğŸ’¬ ì±—ë´‡ê³¼ ëŒ€í™”í•˜ê¸°")

# í˜„ì¬ í´ë”ì— ë”°ë¥¸ ëŒ€í™” ê¸°ë¡ ì„ íƒ
if selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
    current_conversation_key = 'ì¬ë¬´ ì •ë³´-ê³ ê¸‰'
elif selected_folder == "ì¼ë°˜ ì±„íŒ…":
    current_conversation_key = 'ì¼ë°˜'
else:
    current_conversation_key = 'ì¼ë°˜'
current_conversation = st.session_state.conversations.get(current_conversation_key, [])

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ì›ë˜ ìŠ¤íƒ€ì¼)
st.markdown('<div class="bubble-container">', unsafe_allow_html=True)
for i, message in enumerate(current_conversation):
    if message['role'] == 'ì‚¬ìš©ì':
        st.markdown(f"<div class='user-bubble'>ğŸ‘¤ {message['content']}</div>", unsafe_allow_html=True)
    else:
        # ì¬ë¬´ ì •ë³´-ê³ ê¸‰ ë¶„ì„ ê²°ê³¼ì¸ì§€ í™•ì¸ (í˜„ì¬ ì„ íƒëœ í´ë”ë„ í™•ì¸)
        if message.get('persona_analyses') and selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
            # ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ê²°ê³¼ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í‘œì‹œ
            st.markdown("---")
            st.markdown("## ğŸ” ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ê²°ê³¼")
            
            # ë¶„ì„ ìš”ì²­ í‘œì‹œ
            user_message = current_conversation[i-1]['content'] if i > 0 else "ë¶„ì„ ìš”ì²­"
            st.markdown(f"**ğŸ“‹ ë¶„ì„ ìš”ì²­:** {user_message}")
            st.markdown("---")
            
            # ì „ë¬¸ê°€ë³„ ë¶„ì„ í‘œì‹œ
            st.markdown("### ğŸ‘¥ ì „ë¬¸ê°€ë³„ ë¶„ì„")
            for persona_key, analysis in message['persona_analyses'].items():
                if analysis.get('success') and analysis.get('result'):
                    persona_info = FINANCE_PERSONAS[persona_key]
                    with st.expander(f"{persona_info['emoji']} {persona_info['name']} ë¶„ì„", expanded=True):
                        st.markdown(analysis['result'])
            
            # ìµœì¢… ì¢…í•© ë³´ê³ ì„œ í‘œì‹œ
            final_report = message['content'].split("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n")[-1] if "### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n" in message['content'] else ""
            if final_report:
                st.markdown("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ")
                st.markdown(final_report)
            
            # ë‹¤ìš´ë¡œë“œ ì„¹ì…˜
            st.markdown("---")
            st.markdown("### ğŸ“„ ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ")
            
            # íŒŒì¼ í˜•ì‹ ì„ íƒ
            file_format = st.radio(
                "ë‹¤ìš´ë¡œë“œí•  íŒŒì¼ í˜•ì‹ì„ ì„ íƒí•˜ì„¸ìš”:",
                ["ğŸ“ DOCX","ğŸ“„ PDF"],
                key=f"format_radio_{i}"
            )
            
            # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
            if file_format == "ğŸ“„ PDF":
                if st.download_button(
                    label="ğŸ’¾ PDF ë‹¤ìš´ë¡œë“œ",
                    data=create_finance_analysis_pdf(
                        user_message,
                        message['persona_analyses'],
                        message['content'].split("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n")[-1] if "### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n" in message['content'] else "",
                        message.get('rag_context', ''),
                        message.get('market_research', '')
                    ).getvalue() if create_finance_analysis_pdf(
                        user_message,
                        message['persona_analyses'],
                        message['content'].split("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n")[-1] if "### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n" in message['content'] else "",
                        message.get('rag_context', ''),
                        message.get('market_research', '')
                    ) else b"",
                    file_name=f"ì¬ë¬´ë¶„ì„ë³´ê³ ì„œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                    mime="application/pdf",
                    key=f"pdf_download_{i}"
                ):
                    st.success("PDF ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
            else:  # DOCX
                if st.download_button(
                    label="ğŸ’¾ DOCX ë‹¤ìš´ë¡œë“œ",
                    data=create_finance_analysis_docx(
                        user_message,
                        message['persona_analyses'],
                        message['content'].split("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n")[-1] if "### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n" in message['content'] else "",
                        message.get('rag_context', ''),
                        message.get('market_research', '')
                    ).getvalue() if create_finance_analysis_docx(
                        user_message,
                        message['persona_analyses'],
                        message['content'].split("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n")[-1] if "### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n" in message['content'] else "",
                        message.get('rag_context', ''),
                        message.get('market_research', '')
                    ) else b"",
                    file_name=f"ì¬ë¬´ë¶„ì„ë³´ê³ ì„œ_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx",
                    mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                    key=f"docx_download_{i}"
                ):
                    st.success("DOCX ë‹¤ìš´ë¡œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            st.info("ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ê²°ê³¼ë¥¼ PDF ë˜ëŠ” DOCXë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            # ì¼ë°˜ ì±—ë´‡ ì‘ë‹µ í‘œì‹œ
            if selected_folder == "ì¼ë°˜ ì±„íŒ…":
                # ì¼ë°˜ ì±„íŒ… ëª¨ë“œ: ê°„ë‹¨í•œ ì‘ë‹µë§Œ í‘œì‹œ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                formatted_content = message['content'].replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                st.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_content}</div>", unsafe_allow_html=True)
            else:
                # RAG ëª¨ë“œ: ì°¸ê³  ë¬¸ì„œì™€ í•¨ê»˜ í‘œì‹œ
                st.markdown(f"<div class='bot-bubble'>ğŸ¤” {message['content']}</div>", unsafe_allow_html=True)
                
                # ì°¸ê³  ë¬¸ì„œëŠ” AI ë‹µë³€ ë‚´ë¶€ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë³„ë„ í‘œì‹œ ì œê±°
                pass

st.markdown('</div>', unsafe_allow_html=True)

#st.markdown('</div>', unsafe_allow_html=True)

# ì…ë ¥ í•„ë“œ (ë§¨ ì•„ë˜ ê³ ì •)
#st.markdown("---")
st.markdown("### ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

# ì…ë ¥ í•„ë“œ í‚¤ ê´€ë¦¬
if 'chat_input_key' not in st.session_state:
    st.session_state['chat_input_key'] = 0

# ì…ë ¥ í•„ë“œ
user_input = st.text_area(
    "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
    key=f"chat_input_{st.session_state['chat_input_key']}",
    value="",
    height=80,
    label_visibility="collapsed",
    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
)

if st.button("ì „ì†¡", type="primary"):
    if user_input.strip():
        # í˜„ì¬ í´ë”ì— ë”°ë¥¸ ëŒ€í™” ê¸°ë¡ ì„ íƒ
        if selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
            current_conversation_key = 'ì¬ë¬´ ì •ë³´-ê³ ê¸‰'
        elif selected_folder == "ì¼ë°˜ ì±„íŒ…":
            current_conversation_key = 'ì¼ë°˜'
        else:
            current_conversation_key = 'ì¼ë°˜'
            
        if current_conversation_key not in st.session_state.conversations:
            st.session_state.conversations[current_conversation_key] = []
        
        st.session_state.conversations[current_conversation_key].append({'role': 'ì‚¬ìš©ì', 'content': user_input, 'timestamp': datetime.now()})
        
        # ì¼ë°˜ ì±„íŒ… ëª¨ë“œì¸ ê²½ìš° RAG ì—†ì´ ìˆœìˆ˜ LLM ì‘ë‹µ
        if selected_folder == "ì¼ë°˜ ì±„íŒ…":
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•œ ì»¨í…Œì´ë„ˆ ìƒì„±
            response_container = st.empty()
            full_response = ""
            
            # ëŒ€í™” ê¸°ë¡ì„ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ìµœê·¼ 10ê°œ ëŒ€í™”ë§Œ ì‚¬ìš©)
            conversation_messages = []
            recent_messages = st.session_state.conversations['ì¼ë°˜'][-20:]  # ìµœê·¼ 20ê°œ ë©”ì‹œì§€ (10ê°œ ëŒ€í™”)
            
            for msg in recent_messages:
                if msg['role'] == 'ì‚¬ìš©ì':
                    conversation_messages.append({"role": "user", "content": msg['content']})
                elif msg['role'] == 'ì±—ë´‡':
                    conversation_messages.append({"role": "assistant", "content": msg['content']})
            
            # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
            conversation_messages.append({"role": "user", "content": user_input})
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            with st.spinner("ë‹µë³€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    if selected_provider == 'openai':
                        # OpenAI ìŠ¤íŠ¸ë¦¬ë°
                        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                        stream = client.chat.completions.create(
                            model=selected_model,
                            messages=conversation_messages,
                            temperature=temperature,
                            stream=True
                        )
                        
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                full_response += chunk.choices[0].delta.content
                                # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                                formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                                
                    elif selected_provider == 'anthropic':
                        # Anthropic ìŠ¤íŠ¸ë¦¬ë°
                        try:
                            import anthropic
                            client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
                            with client.messages.stream(
                                model=selected_model,
                                max_tokens=2000,
                                temperature=temperature,
                                messages=conversation_messages
                            ) as stream:
                                for text in stream.text_stream:
                                    full_response += text
                                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                                    formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                    response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                        except Exception as e:
                            st.error(f"Anthropic ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
                            # í´ë°±ìœ¼ë¡œ ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
                            response = st.session_state.chatbot.llm_client.generate_response(
                                selected_provider, selected_model, 
                                conversation_messages, 
                                temperature
                            )
                            if isinstance(response, tuple):
                                full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                            else:
                                full_response = response
                            formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                            response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                                
                    elif selected_provider == 'ollama':
                        # Ollama ìŠ¤íŠ¸ë¦¬ë°
                        import requests
                        import json
                        
                        # Ollama API í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ë³€í™˜
                        ollama_messages = []
                        for msg in conversation_messages:
                            if msg['role'] == 'user':
                                ollama_messages.append({
                                    'role': 'user',
                                    'content': msg['content']
                                })
                            elif msg['role'] == 'assistant':
                                ollama_messages.append({
                                    'role': 'assistant',
                                    'content': msg['content']
                                })
                        
                        # Ollama ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
                        response = requests.post(
                            "http://localhost:11434/api/chat",
                            json={
                                "model": selected_model,
                                "messages": ollama_messages,
                                "stream": True,
                                "options": {
                                    "temperature": temperature
                                }
                            },
                            stream=True,
                            timeout=60
                        )
                        
                        for line in response.iter_lines():
                            if line:
                                try:
                                    data = json.loads(line.decode('utf-8'))
                                    if 'message' in data and 'content' in data['message']:
                                        chunk = data['message']['content']
                                        full_response += chunk
                                        # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                                except json.JSONDecodeError:
                                    continue
                                
                    elif selected_provider == 'perplexity':
                        # Perplexity ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
                        try:

                            
                            # ë©”ì‹œì§€ í˜•ì‹ ìˆ˜ì • (ìŠ¤íŠ¸ë¦¬ë°ìš©)
                            formatted_messages = []
                            seen_messages = set()
                            
                            for msg in conversation_messages:
                                message_key = f"{msg['role']}:{msg['content']}"
                                if message_key not in seen_messages:
                                    seen_messages.add(message_key)
                                    if msg['role'] == 'system':
                                        formatted_messages.append({"role": "system", "content": msg['content']})
                                    elif msg['role'] == 'user':
                                        formatted_messages.append({"role": "user", "content": msg['content']})
                                    elif msg['role'] == 'assistant':
                                        formatted_messages.append({"role": "assistant", "content": msg['content']})
                            
                            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ìœ ì§€
                            if len(formatted_messages) > 1:
                                final_messages = []
                                for msg in formatted_messages:
                                    if msg['role'] == 'system':
                                        final_messages.append(msg)
                                
                                for msg in reversed(formatted_messages):
                                    if msg['role'] == 'user':
                                        final_messages.append(msg)
                                        break
                                
                                formatted_messages = final_messages
                            
                            # Perplexity ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
                            client = openai.OpenAI(
                                api_key=os.getenv('PERPLEXITY_API_KEY'),
                                base_url="https://api.perplexity.ai"
                            )
                            
                            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë„
                            test_models = ["sonar-pro", "sonar-small-chat"]
                            stream_success = False
                            
                            for test_model in test_models:
                                try:

                                    stream = client.chat.completions.create(
                                        model=test_model,
                                        messages=formatted_messages,
                                        temperature=temperature,
                                        stream=True
                                    )
                                    
                                    for chunk in stream:
                                        if chunk.choices[0].delta.content is not None:
                                            full_response += chunk.choices[0].delta.content
                                            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                                            formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                            response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                                    
                                    stream_success = True

                                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ
                                    
                                except Exception as model_error:
                                    error_msg = str(model_error)

                                    
                                    if "Invalid model" in error_msg or "model not found" in error_msg.lower():
                                        continue  # ë‹¤ìŒ ëª¨ë¸ ì‹œë„
                                    else:
                                        # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì¤‘ë‹¨
                                        raise model_error
                            
                            if not stream_success:

                                # í´ë°±ìœ¼ë¡œ ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
                                response = st.session_state.chatbot.llm_client.generate_response(
                                    selected_provider, selected_model, 
                                    conversation_messages, 
                                    temperature
                                )
                                if isinstance(response, tuple):
                                    full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                                    error_msg = response[1] if len(response) > 1 else None
                                    if error_msg:
                                        st.error(f"Perplexity ì‘ë‹µ ì˜¤ë¥˜: {error_msg}")
                                else:
                                    full_response = response
                                
                                # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©
                                formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                            
                        except Exception as e:
                            st.error(f"Perplexity ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {str(e)}")
                            st.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                            
                            # í´ë°±ìœ¼ë¡œ ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
                            try:
                                response = st.session_state.chatbot.llm_client.generate_response(
                                    selected_provider, selected_model, 
                                    conversation_messages, 
                                    temperature
                                )
                                if isinstance(response, tuple):
                                    full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                                else:
                                    full_response = response
                            except:
                                full_response = "Perplexity ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                            
                            response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {full_response}</div>", unsafe_allow_html=True)
                    elif selected_provider == 'google':
                        # Google Gemini ìŠ¤íŠ¸ë¦¬ë°
                        try:
                            from langchain_google_genai import ChatGoogleGenerativeAI
                            client = ChatGoogleGenerativeAI(
                                model=selected_model,
                                google_api_key=os.getenv('GOOGLE_API_KEY'),
                                temperature=temperature,
                                max_output_tokens=2000
                            )
                            # LangChainì˜ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
                            for chunk in client.stream(conversation_messages):
                                if hasattr(chunk, 'content'):
                                    full_response += chunk.content
                                    # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                                    formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                    response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                        except Exception as e:
                            error_msg = str(e)
                            if "429" in error_msg or "quota" in error_msg.lower():
                                st.error("âš ï¸ Google Gemini API í• ë‹¹ëŸ‰ ì´ˆê³¼. ë‹¤ë¥¸ LLM ì œê³µìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
                                st.info("ğŸ’¡ OpenAI, Anthropic, Perplexity, Ollama ì¤‘ ì„ íƒí•˜ì„¸ìš”.")
                            else:
                                st.error(f"Google Gemini ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {error_msg}")
                            
                            # í´ë°±ìœ¼ë¡œ ë‹¤ë¥¸ ì œê³µì ì‚¬ìš© ì œì•ˆ

                            # OpenAIë¡œ í´ë°±
                            try:
                                openai_client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
                                stream = openai_client.chat.completions.create(
                                    model="gpt-4o-mini",
                                    messages=conversation_messages,
                                    temperature=temperature,
                                    stream=True
                                )
                                
                                for chunk in stream:
                                    if chunk.choices[0].delta.content is not None:
                                        full_response += chunk.choices[0].delta.content
                                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                            except Exception as fallback_error:
                                st.error(f"í´ë°± ì‘ë‹µë„ ì‹¤íŒ¨: {str(fallback_error)}")
                                full_response = "API í• ë‹¹ëŸ‰ ì´ˆê³¼ë¡œ ì¸í•´ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ë‹¤ë¥¸ LLM ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”."
                                response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {full_response}</div>", unsafe_allow_html=True)
                    else:
                        # ê¸°íƒ€ ì œê³µìëŠ” ì¼ë°˜ ì‘ë‹µ
                        response = st.session_state.chatbot.llm_client.generate_response(
                            selected_provider, selected_model, 
                            conversation_messages, 
                            temperature
                        )
                        full_response = response
                        # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©
                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                        
                except Exception as e:
                    st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    st.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                    st.error(f"ì˜¤ë¥˜ ìƒì„¸: {e}")
                    
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë°˜ ì‘ë‹µìœ¼ë¡œ í´ë°±
                    try:

                        response = st.session_state.chatbot.llm_client.generate_response(
                            selected_provider, selected_model, 
                            conversation_messages, 
                            temperature
                        )
                        # responseê°€ íŠœí”Œì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                        if isinstance(response, tuple):
                            full_response = response[0] if response[0] else "ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                            error_msg = response[1] if len(response) > 1 else None
                            if error_msg:
                                st.error(f"LLM ì‘ë‹µ ì˜¤ë¥˜: {error_msg}")
                        else:
                            full_response = response
                        # ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©
                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                    except Exception as fallback_error:
                        st.error(f"í´ë°± ì‘ë‹µ ìƒì„±ë„ ì‹¤íŒ¨: {str(fallback_error)}")
                        st.error(f"í´ë°± ì˜¤ë¥˜ íƒ€ì…: {type(fallback_error).__name__}")
                        st.error(f"í´ë°± ì˜¤ë¥˜ ìƒì„¸: {fallback_error}")
                        full_response = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {full_response}</div>", unsafe_allow_html=True)
            
            # ìµœì¢… ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
            st.session_state.conversations['ì¼ë°˜'].append({
                'role': 'ì±—ë´‡', 
                'content': full_response, 
                'timestamp': datetime.now(), 
                'provider': selected_provider, 
                'model': selected_model
            })
            
        # ì¬ë¬´ ì •ë³´-ê³ ê¸‰ ì˜µì…˜ì¸ ê²½ìš° ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ ìˆ˜í–‰
        elif selected_folder == "ì¬ë¬´ ì •ë³´-ê³ ê¸‰":
            st.markdown("## ğŸ” ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ì‹œì‘")
            
            # í•µì‹¬ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„ í‘œì‹œ
            with st.expander("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ í˜„í™©", expanded=True):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.markdown("### ğŸ“ RAG íŒŒì¼ ì •ë³´ ìˆ˜ì§‘")
                    # 1. RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    rag_context = ""
                    relevant_docs = st.session_state.chatbot.rag_system.search(user_input, k=5)
                    if relevant_docs:
                        context_parts = []
                        for i, doc in enumerate(relevant_docs, 1):
                            title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                            content = doc.page_content[:1000]
                            context_parts.append(f"ë¬¸ì„œ {i} - {title}:\n{content}")
                        rag_context = "\n\n".join(context_parts)
                        st.success(f"âœ… {len(relevant_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬")
                        for i, doc in enumerate(relevant_docs, 1):
                            title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                            st.info(f"ğŸ“„ {title}")
                    else:
                        st.warning("âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                with col2:
                    st.markdown("### ğŸŒ Perplexity ì‹œì¥ ì¡°ì‚¬")
                    # 2. ì‹œì¥ ì¡°ì‚¬ ìˆ˜í–‰
                    market_research = ""
                    try:
                        market_query = f"{user_input} ê´€ë ¨ ìµœì‹  ì‹œì¥ ë™í–¥ ë° ê²½ìŸì‚¬ ë¶„ì„"
                        with st.spinner("ğŸ” ì‹œì¥ ì¡°ì‚¬ ì¤‘..."):
                            market_research = perform_perplexity_search(market_query)
                        if market_research:
                            st.success("âœ… ì‹œì¥ ì¡°ì‚¬ ì™„ë£Œ")
                            st.info(f"ğŸ“Š ì¡°ì‚¬ í‚¤ì›Œë“œ: {market_query}")
                        else:
                            st.warning("âš ï¸ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    except Exception as e:
                        st.error(f"âŒ ì‹œì¥ ì¡°ì‚¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # ë°ì´í„° ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
            if rag_context or market_research:
                st.markdown("---")
                st.markdown("### ğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ìš”ì•½")
                
                if rag_context:
                    st.markdown("**ğŸ“ RAG íŒŒì¼ ì •ë³´:**")
                    st.markdown(f"```\n{rag_context[:500]}...\n```")
                
                if market_research:
                    st.markdown("**ğŸŒ ì‹œì¥ ì¡°ì‚¬ ì •ë³´:**")
                    st.markdown(f"```\n{market_research[:500]}...\n```")
                
                st.success("ğŸ¯ RAG íŒŒì¼ ì •ë³´ + Perplexity ì‹œì¥ ì¡°ì‚¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì „ë¬¸ê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
            else:
                st.warning("âš ï¸ RAG íŒŒì¼ ì •ë³´ì™€ ì‹œì¥ ì¡°ì‚¬ ê²°ê³¼ê°€ ëª¨ë‘ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì •ë³´ë§Œìœ¼ë¡œ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # 3. ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ ìˆ˜í–‰ (RAG + ì‹œì¥ ì¡°ì‚¬ ê¸°ë°˜)
            st.markdown("## ğŸ‘¥ ì „ë¬¸ê°€ ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„")
            
            # ë¶„ì„ ê¸°ë°˜ ì •ë³´ í‘œì‹œ
            analysis_basis = []
            if rag_context:
                analysis_basis.append("ğŸ“ RAG íŒŒì¼ ì •ë³´")
            if market_research:
                analysis_basis.append("ğŸŒ Perplexity ì‹œì¥ ì¡°ì‚¬")
            
            if analysis_basis:
                st.info(f"**ë¶„ì„ ê¸°ë°˜:** {' + '.join(analysis_basis)}")
            
            # ê° í˜ë¥´ì†Œë‚˜ë³„ ì§„í–‰ ìƒíƒœ ì»¨í…Œì´ë„ˆ ìƒì„±
            persona_status = {}
            persona_progress = {}
            
            for persona_key, persona_info in FINANCE_PERSONAS.items():
                col1, col2 = st.columns([1, 3])
                with col1:
                    persona_status[persona_key] = st.empty()
                with col2:
                    persona_progress[persona_key] = st.progress(0.0)
            
            # ì „ì²´ ì§„í–‰ë¥  í‘œì‹œ
            overall_progress = st.progress(0.0)
            overall_status = st.empty()
            
            # ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì»¨í…Œì´ë„ˆ
            response_container = st.empty()
            streaming_summary = "## ğŸ” ë©€í‹°ì—ì´ì „íŠ¸ ì¬ë¬´ ë¶„ì„ ê²°ê³¼\n\n"
            
            # ë¶„ì„ ê¸°ë°˜ ì •ë³´ ì¶”ê°€
            if rag_context or market_research:
                streaming_summary += "### ğŸ“‹ ë¶„ì„ ê¸°ë°˜ ì •ë³´\n"
                if rag_context:
                    streaming_summary += f"**ğŸ“ RAG íŒŒì¼ ì •ë³´:** {len(relevant_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ í™œìš©\n"
                if market_research:
                    streaming_summary += f"**ğŸŒ Perplexity ì‹œì¥ ì¡°ì‚¬:** ìµœì‹  ì‹œì¥ ë™í–¥ ë° ê²½ìŸì‚¬ ë¶„ì„ í™œìš©\n"
                streaming_summary += "\n### ğŸ“Š ì „ë¬¸ê°€ë³„ ë¶„ì„\n"
            
            # ì§„í–‰ë¥  ì• ë‹ˆë©”ì´ì…˜ í•¨ìˆ˜
            def animate_progress():
                placeholder = st.empty()
                
                for i in range(100):
                    if not hasattr(st.session_state, 'finance_analysis_complete') or not st.session_state.finance_analysis_complete:
                        break
                    
                    placeholder.markdown(f"""
                    <div style="
                        background: linear-gradient(90deg, #ff6b6b, #feca57, #48dbfb, #ff9ff3, #54a0ff);
                        background-size: 500% 500%;
                        animation: rainbow 2s ease infinite;
                        border-radius: 15px;
                        padding: 20px;
                        margin: 15px 0;
                        color: white;
                        text-align: center;
                        font-weight: bold;
                        font-size: 1.1rem;
                        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
                    ">
                        ğŸŒˆ ëª¨ë“  ì¬ë¬´ ì „ë¬¸ê°€ë“¤ì´ ë¶„ì„ì— ëª°ë‘í•˜ê³  ìˆìŠµë‹ˆë‹¤... {i+1}%
                    </div>
                    """, unsafe_allow_html=True)
                    
                    time.sleep(0.05)
            
            # ë¶„ì„ ì‹œì‘ ìƒíƒœ ì„¤ì •
            st.session_state.finance_analysis_complete = False
            st.session_state.completed_finance_personas = set()
            
            # ì§„í–‰ë¥  ì• ë‹ˆë©”ì´ì…˜ ìŠ¤ë ˆë“œ ì‹œì‘
            import threading
            progress_thread = threading.Thread(target=animate_progress)
            progress_thread.daemon = True
            progress_thread.start()
            
            # ê° í˜ë¥´ì†Œë‚˜ë³„ ë¶„ì„ ì‘ì—… ì¤€ë¹„
            analysis_tasks = []
            for persona_key, persona_info in FINANCE_PERSONAS.items():
                task_args = (user_input, persona_key, persona_info, rag_context, market_research, selected_model)
                analysis_tasks.append(task_args)
            
            # ThreadPoolExecutorë¡œ ë™ì‹œ ì‹¤í–‰
            persona_analyses = {}
            with ThreadPoolExecutor(max_workers=len(FINANCE_PERSONAS)) as executor:
                future_to_persona = {
                    executor.submit(analyze_persona_concurrent_finance, task): task[1] 
                    for task in analysis_tasks
                }
                
                completed_count = 0
                for future in as_completed(future_to_persona):
                    persona_key = future_to_persona[future]
                    try:
                        persona_key, result, success = future.result()
                        persona_analyses[persona_key] = {
                            'result': result,
                            'success': success,
                            'completed': True,
                            'timestamp': datetime.now().isoformat()
                        }
                        
                        st.session_state.completed_finance_personas.add(persona_key)
                        completed_count += 1
                        
                        # ê°œë³„ í˜ë¥´ì†Œë‚˜ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        if persona_progress[persona_key] is not None:
                            persona_progress[persona_key].progress(1.0)
                        
                        # ì „ì²´ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        overall_progress.progress(completed_count / len(FINANCE_PERSONAS))
                        
                        # í˜ë¥´ì†Œë‚˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        persona_info = FINANCE_PERSONAS[persona_key]
                        if success:
                            persona_status[persona_key].markdown(f"""
                            <div class="analysis-complete">
                                <h4>ğŸ‰ {persona_info['emoji']} {persona_info['name']}</h4>
                                <p style="margin: 5px 0; font-size: 0.9rem; color: #155724;">
                                    ë¶„ì„ ì™„ë£Œ<br>
                                    ì™„ë£Œ ì‹œê°„: {datetime.fromisoformat(persona_analyses[persona_key]['timestamp']).strftime('%H:%M:%S')}
                                </p>
                            </div>
                            """, unsafe_allow_html=True)
                        else:
                            persona_status[persona_key].markdown(f"""
                            <div style="background: #f8d7da; border: 2px solid #dc3545; border-radius: 12px; padding: 15px; margin: 10px 0; text-align: center;">
                                <h4 style="color: #721c24; margin: 0;">âŒ {persona_info['emoji']} {persona_info['name']} ë¶„ì„ ì˜¤ë¥˜</h4>
                                <p style="margin: 5px 0; font-size: 0.9rem; color: #721c24;">
                                    ì˜¤ë¥˜ ì‹œê°„: {datetime.fromisoformat(persona_analyses[persona_key]['timestamp']).strftime('%H:%M:%S')}
                                </p>
                            </div>
                            """, unsafe_allow_html=True)
                        
                        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ
                        if success:
                            expert_section = f"\n#### {persona_info['emoji']} {persona_info['name']}\n{result}\n"
                            streaming_summary += expert_section
                            
                            # ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                            formatted_response = streaming_summary.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                            response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                            
                    except Exception as e:
                        import traceback
                        error_details = traceback.format_exc()
                        persona_analyses[persona_key] = {
                            'result': f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}\nìƒì„¸: {error_details}",
                            'success': False,
                            'completed': True,
                            'timestamp': datetime.now().isoformat()
                        }
                        st.session_state.completed_finance_personas.add(persona_key)
                        completed_count += 1
                        
                        # ì˜¤ë¥˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                        persona_info = FINANCE_PERSONAS[persona_key]
                        persona_status[persona_key].markdown(f"""
                        <div style="background: #f8d7da; border: 2px solid #dc3545; border-radius: 12px; padding: 15px; margin: 10px 0; text-align: center;">
                            <h4 style="color: #721c24; margin: 0;">âŒ {persona_info['emoji']} {persona_info['name']} ë¶„ì„ ì˜¤ë¥˜</h4>
                            <p style="margin: 5px 0; font-size: 0.9rem; color: #721c24;">
                                ì˜¤ë¥˜ ì‹œê°„: {datetime.fromisoformat(persona_analyses[persona_key]['timestamp']).strftime('%H:%M:%S')}
                            </p>
                        </div>
                        """, unsafe_allow_html=True)
            
            # ë¶„ì„ ì™„ë£Œ ìƒíƒœ ì„¤ì •
            st.session_state.finance_analysis_complete = True
            
            # 4. ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (RAG + ì‹œì¥ ì¡°ì‚¬ ê¸°ë°˜)
            st.markdown("---")
            st.markdown("### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ ì‘ì„±")
            
            final_report = synthesize_finance_analysis(
                user_input, persona_analyses, rag_context, market_research, selected_model
            )
            
            # ìµœì¢… ë³´ê³ ì„œë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¶”ê°€
            final_section = f"\n### ğŸ“‹ ìµœì¢… ì¢…í•© ë³´ê³ ì„œ\n{final_report if final_report else 'ìµœì¢… ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨'}"
            streaming_summary += final_section
            
            # ìµœì¢… ê²°ê³¼ í‘œì‹œ
            formatted_response = streaming_summary.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
            response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
            
            # ì™„ë£Œ ë©”ì‹œì§€
            overall_status.success("ğŸ‰ ëª¨ë“  ì¬ë¬´ ì „ë¬¸ê°€ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ë¶„ì„ ê¸°ë°˜ ì •ë³´ ìš”ì•½
            if rag_context or market_research:
                st.markdown("---")
                st.markdown("### ğŸ“Š ë¶„ì„ ê¸°ë°˜ ì •ë³´ ìš”ì•½")
                
                col1, col2 = st.columns(2)
                with col1:
                    if rag_context:
                        st.markdown("**ğŸ“ RAG íŒŒì¼ ì •ë³´ í™œìš©:**")
                        st.markdown(f"- {len(relevant_docs)}ê°œ ê´€ë ¨ ë¬¸ì„œ")
                        for i, doc in enumerate(relevant_docs[:3], 1):
                            title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                            st.markdown(f"  - {title}")
                        if len(relevant_docs) > 3:
                            st.markdown(f"  - ... ì™¸ {len(relevant_docs)-3}ê°œ")
                        
                        # RAG íŒŒì¼ ìƒì„¸ ì •ë³´
                        with st.expander("ğŸ“„ RAG íŒŒì¼ ìƒì„¸ ì •ë³´"):
                            for i, doc in enumerate(relevant_docs, 1):
                                title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
                                content_preview = doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content
                                st.markdown(f"**{i}. {title}**")
                                st.markdown(f"```\n{content_preview}\n```")
                
                with col2:
                    if market_research:
                        st.markdown("**ğŸŒ Perplexity ì‹œì¥ ì¡°ì‚¬ í™œìš©:**")
                        st.markdown(f"- ì¡°ì‚¬ í‚¤ì›Œë“œ: {market_query}")
                        st.markdown(f"- ì‹œì¥ ë™í–¥ ë° ê²½ìŸì‚¬ ë¶„ì„")
                        st.markdown(f"- ìµœì‹  ì •ë³´ ë°˜ì˜")
                        
                        # ì‹œì¥ ì¡°ì‚¬ ìƒì„¸ ì •ë³´
                        with st.expander("ğŸŒ ì‹œì¥ ì¡°ì‚¬ ìƒì„¸ ì •ë³´"):
                            st.markdown("**ì¡°ì‚¬ ê²°ê³¼ ìš”ì•½:**")
                            market_preview = market_research[:500] + "..." if len(market_research) > 500 else market_research
                            st.markdown(f"```\n{market_preview}\n```")
                
                st.success("âœ… RAG íŒŒì¼ ì •ë³´ì™€ Perplexity ì‹œì¥ ì¡°ì‚¬ë¥¼ ì¢…í•©í•˜ì—¬ ì „ë¬¸ê°€ ë¶„ì„ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")
            
            st.session_state.conversations['ì¬ë¬´ ì •ë³´-ê³ ê¸‰'].append({
                'role': 'ì±—ë´‡', 
                'content': streaming_summary, 
                'timestamp': datetime.now(), 
                'provider': selected_provider, 
                'model': selected_model, 
                'relevant_docs': relevant_docs,
                'persona_analyses': persona_analyses,
                'market_research': market_research,
                'rag_context': rag_context
            })
        else:
            # ì¼ë°˜ RAG ì±—ë´‡ ì‘ë‹µ (ì¼ë°˜ ì±„íŒ… ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°) - RAG ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
            response_container = st.empty()
            full_response = ""
            relevant_docs = None
            with st.spinner("ë‹µë³€ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."):
                try:
                    # ìƒˆë¡œìš´ RAG ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ ì‚¬ìš©
                    streaming_generator = st.session_state.chatbot.generate_streaming_response(
                        user_input, selected_provider, selected_model, temperature
                    )
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                    for response_chunk, docs, error in streaming_generator:
                        if error:
                            st.error(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {error}")
                            break
                        
                        full_response = response_chunk
                        relevant_docs = docs
                        
                        # ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ë§ˆí¬ë‹¤ìš´ í¬ë§·íŒ… ì ìš©)
                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                        
                except Exception as e:
                    st.error(f"RAG ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë°˜ RAG ì‘ë‹µìœ¼ë¡œ í´ë°±
                    try:
                        response, relevant_docs = st.session_state.chatbot.generate_response(
                            user_input, selected_provider, selected_model, temperature
                        )
                        full_response = response
                        formatted_response = full_response.replace('\n', '<br>').replace('**', '<strong>').replace('*', '<em>')
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {formatted_response}</div>", unsafe_allow_html=True)
                    except Exception as fallback_error:
                        st.error(f"í´ë°± ì‘ë‹µë„ ì‹¤íŒ¨: {str(fallback_error)}")
                        full_response = "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                        response_container.markdown(f"<div class='bot-bubble'>ğŸ’¬ {full_response}</div>", unsafe_allow_html=True)
                
            # ìµœì¢… ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì €ì¥
            st.session_state.conversations['ì¼ë°˜'].append({
                'role': 'ì±—ë´‡', 
                'content': full_response, 
                'timestamp': datetime.now(), 
                'provider': selected_provider, 
                'model': selected_model,
                'relevant_docs': relevant_docs
            })
                
        
        # ì…ë ¥ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ í‚¤ ë³€ê²½
        if 'chat_input_key' not in st.session_state:
            st.session_state['chat_input_key'] = 0
        st.session_state['chat_input_key'] += 1
        st.rerun()
