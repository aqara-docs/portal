import streamlit as st
import mysql.connector
import pandas as pd
import plotly.express as px
from datetime import datetime
import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOllama
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import (
    UnstructuredMarkdownLoader,
    UnstructuredPDFLoader,
    UnstructuredFileLoader,
    DirectoryLoader
)
import tempfile
import requests
import json
from openai import OpenAI
import anthropic
from langchain_openai import OpenAI as LangOpenAI
from langchain_anthropic import ChatAnthropic

load_dotenv()

# Set page configuration
st.set_page_config(page_title="Vote ê²°ê³¼", page_icon="ğŸ“Š", layout="wide")

# Page header
st.title("ğŸ—³ï¸ íˆ¬í‘œ ê²°ê³¼")

# ì¸ì¦ ê¸°ëŠ¥ (ê°„ë‹¨í•œ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸)
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.')
    st.stop()

if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:  # ë¹„ë°€ë²ˆí˜¸ê°€ ì…ë ¥ëœ ê²½ìš°ì—ë§Œ ì˜¤ë¥˜ ë©”ì‹œì§€ í‘œì‹œ
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()


# MySQL ì—°ê²° ì„¤ì •
def connect_to_db():
    return mysql.connector.connect(
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        host=os.getenv('SQL_HOST'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4',
        collation='utf8mb4_unicode_ci'
    )

def get_all_questions():
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT q.*, 
                   COUNT(DISTINCT r.response_id) as total_votes,
                   COUNT(DISTINCT r.voter_name) as unique_voters
            FROM vote_questions q
            LEFT JOIN vote_responses r ON q.question_id = r.question_id
            GROUP BY q.question_id
            ORDER BY q.created_at DESC
        """)
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_question_results(question_id):
    """íˆ¬í‘œ ê²°ê³¼ì™€ íˆ¬í‘œì ëª©ë¡ ì¡°íšŒ"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        # ì „ì²´ íˆ¬í‘œ ìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT COUNT(DISTINCT r.response_id) as total_votes
            FROM vote_responses r
            WHERE question_id = %s
        """, (question_id,))
        total_votes = cursor.fetchone()['total_votes']
        
        # ì˜µì…˜ë³„ ê²°ê³¼ ì¡°íšŒ
        cursor.execute("""
            SELECT 
                o.option_id,
                o.option_text,
                COUNT(DISTINCT r.response_id) as vote_count,
                COALESCE(
                    ROUND(COUNT(DISTINCT r.response_id) * 100.0 / NULLIF(%s, 0), 1),
                    0.0
                ) as vote_percentage,
                GROUP_CONCAT(DISTINCT r.reasoning SEPARATOR '\n') as reasonings
            FROM vote_options o
            LEFT JOIN vote_responses r ON o.option_id = r.option_id
            WHERE o.question_id = %s
            GROUP BY o.option_id, o.option_text
            ORDER BY vote_count DESC
        """, (total_votes, question_id))
        results = cursor.fetchall()
        
        # íˆ¬í‘œì ëª©ë¡ ì¡°íšŒ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
        cursor.execute("""
            SELECT DISTINCT 
                r.voter_name,
                COALESCE(uc.credibility_score, 1.0) as credibility_score
            FROM vote_responses r
            LEFT JOIN dot_user_credibility uc 
                ON r.voter_name COLLATE utf8mb4_unicode_ci = uc.user_name COLLATE utf8mb4_unicode_ci
            WHERE r.question_id = %s AND r.voter_name IS NOT NULL
            ORDER BY r.voter_name
        """, (question_id,))
        voters = cursor.fetchall()
        
        return results, voters
    finally:
        cursor.close()
        conn.close()

def ai_vote_llm(question, options, model_name, context=None):
    """LLM íˆ¬í‘œ (OpenAI/Anthropic/Ollama ìë™ ì„ íƒ)"""
    prompt = f"""
ë‹¹ì‹ ì€ íˆ¬í‘œ ì‹œìŠ¤í…œì˜ ì°¸ì—¬ìì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ì‹ ì¤‘íˆ ë¶„ì„í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ì„ íƒì§€:
{options}
"""
    if context:
        prompt += f"\n[ì°¸ê³  ë¬¸ë§¥]\n{context}\n"
    prompt += """
ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”:
{
  "selection": <ì„ íƒí•œ ë²ˆí˜¸>,
  "reasoning": "<ì„ íƒí•œ ì´ìœ ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…>",
  "reference": "<ì°¸ê³ í•œ ë¬¸ë§¥ ë‚´ìš© ìš”ì•½ ë˜ëŠ” 'ë¬¸ë§¥ ì—†ìŒ'>"
}
"""
    if model_name.startswith('gpt'):
        client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=800,
            temperature=0.1
        )
        return response.choices[0].message.content
    elif model_name.startswith('claude'):
        client = ChatAnthropic(model=model_name, api_key=os.getenv('ANTHROPIC_API_KEY'), temperature=0.1, max_tokens=800)
        response = client.invoke([
            {"role": "user", "content": prompt}
        ])
        return response.content if hasattr(response, 'content') else str(response)
    else:
        # Ollama fallback
        return ask_llm(question, options, model_name, context or "")

def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ëª¨ë¸ ëª©ë¡ ë°˜í™˜ (OpenAI/Anthropic/Ollama)"""
    models = []
    has_anthropic_key = os.environ.get('ANTHROPIC_API_KEY') is not None
    if has_anthropic_key:
        models.extend([
            'claude-3-7-sonnet-latest',
            'claude-3-5-sonnet-latest',
            'claude-3-5-haiku-latest',
        ])
    has_openai_key = os.environ.get('OPENAI_API_KEY') is not None
    if has_openai_key:
        models.extend(['gpt-4o', 'gpt-4o-mini'])
    # Ollama ê¸°ë³¸ ëª¨ë¸
    models.extend([
        "deepseek-r1:70b",
        "deepseek-r1:32b",
        "deepseek-r1:14b",
        "phi4:latest",
        "gemma2:latest",
        "llama3.1:latest",
        "mistral:latest",
        "llama2:latest",
        "llama3.2:latest"
    ])
    # gpt-4o-miniê°€ ìˆìœ¼ë©´ ë””í´íŠ¸, ì—†ìœ¼ë©´ ì²« ë²ˆì§¸
    default_model = 'gpt-4o-mini' if 'gpt-4o-mini' in models else (models[0] if models else None)
    return models, default_model

def get_llm_vote(question_id, model_name):
    """LLMì˜ íˆ¬í‘œ ê²°ê³¼ì™€ ì´ìœ  ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    result = None
    
    try:
        cursor.execute("""
            SELECT o.option_text, lr.reasoning
            FROM vote_llm_responses lr
            JOIN vote_options o ON lr.option_id = o.option_id
            WHERE lr.question_id = %s AND lr.llm_model = %s
            ORDER BY lr.voted_at DESC
            LIMIT 1
        """, (question_id, model_name))
        
        # ê²°ê³¼ë¥¼ ì™„ì „íˆ ì½ì–´ì˜´
        result = cursor.fetchone()
        
    except mysql.connector.Error as err:
        st.error(f"LLM íˆ¬í‘œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
    finally:
        # ì»¤ì„œì™€ ì—°ê²° ì •ë¦¬
        cursor.close()
        conn.close()
    
    return result

def load_single_file(file_path):
    """ë‹¨ì¼ íŒŒì¼ ë¡œë“œ"""
    try:
        if file_path.endswith('.md'):
            loader = UnstructuredMarkdownLoader(file_path)
        elif file_path.endswith('.pdf'):
            loader = UnstructuredPDFLoader(file_path)
        else:
            loader = UnstructuredFileLoader(file_path)
        
        return loader.load()
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def load_files(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    documents = []
    for file in files:
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file.flush()
                
                # íŒŒì¼ ë¡œë“œ
                docs = load_single_file(tmp_file.name)
                documents.extend(docs)
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file.name)
        except Exception as e:
            st.error(f"'{file.name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return documents

def create_vectorstore(documents):
    """ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    try:
        # í…ìŠ¤íŠ¸ ë¶„í• 
        text_splitter = CharacterTextSplitter(
            separator="\n",
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len
        )
        
        # ë¬¸ì„œ ë¶„í• 
        splits = text_splitter.split_documents(documents)
        
        # ì„ë² ë”© ìƒì„± (ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©)
        embeddings = OllamaEmbeddings(
            model="llama2"
        )
        
        # ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        from langchain.vectorstores import Chroma
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        with tempfile.TemporaryDirectory() as temp_dir:
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory=temp_dir  # ì„ì‹œ ë””ë ‰í† ë¦¬ ì§€ì •
            )
            
            return vectorstore
            
    except Exception as e:
        st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if "404" in str(e):
            st.info("í•„ìš”í•œ ëª¨ë¸ì„ ì„¤ì¹˜í•˜ë ¤ë©´ í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:\n```bash\nollama pull llama2\n```")
        return None

def get_relevant_context(vectorstore, question, options):
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë§¥ ê²€ìƒ‰"""
    if not vectorstore:
        return ""
        
    # ì§ˆë¬¸ê³¼ ëª¨ë“  ì„ íƒì§€ë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
    search_text = f"{question}\n{options}"
    
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(search_text, k=3)
    
    # ë¬¸ë§¥ ê²°í•©
    context = "\n\n".join([doc.page_content for doc in docs])
    return context

def ask_llm(question, options, model_name, context=""):
    """LLMì—ê²Œ íˆ¬í‘œ ìš”ì²­í•˜ê³  ì‘ë‹µ ë°›ê¸°"""
    try:
        # Ollama API ì§ì ‘ í˜¸ì¶œ
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        if context:
            prompt = f"""
            ë‹¹ì‹ ì€ íˆ¬í‘œ ì‹œìŠ¤í…œì˜ ì°¸ì—¬ìì…ë‹ˆë‹¤. 
            ì•„ë˜ ì œê³µëœ ë¬¸ë§¥, ì§ˆë¬¸, ì„ íƒì§€ë¥¼ ì‹ ì¤‘íˆ ë¶„ì„í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•´ì£¼ì„¸ìš”.
            
            ì°¸ê³ í•  ë¬¸ë§¥:
            {context}
            
            ì§ˆë¬¸: {question}
            
            ì„ íƒì§€:
            {options}
            """
        else:
            prompt = f"""
            ë‹¹ì‹ ì€ íˆ¬í‘œ ì‹œìŠ¤í…œì˜ ì°¸ì—¬ìì…ë‹ˆë‹¤. 
            ì•„ë˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ì‹ ì¤‘íˆ ë¶„ì„í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•´ì£¼ì„¸ìš”.
            
            ì§ˆë¬¸: {question}
            
            ì„ íƒì§€:
            {options}
            """
        
        prompt += """
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:
        {
            "selection": <ì„ íƒí•œ ë²ˆí˜¸>,
            "reasoning": "<ì„ íƒí•œ ì´ìœ ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…>",
            "reference": "<ì°¸ê³ í•œ ë¬¸ë§¥ ë‚´ìš© ìš”ì•½ ë˜ëŠ” 'ë¬¸ë§¥ ì—†ìŒ'>"
        }
        """
        
        # Ollama API í˜¸ì¶œ
        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model_name,
                "prompt": prompt,
                "stream": False,
                "format": "json",
                "temperature": 0.1
            }
        )
        
        response.raise_for_status()
        result = response.json()
        
        return result.get('response', '')
        
    except Exception as e:
        st.error(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None

def parse_llm_response(response_text):
    """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì„ íƒ ë²ˆí˜¸ì™€ ì´ìœ  ì¶”ì¶œ"""
    try:
        # JSON í˜•ì‹ ì°¾ê¸°
        import re
        import json
        
        # JSON í˜•ì‹ ì°¾ê¸° ì‹œë„
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                response_json = json.loads(json_match.group())
                selection = int(response_json['selection'])
                reasoning = response_json['reasoning']
                return selection, reasoning
            except (json.JSONDecodeError, KeyError, ValueError):
                pass
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
        lines = [line.strip() for line in response_text.split('\n') if line.strip()]
        selection = None
        reasoning = []
        parsing_reason = False
        
        for line in lines:
            if 'ì„ íƒ' in line.lower() or 'selection' in line.lower():
                numbers = [int(s) for s in line.split() if s.isdigit()]
                if numbers:
                    selection = numbers[0]
                    continue
            
            if 'ì´ìœ ' in line.lower() or 'reasoning' in line.lower() or parsing_reason:
                parsing_reason = True
                current_reason = line.replace('ì´ìœ :', '').replace('reasoning:', '').strip()
                if current_reason:
                    reasoning.append(current_reason)
        
        if selection is None:
            raise ValueError("ì„ íƒ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        reasoning_text = ' '.join(reasoning) if reasoning else "ì´ìœ ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return selection, reasoning_text
        
    except Exception as e:
        raise ValueError(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸ ì‘ë‹µ: {response_text}")

def save_llm_vote(question_id, option_id, model_name, reasoning, weight):
    """LLMì˜ íˆ¬í‘œ ê²°ê³¼ë¥¼ DBì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€)"""
    conn = connect_to_db()
    cursor = conn.cursor()
    try:
        # ê¸°ì¡´ LLM íˆ¬í‘œ ì‚­ì œ
        cursor.execute(
            "DELETE FROM vote_llm_responses WHERE question_id = %s AND llm_model = %s",
            (question_id, model_name)
        )
        # ìƒˆ íˆ¬í‘œ ì €ì¥
        cursor.execute("""
            INSERT INTO vote_llm_responses 
            (question_id, option_id, llm_model, reasoning, weight)
            VALUES (%s, %s, %s, %s, %s)
        """, (question_id, option_id, model_name, reasoning, weight))
        conn.commit()
    finally:
        cursor.close()
        conn.close()

def get_combined_results(question_id, apply_weights=False):
    """ì¼ë°˜ íˆ¬í‘œì™€ LLM íˆ¬í‘œ ê²°ê³¼ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        if apply_weights:
            # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì¿¼ë¦¬
            cursor.execute("""
                WITH vote_data AS (
                    SELECT 
                        r.option_id,
                        COUNT(DISTINCT r.response_id) as vote_count,
                        SUM(COALESCE(uc.credibility_score, 1.0)) as total_credibility
                    FROM vote_responses r
                    LEFT JOIN dot_user_credibility uc 
                        ON r.voter_name COLLATE utf8mb4_unicode_ci = uc.user_name COLLATE utf8mb4_unicode_ci
                    WHERE r.question_id = %s
                    GROUP BY r.option_id
                ),
                llm_data AS (
                    SELECT 
                        option_id,
                        SUM(weight) as total_weight
                    FROM vote_llm_responses
                    WHERE question_id = %s
                    GROUP BY option_id
                )
                SELECT 
                    o.option_text,
                    COALESCE(v.vote_count, 0) as raw_human_votes,
                    CAST(COALESCE(v.total_credibility, 0) AS SIGNED) as human_votes,
                    CAST(COALESCE(l.total_weight, 0) AS SIGNED) as weighted_llm_votes,
                    CAST(
                        COALESCE(v.total_credibility, 0) + COALESCE(l.total_weight, 0)
                        AS SIGNED
                    ) as total_votes
                FROM vote_options o
                LEFT JOIN vote_data v ON o.option_id = v.option_id
                LEFT JOIN llm_data l ON o.option_id = l.option_id
                WHERE o.question_id = %s
                ORDER BY total_votes DESC
            """, (question_id, question_id, question_id))
        else:
            cursor.execute("""
                WITH vote_data AS (
                    SELECT 
                        option_id,
                        COUNT(DISTINCT response_id) as vote_count
                    FROM vote_responses
                    WHERE question_id = %s
                    GROUP BY option_id
                ),
                llm_data AS (
                    SELECT 
                        option_id,
                        SUM(weight) as total_weight
                    FROM vote_llm_responses
                    WHERE question_id = %s
                    GROUP BY option_id
                )
                SELECT 
                    o.option_text,
                    COALESCE(v.vote_count, 0) as human_votes,
                    CAST(COALESCE(l.total_weight, 0) AS SIGNED) as weighted_llm_votes,
                    CAST(
                        COALESCE(v.vote_count, 0) + COALESCE(l.total_weight, 0)
                        AS SIGNED
                    ) as total_votes
                FROM vote_options o
                LEFT JOIN vote_data v ON o.option_id = v.option_id
                LEFT JOIN llm_data l ON o.option_id = l.option_id
                WHERE o.question_id = %s
                ORDER BY total_votes DESC
            """, (question_id, question_id, question_id))
        
        results = cursor.fetchall()
        return results
    finally:
        cursor.close()
        conn.close()

def get_question_options(question_id):
    """ì§ˆë¬¸ì— ëŒ€í•œ ì„ íƒì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT option_id, option_text
            FROM vote_options
            WHERE question_id = %s
            ORDER BY option_id
        """, (question_id,))
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_vote_results():
    """íˆ¬í‘œ ê²°ê³¼ ì¡°íšŒ"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT 
                v.option_id,
                o.option_text,
                COUNT(v.vote_id) as vote_count,
                COALESCE(
                    (COUNT(v.vote_id) * 100.0 / 
                    NULLIF((SELECT COUNT(*) FROM votes), 0)), 
                    0
                ) as vote_percentage
            FROM vote_options o
            LEFT JOIN votes v ON o.option_id = v.option_id
            GROUP BY o.option_id, o.option_text
            ORDER BY vote_count DESC
        """)
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_subjective_question_results(question_id):
    """ì£¼ê´€ì‹ ì§ˆë¬¸ì˜ ê²°ê³¼ì™€ ì‘ë‹µì ëª©ë¡ ì¡°íšŒ"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        # ì „ì²´ ì‘ë‹µ ìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT COUNT(DISTINCT r.response_id) as total_responses
            FROM subjective_responses r
            WHERE question_id = %s
        """, (question_id,))
        total_responses = cursor.fetchone()['total_responses']
        
        # ì‘ë‹µë³„ ê²°ê³¼ ì¡°íšŒ (ë™ì¼ ì‘ë‹µ ê·¸ë£¹í™”)
        cursor.execute("""
            SELECT 
                response_text,
                COUNT(*) as response_count,
                COALESCE(
                    ROUND(COUNT(*) * 100.0 / NULLIF(%s, 0), 1),
                    0.0
                ) as response_percentage,
                GROUP_CONCAT(DISTINCT voter_name ORDER BY voter_name SEPARATOR ', ') as voters
            FROM subjective_responses
            WHERE question_id = %s
            GROUP BY response_text
            ORDER BY response_count DESC, response_text
        """, (total_responses, question_id))
        results = cursor.fetchall()
        
        # ì‘ë‹µì ëª©ë¡ ì¡°íšŒ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
        cursor.execute("""
            SELECT DISTINCT 
                r.voter_name,
                COALESCE(uc.credibility_score, 1.0) as credibility_score
            FROM subjective_responses r
            LEFT JOIN dot_user_credibility uc 
                ON r.voter_name COLLATE utf8mb4_unicode_ci = uc.user_name COLLATE utf8mb4_unicode_ci
            WHERE r.question_id = %s AND r.voter_name IS NOT NULL AND r.voter_name != 'ìµëª…'
            ORDER BY r.voter_name
        """, (question_id,))
        voters = cursor.fetchall()
        
        return results, voters, total_responses
    finally:
        cursor.close()
        conn.close()

def get_subjective_llm_vote(question_id, model_name):
    """LLMì˜ ì£¼ê´€ì‹ ë‹µë³€ ê²°ê³¼ì™€ ì´ìœ  ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    result = None
    try:
        # Ensure parameters are scalar, not list
        if isinstance(question_id, list):
            question_id = question_id[0]
        if isinstance(model_name, list):
            model_name = model_name[0]
        cursor.execute("""
            SELECT response_text, reasoning
            FROM subjective_llm_responses
            WHERE question_id = %s AND llm_model = %s
            ORDER BY voted_at DESC
            LIMIT 1
        """, (question_id, model_name))
        result = cursor.fetchone()
    except mysql.connector.Error as err:
        st.error(f"LLM ë‹µë³€ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
    finally:
        cursor.close()
        conn.close()
    return result

def save_subjective_llm_response(question_id, model_name, response_text, reasoning, weight):
    """LLMì˜ ì£¼ê´€ì‹ ë‹µë³€ì„ DBì— ì €ì¥"""
    conn = connect_to_db()
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            INSERT INTO subjective_llm_responses 
            (question_id, llm_model, response_text, reasoning, weight)
            VALUES (%s, %s, %s, %s, %s)
        """, (question_id, model_name, response_text, reasoning, weight))
        conn.commit()
    finally:
        cursor.close()
        conn.close()

def get_combined_subjective_results(question_id, apply_weights=False):
    """ì¼ë°˜ ë‹µë³€ê³¼ LLM ë‹µë³€ ê²°ê³¼ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        if apply_weights:
            # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì¿¼ë¦¬
            cursor.execute("""
                WITH response_data AS (
                    SELECT 
                        response_text,
                        COUNT(*) as response_count,
                        SUM(COALESCE(uc.credibility_score, 1.0)) as total_credibility
                    FROM subjective_responses r
                    LEFT JOIN dot_user_credibility uc 
                        ON r.voter_name COLLATE utf8mb4_unicode_ci = uc.user_name COLLATE utf8mb4_unicode_ci
                    WHERE r.question_id = %s
                    GROUP BY response_text
                ),
                llm_data AS (
                    SELECT 
                        response_text,
                        SUM(weight) as total_weight
                    FROM subjective_llm_responses
                    WHERE question_id = %s
                    GROUP BY response_text
                )
                SELECT 
                    r.response_text,
                    r.response_count as raw_human_responses,
                    CAST(COALESCE(r.total_credibility, 0) AS SIGNED) as human_responses,
                    CAST(COALESCE(l.total_weight, 0) AS SIGNED) as weighted_llm_responses,
                    CAST(
                        COALESCE(r.total_credibility, 0) + COALESCE(l.total_weight, 0)
                        AS SIGNED
                    ) as total_responses
                FROM response_data r
                LEFT JOIN llm_data l ON r.response_text = l.response_text
                UNION
                SELECT 
                    l.response_text,
                    0 as raw_human_responses,
                    0 as human_responses,
                    CAST(l.total_weight AS SIGNED) as weighted_llm_responses,
                    CAST(l.total_weight AS SIGNED) as total_responses
                FROM llm_data l
                LEFT JOIN response_data r ON l.response_text = r.response_text
                WHERE r.response_text IS NULL
                ORDER BY total_responses DESC
            """, (question_id, question_id))
        else:
            cursor.execute("""
                WITH response_data AS (
                    SELECT 
                        response_text,
                        COUNT(*) as response_count
                    FROM subjective_responses
                    WHERE question_id = %s
                    GROUP BY response_text
                ),
                llm_data AS (
                    SELECT 
                        response_text,
                        SUM(weight) as total_weight
                    FROM subjective_llm_responses
                    WHERE question_id = %s
                    GROUP BY response_text
                )
                SELECT 
                    r.response_text,
                    COALESCE(r.response_count, 0) as human_responses,
                    CAST(COALESCE(l.total_weight, 0) AS SIGNED) as weighted_llm_responses,
                    CAST(
                        COALESCE(r.response_count, 0) + COALESCE(l.total_weight, 0)
                        AS SIGNED
                    ) as total_responses
                FROM response_data r
                LEFT JOIN llm_data l ON r.response_text = l.response_text
                UNION
                SELECT 
                    l.response_text,
                    0 as human_responses,
                    CAST(l.total_weight AS SIGNED) as weighted_llm_responses,
                    CAST(l.total_weight AS SIGNED) as total_responses
                FROM llm_data l
                LEFT JOIN response_data r ON l.response_text = r.response_text
                WHERE r.response_text IS NULL
                ORDER BY total_responses DESC
            """, (question_id, question_id))
        
        results = cursor.fetchall()
        return results
    finally:
        cursor.close()
        conn.close()

def main():
    # ëª¨ë“  íˆ¬í‘œ ë¬¸ì œ ê°€ì ¸ì˜¤ê¸°
    questions = get_all_questions()
    
    if not questions:
        st.info("ë“±ë¡ëœ íˆ¬í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ğŸ“Š ê°ê´€ì‹ íˆ¬í‘œ", "âœï¸ ì£¼ê´€ì‹ íˆ¬í‘œ"])
    
    with tab1:
        # ë¬¸ì œ ì„ íƒ
        selected_question = st.selectbox(
            "ê²°ê³¼ë¥¼ ë³¼ íˆ¬í‘œë¥¼ ì„ íƒí•˜ì„¸ìš”",
            questions,
            format_func=lambda x: f"{x['title']} ({x['created_at'].strftime('%Y-%m-%d %H:%M')})"
        )
        
        if selected_question:
            st.write("---")
            st.write(f"## {selected_question['title']}")
            st.write(selected_question['description'])
            
            # íˆ¬í‘œ ìƒíƒœ í‘œì‹œ
            status_color = "ğŸŸ¢" if selected_question['status'] == 'active' else "ğŸ”´"
            st.write(f"ìƒíƒœ: {status_color} {selected_question['status'].upper()}")
            
            # ê¸°ë³¸ í†µê³„
            st.write(f"ì´ íˆ¬í‘œ ìˆ˜: {selected_question['total_votes']}")
            st.write(f"ì°¸ì—¬ì ìˆ˜: {selected_question['unique_voters']}")
            
            # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            results, voters = get_question_results(selected_question['question_id'])
            
            if results:
                # Create DataFrame with correct column names
                df_results = pd.DataFrame(results).astype({
                    'vote_count': 'int64',
                    'vote_percentage': 'float64'
                })
                
                # ì°¨íŠ¸ ê·¸ë¦¬ê¸°
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.write("### íˆ¬í‘œ ê²°ê³¼ ì°¨íŠ¸")
                    fig = px.bar(
                        df_results,
                        x='option_text',
                        y='vote_count',
                        text='vote_count',
                        title="ì„ íƒì§€ë³„ íˆ¬í‘œ ìˆ˜",
                        labels={'option_text': 'ì„ íƒì§€', 'vote_count': 'íˆ¬í‘œ ìˆ˜'}
                    )
                    fig.update_traces(textposition='outside')
                    st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.write("### ìƒì„¸ ê²°ê³¼")
                    for result in results:
                        st.write(f"#### {result['option_text']}")
                        
                        # ì•ˆì „í•œ ê°’ ì¶”ì¶œ
                        vote_count = result.get('vote_count', 0) or 0
                        vote_percentage = result.get('vote_percentage', 0.0) or 0.0
                        
                        # ê²°ê³¼ í‘œì‹œ
                        st.write(f"íˆ¬í‘œ ìˆ˜: {vote_count} ({vote_percentage:.1f}%)")
                        
                        # ì„ íƒ ì´ìœ  í‘œì‹œ
                        if result.get('reasonings'):
                            with st.expander("ğŸ’¬ ì„ íƒ ì´ìœ  ë³´ê¸°"):
                                reasonings = result['reasonings'].split('\n')
                                for reasoning in reasonings:
                                    if reasoning.strip():
                                        st.markdown(f"- {reasoning}")
                
                # íˆ¬í‘œì ëª©ë¡ (ìµëª… ì œì™¸)
                if voters:
                    with st.expander("íˆ¬í‘œì ëª©ë¡ ë³´ê¸° (ìµëª… ì œì™¸)"):
                        for voter in voters:
                            st.write(f"- {voter['voter_name']} (ì‹ ë¢°ë„: {voter['credibility_score']:.2f})")
                
                # ê´€ë¦¬ì ê¸°ëŠ¥
                if selected_question['status'] == 'active':
                    if st.button("íˆ¬í‘œ ì¢…ë£Œí•˜ê¸°"):
                        conn = connect_to_db()
                        cursor = conn.cursor()
                        try:
                            cursor.execute("""
                                UPDATE vote_questions
                                SET status = 'closed'
                                WHERE question_id = %s
                            """, (selected_question['question_id'],))
                            conn.commit()
                            st.success("íˆ¬í‘œê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                        except mysql.connector.Error as err:
                            st.error(f"íˆ¬í‘œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {err}")
                        finally:
                            cursor.close()
                            conn.close()

            # LLM íˆ¬í‘œ ì„¹ì…˜
            st.write("---")
            st.write("## ğŸ¤– LLM íˆ¬í‘œ")
            
            models, default_model = get_available_models()
            if 'selected_model' not in st.session_state:
                st.session_state.selected_model = default_model
            selected_model = st.selectbox(
                "LLM ëª¨ë¸ ì„ íƒ",
                models,
                index=models.index(st.session_state.selected_model) if st.session_state.selected_model in models else 0
            )
            st.session_state.selected_model = selected_model
            
            # LLM íˆ¬í‘œ ê°€ì¤‘ì¹˜ ì„¤ì •
            llm_weight = st.slider(
                "LLM íˆ¬í‘œ ê°€ì¤‘ì¹˜",
                min_value=1,
                max_value=10,
                value=1,
                help="LLMì˜ íˆ¬í‘œê°€ ëª‡ ëª…ì˜ íˆ¬í‘œì™€ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§ˆì§€ ì„¤ì •í•©ë‹ˆë‹¤."
            )
            
            # RAG ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
            use_rag = st.checkbox("ë¬¸ì„œ ì°¸ì¡° ì‚¬ìš© (RAG)", 
                                help="ì„ íƒí•œ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.")
            
            if use_rag:
                # íŒŒì¼ ì…ë ¥ ë°©ì‹ ì„ íƒ
                input_method = st.radio(
                    "ì°¸ì¡° ë¬¸ì„œ ì…ë ¥ ë°©ì‹",
                    ["íŒŒì¼ ì—…ë¡œë“œ", "ë””ë ‰í† ë¦¬ ê²½ë¡œ"]
                )
                
                context = ""
                if input_method == "íŒŒì¼ ì—…ë¡œë“œ":
                    uploaded_files = st.file_uploader(
                        "ì°¸ì¡°í•  íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)",
                        accept_multiple_files=True,
                        type=['txt', 'md', 'pdf']
                    )
                    
                    if uploaded_files:
                        with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
                            documents = load_files(uploaded_files)
                            if documents:
                                vectorstore = create_vectorstore(documents)
                                
                else:  # ë””ë ‰í† ë¦¬ ê²½ë¡œ
                    doc_directory = st.text_input(
                        "ì°¸ì¡°í•  ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ",
                        help="ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸/PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬"
                    )
                    
                    if doc_directory and os.path.exists(doc_directory):
                        with st.spinner("ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘..."):
                            documents = load_documents(doc_directory)
                            if documents:
                                vectorstore = create_vectorstore(documents)
            
            # LLM íˆ¬í‘œ ë²„íŠ¼
            if st.button("LLM íˆ¬í‘œ ì‹¤í–‰"):
                options = get_question_options(selected_question['question_id'])
                options_text = "\n".join([f"{i+1}. {opt['option_text']}" 
                                        for i, opt in enumerate(options)])
                
                context = ""
                if use_rag and 'vectorstore' in locals():
                    with st.spinner("ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
                        context = get_relevant_context(
                            vectorstore,
                            selected_question['description'],
                            options_text
                        )
                        if context:
                            st.write("### ì°¸ì¡°í•œ ë¬¸ë§¥:")
                            st.write(context)
                
                # LLMì—ê²Œ ë¬¼ì–´ë³´ê¸°
                with st.spinner("LLM ì‘ë‹µ ëŒ€ê¸° ì¤‘..."):
                    llm_response = ai_vote_llm(
                        selected_question['description'],
                        options_text,
                        selected_model,
                        context if use_rag else ""
                    )
                
                # ì‘ë‹µ íŒŒì‹± ë° ì €ì¥
                try:
                    selection, reasoning = parse_llm_response(llm_response)
                    
                    if selection < 1 or selection > len(options):
                        st.error(f"LLMì´ ì˜ëª»ëœ ì„ íƒì§€ ë²ˆí˜¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {selection}")
                    else:
                        save_llm_vote(
                            selected_question['question_id'],
                            options[selection - 1]['option_id'],
                            selected_model,
                            reasoning,
                            llm_weight
                        )
                        st.session_state['last_llm_vote'] = {
                            'selection': selection,
                            'reasoning': reasoning,
                            'option_text': options[selection - 1]['option_text'],
                            'model': selected_model,
                            'weight': llm_weight
                        }
                        st.success(f"LLM íˆ¬í‘œê°€ ê°€ì¤‘ì¹˜ {llm_weight}ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:
                    st.error(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # LLM íˆ¬í‘œ ê²°ê³¼ í‘œì‹œ
            if 'last_llm_vote' in st.session_state:
                llm_vote = st.session_state['last_llm_vote']
                st.write("### ğŸ¤– LLM íˆ¬í‘œ ê²°ê³¼")
                st.write(f"**ì„ íƒí•œ í•­ëª©:** {llm_vote['option_text']}")
                st.write("**ì„ íƒ ì´ìœ :**")
                st.write(llm_vote['reasoning'])
            
            # ê²°ê³¼ ë¹„êµ í‘œì‹œ
            st.write("---")
            st.write("## ğŸ“Š í†µí•© ê²°ê³¼ ë¹„êµ")
            
            # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš© ì—¬ë¶€ ì„ íƒ
            apply_weights = st.checkbox("ì°¸ì—¬ì ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©", 
                                      help="ì²´í¬í•˜ë©´ 00_12_ì‹ ë¢°ë„_ê°€ì¤‘ì¹˜_ë¶€ì—¬.pyì—ì„œ ì„¤ì •ëœ ì‹ ë¢°ë„ ì ìˆ˜ê°€ íˆ¬í‘œì— ë°˜ì˜ë©ë‹ˆë‹¤.")
            
            results = get_combined_results(selected_question['question_id'], apply_weights)
            if results:
                # Create DataFrame with correct column names
                df_results = pd.DataFrame(results)
                
                # Convert numeric columns to appropriate types
                numeric_columns = ['human_votes', 'weighted_llm_votes', 'total_votes']
                if apply_weights:
                    numeric_columns.append('raw_human_votes')
                
                for col in numeric_columns:
                    if col in df_results.columns:
                        df_results[col] = pd.to_numeric(df_results[col], errors='coerce').fillna(0).astype('int64')
                
                # ì¸ê°„ íˆ¬í‘œ ì°¨íŠ¸
                fig1 = px.bar(
                    df_results,
                    x='option_text',
                    y='raw_human_votes' if apply_weights else 'human_votes',
                    title=f"ì¸ê°„ íˆ¬í‘œ ê²°ê³¼ {'(ê°€ì¤‘ì¹˜ ì ìš© ì „)' if apply_weights else ''}",
                    labels={'option_text': 'ì„ íƒì§€', 
                           'raw_human_votes': 'íˆ¬í‘œ ìˆ˜', 
                           'human_votes': 'íˆ¬í‘œ ìˆ˜'}
                )
                st.plotly_chart(fig1, use_container_width=True)
                
                if apply_weights:
                    # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì¸ê°„ íˆ¬í‘œ ì°¨íŠ¸
                    fig_weighted = px.bar(
                        df_results,
                        x='option_text',
                        y='human_votes',
                        title="ì¸ê°„ íˆ¬í‘œ ê²°ê³¼ (ê°€ì¤‘ì¹˜ ì ìš© í›„)",
                        labels={'option_text': 'ì„ íƒì§€', 'human_votes': 'ê°€ì¤‘ì¹˜ ì ìš©ëœ íˆ¬í‘œ ìˆ˜'}
                    )
                    st.plotly_chart(fig_weighted, use_container_width=True)
                
                # í†µí•© ê²°ê³¼ ì°¨íŠ¸
                df_melted = pd.melt(
                    df_results,
                    id_vars=['option_text'],
                    value_vars=['human_votes', 'weighted_llm_votes']
                )
                
                fig2 = px.bar(
                    df_melted,
                    x='option_text',
                    y='value',
                    color='variable',
                    title=f"í†µí•© íˆ¬í‘œ ê²°ê³¼ (ì¸ê°„{' (ê°€ì¤‘ì¹˜ ì ìš©)' if apply_weights else ''} + LLM)",
                    labels={
                        'option_text': 'ì„ íƒì§€',
                        'value': 'íˆ¬í‘œ ìˆ˜',
                        'variable': 'íˆ¬í‘œì ìœ í˜•'
                    },
                    barmode='stack'
                )
                
                # ë²”ë¡€ ì´ë¦„ ë³€ê²½
                fig2.update_traces(
                    name=f"ì¸ê°„ íˆ¬í‘œ{' (ê°€ì¤‘ì¹˜ ì ìš©)' if apply_weights else ''}",
                    selector=dict(name="human_votes")
                )
                fig2.update_traces(
                    name="LLM íˆ¬í‘œ (ê°€ì¤‘ì¹˜ ì ìš©)",
                    selector=dict(name="weighted_llm_votes")
                )
                
                st.plotly_chart(fig2, use_container_width=True)

    with tab2:
        st.write("## ì£¼ê´€ì‹ íˆ¬í‘œ ê²°ê³¼")
        
        # ì£¼ê´€ì‹ ì§ˆë¬¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        conn = connect_to_db()
        cursor = conn.cursor(dictionary=True)
        
        try:
            cursor.execute("""
                SELECT q.*, 
                       COUNT(DISTINCT r.response_id) as total_responses,
                       COUNT(DISTINCT r.voter_name) as unique_voters
                FROM subjective_questions q
                LEFT JOIN subjective_responses r ON q.question_id = r.question_id
                GROUP BY q.question_id
                ORDER BY q.created_at DESC
            """)
            subjective_questions = cursor.fetchall()
            
            if not subjective_questions:
                st.info("ë“±ë¡ëœ ì£¼ê´€ì‹ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ì§ˆë¬¸ ì„ íƒ
            selected_question = st.selectbox(
                "ê²°ê³¼ë¥¼ ë³¼ ì§ˆë¬¸ì„ ì„ íƒí•˜ì„¸ìš”",
                subjective_questions,
                format_func=lambda x: f"{x['title']} ({x['created_at'].strftime('%Y-%m-%d %H:%M')})",
                key="subjective_question_selector"
            )
            
            if selected_question:
                st.write("---")
                st.write(f"## {selected_question['title']}")
                st.write(selected_question['description'])
                
                # íˆ¬í‘œ ìƒíƒœ í‘œì‹œ
                status_color = "ğŸŸ¢" if selected_question['status'] == 'active' else "ğŸ”´"
                st.write(f"ìƒíƒœ: {status_color} {selected_question['status'].upper()}")
                
                # ê¸°ë³¸ í†µê³„
                st.write(f"ì´ ì‘ë‹µ ìˆ˜: {selected_question['total_responses']}")
                st.write(f"ì°¸ì—¬ì ìˆ˜: {selected_question['unique_voters']}")
                
                # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
                results, voters, total_responses = get_subjective_question_results(selected_question['question_id'])
                
                if results:
                    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
                    df_results = pd.DataFrame(results)
                    
                    # ì°¨íŠ¸ ê·¸ë¦¬ê¸°
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        st.write("### ì‘ë‹µ ë¶„í¬ ì°¨íŠ¸")
                        # ìƒìœ„ 10ê°œ ì‘ë‹µë§Œ í‘œì‹œ
                        top_responses = df_results.head(10)
                        fig = px.bar(
                            top_responses,
                            x='response_text',
                            y='response_count',
                            text='response_count',
                            title="ì‘ë‹µë³„ ë¹ˆë„ (ìƒìœ„ 10ê°œ)",
                            labels={'response_text': 'ì‘ë‹µ', 'response_count': 'ì‘ë‹µ ìˆ˜'}
                        )
                        fig.update_traces(textposition='outside')
                        fig.update_layout(xaxis_tickangle=-45)
                        st.plotly_chart(fig, use_container_width=True)
                    
                    with col2:
                        st.write("### ìƒì„¸ ê²°ê³¼")
                        for result in results:
                            with st.expander(f"ğŸ“ {result['response_text']} ({result['response_count']}íšŒ)"):
                                st.write(f"ì‘ë‹µ ë¹„ìœ¨: {result['response_percentage']}%")
                                if result['voters'] and result['voters'] != 'ìµëª…':
                                    st.write("ì‘ë‹µì:")
                                    voters_list = result['voters'].split(', ')
                                    for voter in voters_list:
                                        if voter != 'ìµëª…':
                                            st.write(f"- {voter}")
                    
                    # LLM ë‹µë³€ ì„¹ì…˜
                    st.write("---")
                    st.write("## ğŸ¤– LLM ë‹µë³€")
                    
                    col1, col2 = st.columns([1, 2])
                    
                    with col1:
                        selected_model = st.selectbox(
                            "LLM ëª¨ë¸ ì„ íƒ",
                            get_available_models(),
                            key="subjective_llm_model"
                        )
                        
                        # LLM ë‹µë³€ ê°€ì¤‘ì¹˜ ì„¤ì •
                        llm_weight = st.slider(
                            "LLM ë‹µë³€ ê°€ì¤‘ì¹˜",
                            min_value=1,
                            max_value=10,
                            value=1,
                            help="LLMì˜ ë‹µë³€ì´ ëª‡ ëª…ì˜ ë‹µë³€ê³¼ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§ˆì§€ ì„¤ì •í•©ë‹ˆë‹¤.",
                            key="subjective_llm_weight"
                        )
                        
                        # RAG ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
                        use_rag = st.checkbox("ë¬¸ì„œ ì°¸ì¡° ì‚¬ìš© (RAG)", 
                                            help="ì„ íƒí•œ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.",
                                            key="subjective_use_rag")
                        
                        if use_rag:
                            # íŒŒì¼ ì…ë ¥ ë°©ì‹ ì„ íƒ
                            input_method = st.radio(
                                "ì°¸ì¡° ë¬¸ì„œ ì…ë ¥ ë°©ì‹",
                                ["íŒŒì¼ ì—…ë¡œë“œ", "ë””ë ‰í† ë¦¬ ê²½ë¡œ"],
                                key="subjective_input_method"
                            )
                            
                            context = ""
                            if input_method == "íŒŒì¼ ì—…ë¡œë“œ":
                                uploaded_files = st.file_uploader(
                                    "ì°¸ì¡°í•  íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)",
                                    accept_multiple_files=True,
                                    type=['txt', 'md', 'pdf'],
                                    key="subjective_file_uploader"
                                )
                                
                                if uploaded_files:
                                    with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
                                        documents = load_files(uploaded_files)
                                        if documents:
                                            vectorstore = create_vectorstore(documents)
                                            
                            else:  # ë””ë ‰í† ë¦¬ ê²½ë¡œ
                                doc_directory = st.text_input(
                                    "ì°¸ì¡°í•  ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ",
                                    help="ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸/PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬",
                                    key="subjective_doc_directory"
                                )
                                
                                if doc_directory and os.path.exists(doc_directory):
                                    with st.spinner("ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘..."):
                                        documents = load_documents(doc_directory)
                                        if documents:
                                            vectorstore = create_vectorstore(documents)
                        
                        # LLM ë‹µë³€ ë²„íŠ¼
                        if st.button("LLM ë‹µë³€ ìƒì„±", key="subjective_llm_button"):
                            context = ""
                            if use_rag and 'vectorstore' in locals():
                                with st.spinner("ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
                                    context = get_relevant_context(
                                        vectorstore,
                                        selected_question['description'],
                                        ""
                                    )
                                    if context:
                                        st.write("### ì°¸ì¡°í•œ ë¬¸ë§¥:")
                                        st.write(context)
                            
                            # LLMì—ê²Œ ë¬¼ì–´ë³´ê¸°
                            with st.spinner("LLM ì‘ë‹µ ëŒ€ê¸° ì¤‘..."):
                                llm_response = ai_vote_llm(
                                    selected_question['description'],
                                    "",
                                    selected_model,
                                    context if use_rag else ""
                                )
                            
                            # ì‘ë‹µ íŒŒì‹± ë° ì €ì¥
                            try:
                                response_text = llm_response.strip()
                                save_subjective_llm_response(
                                    selected_question['question_id'],
                                    selected_model,
                                    response_text,
                                    context if use_rag else "",
                                    llm_weight
                                )
                                st.success(f"LLM ë‹µë³€ì´ ê°€ì¤‘ì¹˜ {llm_weight}ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.rerun()
                                
                            except Exception as e:
                                st.error(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    
                    with col2:
                        # ê¸°ì¡´ LLM ë‹µë³€ ê²°ê³¼ í‘œì‹œ
                        llm_response = get_subjective_llm_vote(selected_question['question_id'], selected_model)
                        if llm_response:
                            st.write("### ğŸ¤– LLM ë‹µë³€ ê²°ê³¼")
                            st.write(f"**ë‹µë³€:** {llm_response['response_text']}")
                            if llm_response['reasoning']:
                                st.write("**ì°¸ì¡°í•œ ë¬¸ë§¥:**")
                                st.write(llm_response['reasoning'])
                    
                    # ê²°ê³¼ ë¹„êµ í‘œì‹œ
                    st.write("---")
                    st.write("## ğŸ“Š í†µí•© ê²°ê³¼ ë¹„êµ")
                    
                    # ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš© ì—¬ë¶€ ì„ íƒ
                    apply_weights = st.checkbox(
                        "ì°¸ì—¬ì ì‹ ë¢°ë„ ê°€ì¤‘ì¹˜ ì ìš©", 
                        help="ì²´í¬í•˜ë©´ 00_12_ì‹ ë¢°ë„_ê°€ì¤‘ì¹˜_ë¶€ì—¬.pyì—ì„œ ì„¤ì •ëœ ì‹ ë¢°ë„ ì ìˆ˜ê°€ ë‹µë³€ì— ë°˜ì˜ë©ë‹ˆë‹¤.",
                        key="subjective_apply_weights"
                    )
                    
                    combined_results = get_combined_subjective_results(selected_question['question_id'], apply_weights)
                    if combined_results:
                        # Create DataFrame with correct column names
                        df_combined = pd.DataFrame(combined_results)
                        
                        # Convert numeric columns to appropriate types
                        numeric_columns = ['human_responses', 'weighted_llm_responses', 'total_responses']
                        if apply_weights:
                            numeric_columns.append('raw_human_responses')
                        
                        for col in numeric_columns:
                            if col in df_combined.columns:
                                df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce').fillna(0).astype('int64')
                        
                        # ì¸ê°„ ë‹µë³€ ì°¨íŠ¸
                        fig1 = px.bar(
                            df_combined,
                            x='response_text',
                            y='raw_human_responses' if apply_weights else 'human_responses',
                            title=f"ì¸ê°„ ë‹µë³€ ê²°ê³¼ {'(ê°€ì¤‘ì¹˜ ì ìš© ì „)' if apply_weights else ''}",
                            labels={'response_text': 'ë‹µë³€', 
                                   'raw_human_responses': 'ë‹µë³€ ìˆ˜', 
                                   'human_responses': 'ë‹µë³€ ìˆ˜'}
                        )
                        st.plotly_chart(fig1, use_container_width=True)
                        
                        if apply_weights:
                            # ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì¸ê°„ ë‹µë³€ ì°¨íŠ¸
                            fig_weighted = px.bar(
                                df_combined,
                                x='response_text',
                                y='human_responses',
                                title="ì¸ê°„ ë‹µë³€ ê²°ê³¼ (ê°€ì¤‘ì¹˜ ì ìš© í›„)",
                                labels={'response_text': 'ë‹µë³€', 'human_responses': 'ê°€ì¤‘ì¹˜ ì ìš©ëœ ë‹µë³€ ìˆ˜'}
                            )
                            st.plotly_chart(fig_weighted, use_container_width=True)
                        
                        # í†µí•© ê²°ê³¼ ì°¨íŠ¸
                        df_melted = pd.melt(
                            df_combined,
                            id_vars=['response_text'],
                            value_vars=['human_responses', 'weighted_llm_responses']
                        )
                        
                        fig2 = px.bar(
                            df_melted,
                            x='response_text',
                            y='value',
                            color='variable',
                            title=f"í†µí•© ë‹µë³€ ê²°ê³¼ (ì¸ê°„{' (ê°€ì¤‘ì¹˜ ì ìš©)' if apply_weights else ''} + LLM)",
                            labels={
                                'response_text': 'ë‹µë³€',
                                'value': 'ë‹µë³€ ìˆ˜',
                                'variable': 'ë‹µë³€ì ìœ í˜•'
                            },
                            barmode='stack'
                        )
                        
                        # ë²”ë¡€ ì´ë¦„ ë³€ê²½
                        fig2.update_traces(
                            name=f"ì¸ê°„ ë‹µë³€{' (ê°€ì¤‘ì¹˜ ì ìš©)' if apply_weights else ''}",
                            selector=dict(name="human_responses")
                        )
                        fig2.update_traces(
                            name="LLM ë‹µë³€ (ê°€ì¤‘ì¹˜ ì ìš©)",
                            selector=dict(name="weighted_llm_responses")
                        )
                        
                        st.plotly_chart(fig2, use_container_width=True)

        except mysql.connector.Error as err:
            st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {err}")
        finally:
            cursor.close()
            conn.close()

if __name__ == "__main__":
    os.environ['PYTHONPATH'] = os.getcwd()
    main() 