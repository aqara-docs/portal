import streamlit as st
import os
from dotenv import load_dotenv
import openai
import numpy as np
import time
import json
from datetime import datetime
import hashlib
import glob

# langchain ë° FAISS ê´€ë ¨
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.document_loaders import (
    PyPDFLoader, UnstructuredPowerPointLoader, UnstructuredExcelLoader, 
    UnstructuredWordDocumentLoader, UnstructuredMarkdownLoader
)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– ì•„ì¹´ë¼ë¼ì´í”„ ì‚¬ë‚´ ì±—ë´‡ (FAISS)",
    page_icon="ğŸ¤–",
    layout="wide"
)

load_dotenv()

# ===== LLM í´ë¼ì´ì–¸íŠ¸ ê´€ë¦¬ =====
class LLMClient:
    """ë‹¤ì–‘í•œ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì„¤ì •"""
        # OpenAI í´ë¼ì´ì–¸íŠ¸ (ê¸°ë³¸)
        openai_key = os.getenv('OPENAI_API_KEY')
        if openai_key:
            try:
                self.clients['openai'] = openai.OpenAI(api_key=openai_key)
                self.models['openai'] = [
                    'gpt-4o-mini',
                    'gpt-4o',
                    'gpt-4-turbo',
                    'gpt-4',
                    'gpt-3.5-turbo'
                ]
            except Exception as e:
                st.warning(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ LLM) - ì„ íƒì 
        try:
            import requests
            # Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì§§ì€ íƒ€ì„ì•„ì›ƒ)
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                self.clients['ollama'] = requests
                self.models['ollama'] = [
                    'mistral:latest',
                    'llama3.1:latest',
                    'llama3.1:8b',
                    'phi4:latest',
                    'llama3.3:latest',
                    'llama2:latest',
                    'gemma2:latest',
                    'gemma:latest',
                    'llama3.2:latest',
                    'deepseek-r1:32b',
                    'deepseek-r1:70b',
                    'deepseek-r1:14b',
                    'nomic-embed-text:latest'
                ]
        except Exception as e:
            # Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
            pass
        
        # Perplexity í´ë¼ì´ì–¸íŠ¸
        perplexity_key = os.getenv('PERPLEXITY_API_KEY')
        if perplexity_key:
            try:
                self.clients['perplexity'] = openai.OpenAI(
                    api_key=perplexity_key,
                    base_url="https://api.perplexity.ai"
                )
                self.models['perplexity'] = [
                    'sonar-pro',
                    'sonar-small-online',
                    'llama-3.1-sonar-small-128k-online',
                    'llama-3.1-sonar-medium-128k-online',
                    'llama-3.1-sonar-large-128k-online'
                ]
            except Exception as e:
                st.warning(f"Perplexity í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # Anthropic í´ë¼ì´ì–¸íŠ¸ (Claude)
        anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        if anthropic_key:
            try:
                import anthropic
                self.clients['anthropic'] = anthropic.Anthropic(api_key=anthropic_key)
                self.models['anthropic'] = [
                    'claude-3-7-sonnet-latest',
                    'claude-3-5-sonnet-20241022',
                    'claude-3-5-haiku-20241022',
                    'claude-3-opus-20240229',
                    'claude-3-sonnet-20240229',
                    'claude-3-haiku-20240307'
                ]
            except Exception as e:
                st.warning(f"Anthropic í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
        
        # Google Gemini í´ë¼ì´ì–¸íŠ¸
        google_key = os.getenv('GOOGLE_API_KEY')
        if google_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=google_key)
                self.clients['google'] = genai
                self.models['google'] = [
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-pro',
                    'gemini-pro-vision'
                ]
            except Exception as e:
                st.warning(f"Google Gemini í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def get_available_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ ë°˜í™˜"""
        return list(self.clients.keys())
    
    def get_models_for_provider(self, provider):
        """íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.models.get(provider, [])
    
    def generate_response(self, provider, model, messages, temperature=0.7, max_tokens=2000):
        """ì„ íƒëœ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            if provider == 'ollama':
                return self._generate_ollama_response(model, messages, temperature, max_tokens)
            elif provider == 'openai':
                return self._generate_openai_response(model, messages, temperature, max_tokens)
            elif provider == 'perplexity':
                return self._generate_perplexity_response(model, messages, temperature, max_tokens)
            elif provider == 'anthropic':
                return self._generate_anthropic_response(model, messages, temperature, max_tokens)
            elif provider == 'google':
                return self._generate_google_response(model, messages, temperature, max_tokens)
            else:
                return None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}"
        except Exception as e:
            return None, f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_ollama_response(self, model, messages, temperature, max_tokens):
        """Ollama ì‘ë‹µ ìƒì„±"""
        try:
            # Ollama API í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ë³€í™˜
            ollama_messages = []
            for msg in messages:
                if msg['role'] == 'system':
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” í”„ë¡¬í”„íŠ¸ì— í¬í•¨
                    continue
                elif msg['role'] == 'user':
                    ollama_messages.append({
                        'role': 'user',
                        'content': msg['content']
                    })
                elif msg['role'] == 'assistant':
                    ollama_messages.append({
                        'role': 'assistant',
                        'content': msg['content']
                    })
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì— í¬í•¨
            system_content = ""
            for msg in messages:
                if msg['role'] == 'system':
                    system_content = msg['content']
                    break
            
            if system_content and ollama_messages:
                ollama_messages[0]['content'] = f"{system_content}\n\n{ollama_messages[0]['content']}"
            
            # Ollama API í˜¸ì¶œ
            response = self.clients['ollama'].post(
                "http://localhost:11434/api/chat",
                json={
                    "model": model,
                    "messages": ollama_messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['message']['content'], None
            else:
                return None, f"Ollama API ì˜¤ë¥˜: {response.status_code}"
                
        except Exception as e:
            return None, f"Ollama ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_openai_response(self, model, messages, temperature, max_tokens):
        """OpenAI ì‘ë‹µ ìƒì„±"""
        response = self.clients['openai'].chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content, None
    
    def _generate_perplexity_response(self, model, messages, temperature, max_tokens):
        """Perplexity ì‘ë‹µ ìƒì„±"""
        response = self.clients['perplexity'].chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens
        )
        return response.choices[0].message.content, None
    
    def _generate_anthropic_response(self, model, messages, temperature, max_tokens):
        """Anthropic ì‘ë‹µ ìƒì„±"""
        # Anthropicì€ ë‹¤ë¥¸ ë©”ì‹œì§€ í˜•ì‹ì„ ì‚¬ìš©
        system_message = ""
        user_messages = []
        
        for msg in messages:
            if msg['role'] == 'system':
                system_message = msg['content']
            else:
                user_messages.append(msg['content'])
        
        user_content = "\n\n".join(user_messages)
        
        response = self.clients['anthropic'].messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            system=system_message,
            messages=[{"role": "user", "content": user_content}]
        )
        return response.content[0].text, None
    
    def _generate_google_response(self, model, messages, temperature, max_tokens):
        """Google Gemini ì‘ë‹µ ìƒì„±"""
        # GeminiëŠ” ë‹¤ë¥¸ ë©”ì‹œì§€ í˜•ì‹ì„ ì‚¬ìš©
        user_content = ""
        for msg in messages:
            if msg['role'] == 'user':
                user_content += msg['content'] + "\n\n"
        
        model_instance = self.clients['google'].GenerativeModel(model)
        response = model_instance.generate_content(
            user_content,
            generation_config=self.clients['google'].types.GenerationConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )
        )
        return response.text, None

# ===== FAISS ê¸°ë°˜ íŒŒì¼ì‹œìŠ¤í…œ RAGSystem =====
class FileRAGSystem:
    """FAISS + íŒŒì¼ì‹œìŠ¤í…œ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    def __init__(self, folder_path="./pages/rag_files"):
        self.folder_path = folder_path
        self.vectorstore = None
        self.docs = []
        self.is_loaded = False
        self.embeddings = OpenAIEmbeddings()
        self.load_files_and_build_index()

    def load_files_and_build_index(self):
        """í´ë” ë‚´ ëª¨ë“  ì§€ì› íŒŒì¼ì„ ì½ì–´ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶•"""
        loaders = [
            ("*.pdf", PyPDFLoader),
            ("*.pptx", UnstructuredPowerPointLoader),
            ("*.xlsx", UnstructuredExcelLoader),
            ("*.docx", UnstructuredWordDocumentLoader),
            ("*.md", UnstructuredMarkdownLoader),
        ]
        all_docs = []
        
        # ì „ì²´ ê²€ìƒ‰ì¸ ê²½ìš° finance í´ë” ì œì™¸
        if self.folder_path == "./pages/rag_files":
            for pattern, loader_cls in loaders:
                for file in glob.glob(os.path.join(self.folder_path, pattern)):
                    # finance í´ë” ì œì™¸
                    if "finance" not in file:
                        try:
                            loader = loader_cls(file)
                            all_docs.extend(loader.load())
                        except Exception as e:
                            st.warning(f"{file} ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            # íŠ¹ì • í´ë” ê²€ìƒ‰ì¸ ê²½ìš° ëª¨ë“  íŒŒì¼ í¬í•¨
            for pattern, loader_cls in loaders:
                for file in glob.glob(os.path.join(self.folder_path, pattern)):
                    try:
                        loader = loader_cls(file)
                        all_docs.extend(loader.load())
                    except Exception as e:
                        st.warning(f"{file} ë¡œë”© ì‹¤íŒ¨: {e}")
        
        self.docs = all_docs
        if all_docs:
            self.vectorstore = FAISS.from_documents(all_docs, self.embeddings)
            self.is_loaded = True
        else:
            self.vectorstore = None
            self.is_loaded = False

    def search(self, query, k=5):
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ top-k ë°˜í™˜"""
        if not self.is_loaded:
            return []
        try:
            results = self.vectorstore.similarity_search(query, k=k)
            return results
        except Exception as e:
            st.error(f"FAISS ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

# ===== ì±—ë´‡ í´ë˜ìŠ¤ (FAISS ê¸°ë°˜) =====
class FileRAGChatbot:
    def __init__(self):
        self.llm_client = LLMClient()
        self.rag_system = None
        self.conversation_history = []

    def set_rag_system(self, folder_path):
        """RAG ì‹œìŠ¤í…œì„ íŠ¹ì • í´ë”ë¡œ ì„¤ì •"""
        self.rag_system = FileRAGSystem(folder_path)

    def generate_response(self, user_query, provider='openai', model=None, temperature=0.7):
        try:
            if not self.rag_system:
                return "RAG ì‹œìŠ¤í…œì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í´ë”ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.", None
            
            # 1. RAGë¥¼ í†µí•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self.rag_system.search(user_query, k=5)
            if not relevant_docs:
                return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.", None
            # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            context = self._build_context(relevant_docs)
            # 3. í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(user_query, context)
            # 4. LLM ì‘ë‹µ ìƒì„±
            if model is None:
                models = self.llm_client.get_models_for_provider(provider)
                model = models[0] if models else None
            if not model:
                return "ì§€ì›í•˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.", None
            response, error = self.llm_client.generate_response(
                provider, model, prompt, temperature
            )
            if error:
                return f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {error}", None
            # 5. ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            self.conversation_history.append({
                'user': user_query,
                'assistant': response,
                'timestamp': datetime.now(),
                'provider': provider,
                'model': model
            })
            return response, relevant_docs
        except Exception as e:
            return f"ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", None

    def _build_context(self, relevant_docs):
        context_parts = []
        for i, doc in enumerate(relevant_docs, 1):
            title = doc.metadata.get('source', f'ë¬¸ì„œ {i}')
            content = doc.page_content[:1000]
            context_parts.append(f"ë¬¸ì„œ {i} - {title}:\n{content}")
        return "\n\n".join(context_parts)

    def _build_prompt(self, user_query, context):
        system_prompt = f"""ë‹¹ì‹ ì€ ì•„ì¹´ë¼ë¼ì´í”„ì˜ ì‚¬ë‚´ ì±—ë´‡ì…ë‹ˆë‹¤.\n\nì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì— ê·¼ê±°í•´ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.\në¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” 'í•´ë‹¹ ì •ë³´ê°€ ë¬¸ì„œì— ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n\nì•„ë˜ëŠ” ê´€ë ¨ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:\n{context}\n\nì‚¬ìš©ì ì§ˆë¬¸: {user_query}"""
        return [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]

    def clear_history(self):
        self.conversation_history = []

# ===== Streamlit UI =====
if 'chatbot' not in st.session_state:
    st.session_state.chatbot = FileRAGChatbot()
if 'conversation' not in st.session_state:
    st.session_state.conversation = []

st.title("ğŸ¤– ì•„ì¹´ë¼ë¼ì´í”„ ì‚¬ë‚´ ì±—ë´‡ (FAISS)")
st.markdown("ì—…ë¬´ ë¬¸ì„œ, PDF, PPTX, XLSX, DOCX, MD íŒŒì¼ ê¸°ë°˜ RAG ì±—ë´‡ì…ë‹ˆë‹¤.")

# í´ë” ì„ íƒ ë¼ë””ì˜¤ ë²„íŠ¼ (ë©”ì¸ í™”ë©´)
st.markdown("### ğŸ“ ê²€ìƒ‰ ë²”ìœ„ ì„ íƒ")
folder_option = st.radio(
    "ê²€ìƒ‰í•  í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
    [
        "ì „ì²´ ê²€ìƒ‰",
        "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤", 
        "ë™ë£Œ ì°¾ê¸°",
        "ì¬ë¬´"
    ],
    index=0,
    help="ì „ì²´ ê²€ìƒ‰: ëª¨ë“  í´ë” (ì¬ë¬´ ì œì™¸), ì—…ë¬´ í”„ë¡œì„¸ìŠ¤: process í´ë”, ë™ë£Œ ì°¾ê¸°: colleagues í´ë”, ì¬ë¬´: finance í´ë”"
)

# í´ë” ê²½ë¡œ ë§¤í•‘
folder_paths = {
    "ì „ì²´ ê²€ìƒ‰": "./pages/rag_files",
    "ì—…ë¬´ í”„ë¡œì„¸ìŠ¤": "./pages/rag_files/process",
    "ë™ë£Œ ì°¾ê¸°": "./pages/rag_files/colleagues", 
    "ì¬ë¬´": "./pages/rag_files/finance"
}

selected_folder = folder_paths[folder_option]

# ì„ íƒëœ í´ë” ì •ë³´ í‘œì‹œ
st.success(f"âœ… í˜„ì¬ ì„ íƒ: **{folder_option}** - {selected_folder}")

# RAG ì‹œìŠ¤í…œ ì„¤ì •
if 'current_folder' not in st.session_state or st.session_state.current_folder != selected_folder:
    st.session_state.current_folder = selected_folder
    with st.spinner(f"ğŸ“ {selected_folder} í´ë”ì˜ ë¬¸ì„œë¥¼ ë¡œë”© ì¤‘ì…ë‹ˆë‹¤..."):
        st.session_state.chatbot.set_rag_system(selected_folder)
    st.success(f"âœ… {selected_folder} í´ë”ì˜ ë¬¸ì„œê°€ ë¡œë”©ë˜ì—ˆìŠµë‹ˆë‹¤!")

# LLM ì œê³µì/ëª¨ë¸ ì„ íƒ
with st.sidebar:
    st.markdown("## âš™ï¸ LLM ì„¤ì •")
    providers = st.session_state.chatbot.llm_client.get_available_providers()
    if providers:
        # OpenAIë¥¼ ê¸°ë³¸ ì œê³µìë¡œ ì„¤ì •
        default_provider = 'openai' if 'openai' in providers else providers[0]
        provider_index = providers.index(default_provider) if default_provider in providers else 0
        
        selected_provider = st.selectbox(
            "LLM ì œê³µì ì„ íƒ",
            providers,
            index=provider_index
        )
        models = st.session_state.chatbot.llm_client.get_models_for_provider(selected_provider)
        if models:
            # ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
            if selected_provider == 'openai' and 'gpt-4o-mini' in models:
                model_index = models.index('gpt-4o-mini')
            elif selected_provider == 'anthropic' and 'claude-3-7-sonnet-latest' in models:
                model_index = models.index('claude-3-7-sonnet-latest')
            elif selected_provider == 'ollama' and 'mistral:latest' in models:
                model_index = models.index('mistral:latest')
            else:
                model_index = 0
            
            selected_model = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                models,
                index=model_index
            )
        else:
            selected_model = None
    else:
        st.error("ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤.")
        selected_provider = None
        selected_model = None
    temperature = st.slider("ì°½ì˜ì„± (Temperature)", 0.0, 1.0, 0.7, 0.1)
    st.markdown("---")
    if st.button("ğŸ”„ íŒŒì¼ ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"):
        with st.spinner("íŒŒì¼ ì¸ë±ìŠ¤ë¥¼ ì¬êµ¬ì¶• ì¤‘ì…ë‹ˆë‹¤..."):
            st.session_state.chatbot.set_rag_system(selected_folder)
            st.success("íŒŒì¼ ì¸ë±ìŠ¤ê°€ ì¬êµ¬ì¶•ë˜ì—ˆìŠµë‹ˆë‹¤!")

# CSS ìŠ¤íƒ€ì¼ ì¶”ê°€ (00_ğŸ¤”_Q-Li.pyì™€ ë™ì¼)
st.markdown("""
<style>
.user-bubble {
    background: #2d2d2d;
    color: #fff;
    padding: 0.7em 1em;
    border-radius: 1em 1em 0 1em;
    margin: 0.5em 0;
    max-width: 70%;
    align-self: flex-end;
    text-align: right;
}
.bot-bubble {
    background: #2323a7;
    color: #fff;
    padding: 0.7em 1em;
    border-radius: 1em 1em 1em 0;
    margin: 0.5em 0;
    max-width: 70%;
    align-self: flex-start;
    text-align: left;
}
.bubble-container {
    display: flex;
    flex-direction: column;
}
</style>
""", unsafe_allow_html=True)

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ (ë§¨ ìœ„)
st.markdown("### ğŸ’¬ ì±—ë´‡ê³¼ ëŒ€í™”í•˜ê¸°")
st.markdown('<div class="bubble-container">', unsafe_allow_html=True)
for i, message in enumerate(st.session_state.conversation):
    if message['role'] == 'ì‚¬ìš©ì':
        st.markdown(f"<div class='user-bubble'>ğŸ‘¤ {message['content']}</div>", unsafe_allow_html=True)
    else:
        st.markdown(f"<div class='bot-bubble'>ğŸ¤– {message['content']}</div>", unsafe_allow_html=True)
        if 'relevant_docs' in message and message['relevant_docs']:
            with st.expander("ğŸ“š ì°¸ê³  ë¬¸ì„œ", expanded=False):
                for j, doc in enumerate(message['relevant_docs'], 1):
                    title = doc.metadata.get('source', f'ë¬¸ì„œ {j}')
                    st.markdown(f"**{title}**")
                    st.markdown(doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content)
st.markdown('</div>', unsafe_allow_html=True)

# ì…ë ¥ í•„ë“œ (ë§¨ ì•„ë˜ ê³ ì •)
st.markdown("---")
st.markdown("### ğŸ’¬ ì§ˆë¬¸í•˜ê¸°")

# ì…ë ¥ í•„ë“œ í‚¤ ê´€ë¦¬
if 'chat_input_key' not in st.session_state:
    st.session_state['chat_input_key'] = 0

# ì…ë ¥ í•„ë“œ
user_input = st.text_area(
    "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
    key=f"chat_input_{st.session_state['chat_input_key']}",
    value="",
    height=80,
    label_visibility="collapsed",
    placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
)

if st.button("ì „ì†¡", type="primary"):
    if user_input.strip():
        st.session_state.conversation.append({'role': 'ì‚¬ìš©ì', 'content': user_input, 'timestamp': datetime.now()})
        with st.spinner("ğŸ¤– ë‹µë³€ ìƒì„± ì¤‘..."):
            response, relevant_docs = st.session_state.chatbot.generate_response(
                user_input, selected_provider, selected_model, temperature
            )
            st.session_state.conversation.append({'role': 'ì±—ë´‡', 'content': response, 'timestamp': datetime.now(), 'provider': selected_provider, 'model': selected_model, 'relevant_docs': relevant_docs})
        # ì…ë ¥ ì´ˆê¸°í™”ë¥¼ ìœ„í•œ í‚¤ ë³€ê²½
        if 'chat_input_key' not in st.session_state:
            st.session_state['chat_input_key'] = 0
        st.session_state['chat_input_key'] += 1
        st.rerun() 