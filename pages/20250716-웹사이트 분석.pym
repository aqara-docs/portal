import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
import json
import base64
import requests
import graphviz
import plotly.express as px
import plotly.graph_objects as go
import mysql.connector
import concurrent.futures
import time
import re
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import requests
from PIL import Image
import io
import hashlib

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¢ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„",
    page_icon="ğŸ¢",
    layout="wide"
)
st.title("ğŸ¯ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì œí’ˆ ë¶„ì„ ì‹œìŠ¤í…œ")

# ì¸ì¦ ê¸°ëŠ¥
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.')
    st.stop()

if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()

# API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
anthropic_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

# MySQL Database configuration
db_config = {
    'user': os.getenv('SQL_USER'),
    'password': os.getenv('SQL_PASSWORD'),
    'host': os.getenv('SQL_HOST'),
    'database': os.getenv('SQL_DATABASE_NEWBIZ'),
    'charset': 'utf8mb4',
    'collation': 'utf8mb4_general_ci'
}

# === ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë° ì œí’ˆ ë¶„ì„ ì „ë¬¸ê°€ ì—ì´ì „íŠ¸ ì •ì˜ ===
BUSINESS_ANALYSIS_AGENTS = {
    "business_model_analyzer": {
        "name": "ğŸ¢ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„ê°€",
        "emoji": "ğŸ¢",
        "description": "íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ìˆ˜ìµ êµ¬ì¡° ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìº”ë²„ìŠ¤ ë¶„ì„
- ìˆ˜ìµ ëª¨ë¸ ë° ê°€ì¹˜ ì œì•ˆ ë¶„ì„
- ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë° ì±„ë„ ë¶„ì„
- í•µì‹¬ íŒŒíŠ¸ë„ˆì‹­ ë° ë¦¬ì†ŒìŠ¤ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ìˆ˜ìµì› ë° ìˆ˜ìµ êµ¬ì¡°
- ê³ ê° ê°€ì¹˜ ì œì•ˆ (CVP)
- ë¹„ìš© êµ¬ì¡° ë° í•µì‹¬ í™œë™
- ê²½ìŸ ìš°ìœ„ ë° ì°¨ë³„í™” ìš”ì†Œ
- ì‹œì¥ í¬ì§€ì…”ë‹

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "product_service_analyzer": {
        "name": "ğŸ“¦ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ê°€",
        "emoji": "ğŸ“¦",
        "description": "ì œí’ˆê³¼ ì„œë¹„ìŠ¤ ë¼ì¸ì—… ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 12ë…„ ê²½ë ¥ì˜ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì œí’ˆ/ì„œë¹„ìŠ¤ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„
- ì œí’ˆ íŠ¹ì§• ë° ê¸°ëŠ¥ ë¶„ì„
- ê°€ê²© ì „ëµ ë° ê°€ì¹˜ ë¶„ì„
- ì œí’ˆ ë¼ì´í”„ì‚¬ì´í´ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- í•µì‹¬ ì œí’ˆ/ì„œë¹„ìŠ¤ ì‹ë³„
- ì œí’ˆ íŠ¹ì§• ë° í˜œíƒ
- ê°€ê²© ì •ì±… ë° ê°€ì¹˜ ì œì•ˆ
- ì œí’ˆ ì°¨ë³„í™” ìš”ì†Œ
- ì‹ ì œí’ˆ ê°œë°œ ë°©í–¥

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "market_position_analyzer": {
        "name": "ğŸ¯ ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„ê°€",
        "emoji": "ğŸ¯",
        "description": "ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜ì™€ ê²½ìŸ ìƒí™© ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„
- ê²½ìŸì‚¬ ë¹„êµ ë¶„ì„
- íƒ€ê²Ÿ ì‹œì¥ ë° ê³ ê° ë¶„ì„
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜
- ê²½ìŸ ìš°ìœ„ ë° ì°¨ë³„í™”
- íƒ€ê²Ÿ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸
- ì‹œì¥ ê¸°íšŒ ë° ìœ„í—˜ ìš”ì†Œ
- ë¸Œëœë“œ í¬ì§€ì…”ë‹

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì‹œì¥ í¬ì§€ì…”ë‹ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "technology_stack_analyzer": {
        "name": "âš™ï¸ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„ê°€",
        "emoji": "âš™ï¸",
        "description": "ì‚¬ìš© ê¸°ìˆ ê³¼ ê¸°ìˆ ì  ì—­ëŸ‰ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 8ë…„ ê²½ë ¥ì˜ ê¸°ìˆ  ìŠ¤íƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ê¸°ìˆ  ìŠ¤íƒ ë° í”Œë«í¼ ë¶„ì„
- ê°œë°œ ì—­ëŸ‰ ë° ê¸°ìˆ ì  íŠ¹ì§•
- ê¸°ìˆ ì  ì°¨ë³„í™” ìš”ì†Œ
- ê¸°ìˆ ì  ì œì•½ ë° ê¸°íšŒ

**ë¶„ì„ ê´€ì :**
- ì‚¬ìš© ê¸°ìˆ  ë° í”Œë«í¼
- ê¸°ìˆ ì  íŠ¹ì§• ë° ì¥ì 
- ê°œë°œ ì—­ëŸ‰ ë° íŒ€ êµ¬ì„±
- ê¸°ìˆ ì  ì°¨ë³„í™” ìš”ì†Œ
- ê¸°ìˆ ì  ì œì•½ ë° í•œê³„

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ê¸°ìˆ ì  íŠ¹ì§•ì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "growth_strategy_analyzer": {
        "name": "ğŸ“ˆ ì„±ì¥ ì „ëµ ë¶„ì„ê°€",
        "emoji": "ğŸ“ˆ",
        "description": "ì„±ì¥ ì „ëµê³¼ ë¹„ì „ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 12ë…„ ê²½ë ¥ì˜ ì„±ì¥ ì „ëµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì„±ì¥ ì „ëµ ë° ë¹„ì „ ë¶„ì„
- ì‹œì¥ í™•ì¥ ê³„íš ë¶„ì„
- íˆ¬ì ë° íŒŒì´ë‚¸ì‹± ë¶„ì„
- ë¯¸ë˜ ê³„íš ë° ë¡œë“œë§µ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì„±ì¥ ì „ëµ ë° ë°©í–¥
- ì‹œì¥ í™•ì¥ ê³„íš
- íˆ¬ì ìœ ì¹˜ ë° íŒŒì´ë‚¸ì‹±
- ë¯¸ë˜ ë¹„ì „ ë° ëª©í‘œ
- ì „ëµì  íŒŒíŠ¸ë„ˆì‹­

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì„±ì¥ ì „ëµì„ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    },
    
    "financial_health_analyzer": {
        "name": "ğŸ’° ì¬ë¬´ ìƒíƒœ ë¶„ì„ê°€",
        "emoji": "ğŸ’°",
        "description": "ì¬ë¬´ ìƒíƒœì™€ íˆ¬ì ì •ë³´ ë¶„ì„",
        "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì¬ë¬´ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì¬ë¬´ ìƒíƒœ ë° ì„±ê³¼ ë¶„ì„
- íˆ¬ì ì •ë³´ ë° íŒŒì´ë‚¸ì‹± ë¶„ì„
- ìˆ˜ìµì„± ë° ì„±ì¥ì„± ë¶„ì„
- ì¬ë¬´ì  ìœ„í—˜ ìš”ì†Œ ë¶„ì„

**ë¶„ì„ ê´€ì :**
- ì¬ë¬´ ì„±ê³¼ ë° ì§€í‘œ
- íˆ¬ì ìœ ì¹˜ í˜„í™©
- ìˆ˜ìµì„± ë° ì„±ì¥ì„±
- ì¬ë¬´ì  ìœ„í—˜ ìš”ì†Œ
- ìë³¸ êµ¬ì¡° ë° ìœ ë™ì„±

ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ë¶„ì„í•˜ì—¬ íšŒì‚¬ì˜ ì¬ë¬´ ìƒíƒœë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”."""
    }
}

# === ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë“¤ ===
def scrape_website_basic(url, max_pages=10):
    """ê¸°ë³¸ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        data = {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'meta_keywords': '',
            'headings': [],
            'links': [],
            'images': [],
            'text_content': '',
            'html_structure': str(soup)[:5000],  # HTML êµ¬ì¡° (ì²˜ìŒ 5000ì)
            'status_code': response.status_code,
            'response_time': response.elapsed.total_seconds(),
            'content_length': len(response.content)
        }
        
        # ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                data['meta_description'] = meta.get('content', '')
            elif meta.get('name') == 'keywords':
                data['meta_keywords'] = meta.get('content', '')
        
        # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
        for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
            headings = soup.find_all(tag)
            for heading in headings:
                data['headings'].append({
                    'tag': tag,
                    'text': heading.get_text(strip=True)
                })
        
        # ë§í¬ ì¶”ì¶œ
        links = soup.find_all('a', href=True)
        for link in links[:50]:  # ì²˜ìŒ 50ê°œë§Œ
            data['links'].append({
                'text': link.get_text(strip=True),
                'href': link['href'],
                'title': link.get('title', '')
            })
        
        # ì´ë¯¸ì§€ ì¶”ì¶œ
        images = soup.find_all('img')
        for img in images[:20]:  # ì²˜ìŒ 20ê°œë§Œ
            data['images'].append({
                'src': img.get('src', ''),
                'alt': img.get('alt', ''),
                'title': img.get('title', '')
            })
        
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
        text_content = soup.get_text()
        data['text_content'] = ' '.join(text_content.split())[:10000]  # ì²˜ìŒ 10000ë‹¨ì–´
        
        return data
        
    except Exception as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def analyze_website_performance(url):
    """ì›¹ì‚¬ì´íŠ¸ ì„±ëŠ¥ ë¶„ì„ (PageSpeed Insights API ì‚¬ìš©)"""
    try:
        # Google PageSpeed Insights API í‚¤ê°€ ìˆë‹¤ë©´ ì‚¬ìš©
        api_key = os.getenv('GOOGLE_PAGESPEED_API_KEY')
        
        if api_key:
            api_url = f"https://www.googleapis.com/pagespeedonline/v5/runPagespeed"
            params = {
                'url': url,
                'key': api_key,
                'strategy': 'mobile'  # ëª¨ë°”ì¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„
            }
            
            response = requests.get(api_url, params=params)
            response.raise_for_status()
            data = response.json()
            
            return {
                'performance_score': data.get('lighthouseResult', {}).get('categories', {}).get('performance', {}).get('score', 0) * 100,
                'accessibility_score': data.get('lighthouseResult', {}).get('categories', {}).get('accessibility', {}).get('score', 0) * 100,
                'best_practices_score': data.get('lighthouseResult', {}).get('categories', {}).get('best-practices', {}).get('score', 0) * 100,
                'seo_score': data.get('lighthouseResult', {}).get('categories', {}).get('seo', {}).get('score', 0) * 100,
                'first_contentful_paint': data.get('lighthouseResult', {}).get('audits', {}).get('first-contentful-paint', {}).get('numericValue', 0),
                'largest_contentful_paint': data.get('lighthouseResult', {}).get('audits', {}).get('largest-contentful-paint', {}).get('numericValue', 0),
                'cumulative_layout_shift': data.get('lighthouseResult', {}).get('audits', {}).get('cumulative-layout-shift', {}).get('numericValue', 0)
            }
        else:
            # API í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë¶„ì„
            return {
                'performance_score': None,
                'accessibility_score': None,
                'best_practices_score': None,
                'seo_score': None,
                'note': 'Google PageSpeed Insights API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.'
            }
            
    except Exception as e:
        st.warning(f"ì„±ëŠ¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def analyze_website_security(url):
    """ì›¹ì‚¬ì´íŠ¸ ë³´ì•ˆ ë¶„ì„"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        
        security_data = {
            'https_enabled': url.startswith('https://'),
            'security_headers': {},
            'ssl_certificate': None,
            'content_security_policy': None,
            'x_frame_options': None,
            'x_content_type_options': None,
            'x_xss_protection': None,
            'strict_transport_security': None
        }
        
        # ë³´ì•ˆ í—¤ë” ë¶„ì„
        headers_to_check = [
            'Content-Security-Policy',
            'X-Frame-Options',
            'X-Content-Type-Options',
            'X-XSS-Protection',
            'Strict-Transport-Security'
        ]
        
        for header in headers_to_check:
            value = response.headers.get(header)
            if value:
                security_data['security_headers'][header] = value
                if header == 'Content-Security-Policy':
                    security_data['content_security_policy'] = value
                elif header == 'X-Frame-Options':
                    security_data['x_frame_options'] = value
                elif header == 'X-Content-Type-Options':
                    security_data['x_content_type_options'] = value
                elif header == 'X-XSS-Protection':
                    security_data['x_xss_protection'] = value
                elif header == 'Strict-Transport-Security':
                    security_data['strict_transport_security'] = value
        
        return security_data
        
    except Exception as e:
        st.warning(f"ë³´ì•ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

# === ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ë“¤ ===
def scrape_website_content(url, max_pages=20):
    """ì›¹ì‚¬ì´íŠ¸ ì „ì²´ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ (ëª¨ë“  í˜ì´ì§€ í¬í•¨)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # ë©”ì¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ëª¨ë“  ë‚´ë¶€ ë§í¬ ìˆ˜ì§‘
        base_url = url.rstrip('/')
        internal_links = set()
        
        # ë§í¬ ì¶”ì¶œ
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            
            # ê°™ì€ ë„ë©”ì¸ì˜ ë‚´ë¶€ ë§í¬ë§Œ ìˆ˜ì§‘
            if urlparse(full_url).netloc == urlparse(url).netloc:
                # ë¶ˆí•„ìš”í•œ íŒŒë¼ë¯¸í„° ì œê±°
                clean_url = urlparse(full_url)._replace(query='', fragment='').geturl()
                internal_links.add(clean_url)
        
        # ìˆ˜ì§‘ëœ ë§í¬ë“¤ ì¤‘ max_pagesê°œë§Œ ì„ íƒ
        links_to_scrape = list(internal_links)[:max_pages]
        
        # ì „ì²´ ì½˜í…ì¸  ë°ì´í„°
        all_content = {
            'url': url,
            'title': soup.title.string if soup.title else '',
            'meta_description': '',
            'headings': [],
            'paragraphs': [],
            'lists': [],
            'text_content': '',
            'word_count': 0,
            'sentence_count': 0,
            'paragraph_count': 0,
            'status_code': response.status_code,
            'response_time': response.elapsed.total_seconds(),
            'scraped_pages': [],
            'total_pages': len(links_to_scrape) + 1
        }
        
        # ë©”ì¸ í˜ì´ì§€ ë©”íƒ€ íƒœê·¸ ì¶”ì¶œ
        meta_tags = soup.find_all('meta')
        for meta in meta_tags:
            if meta.get('name') == 'description':
                all_content['meta_description'] = meta.get('content', '')
        
        # ë©”ì¸ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ
        main_page_content = extract_page_content(soup, url)
        all_content['scraped_pages'].append(main_page_content)
        
        # ë‚´ë¶€ í˜ì´ì§€ë“¤ ìŠ¤í¬ë˜í•‘
        st.info(f"ğŸ” {len(links_to_scrape)}ê°œì˜ ë‚´ë¶€ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë˜í•‘ ì¤‘...")
        
        with st.spinner("ë‚´ë¶€ í˜ì´ì§€ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            for i, link_url in enumerate(links_to_scrape):
                try:
                    page_response = requests.get(link_url, headers=headers, timeout=5)
                    if page_response.status_code == 200:
                        page_soup = BeautifulSoup(page_response.content, 'html.parser')
                        page_content = extract_page_content(page_soup, link_url)
                        all_content['scraped_pages'].append(page_content)
                        
                        # ì§„í–‰ë¥  í‘œì‹œ
                        progress = (i + 1) / len(links_to_scrape)
                        st.progress(progress)
                        
                except Exception as e:
                    st.warning(f"í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {link_url} - {str(e)}")
                    continue
        
        # ëª¨ë“  í˜ì´ì§€ ì½˜í…ì¸  í†µí•©
        all_content = merge_all_page_content(all_content)
        
        st.success(f"âœ… ì´ {len(all_content['scraped_pages'])}ê°œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ")
        
        return all_content
        
    except Exception as e:
        st.error(f"ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def extract_page_content(soup, page_url):
    """ê°œë³„ í˜ì´ì§€ ì½˜í…ì¸  ì¶”ì¶œ"""
    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
    for script in soup(["script", "style", "nav", "footer", "header"]):
        script.decompose()
    
    page_data = {
        'url': page_url,
        'title': soup.title.string if soup.title else '',
        'headings': [],
        'paragraphs': [],
        'lists': [],
        'text_content': ''
    }
    
    # í—¤ë”© íƒœê·¸ ì¶”ì¶œ
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        headings = soup.find_all(tag)
        for heading in headings:
            text = heading.get_text(strip=True)
            if text:
                page_data['headings'].append({
                    'tag': tag,
                    'text': text,
                    'level': int(tag[1])
                })
    
    # ë¬¸ë‹¨ ì¶”ì¶œ
    paragraphs = soup.find_all('p')
    for p in paragraphs:
        text = p.get_text(strip=True)
        if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” ë¬¸ë‹¨ë§Œ
            page_data['paragraphs'].append(text)
    
    # ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
    lists = soup.find_all(['ul', 'ol'])
    for lst in lists:
        items = lst.find_all('li')
        list_items = []
        for item in items:
            text = item.get_text(strip=True)
            if text:
                list_items.append(text)
        if list_items:
            page_data['lists'].append({
                'type': lst.name,
                'items': list_items
            })
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
    text_content = soup.get_text()
    text_content = ' '.join(text_content.split())
    page_data['text_content'] = text_content
    
    return page_data

def merge_all_page_content(all_content):
    """ëª¨ë“  í˜ì´ì§€ ì½˜í…ì¸  í†µí•©"""
    merged_content = all_content.copy()
    
    # ëª¨ë“  í˜ì´ì§€ì˜ ì½˜í…ì¸  í†µí•©
    all_text = ""
    all_headings = []
    all_paragraphs = []
    all_lists = []
    
    for page in all_content['scraped_pages']:
        # í…ìŠ¤íŠ¸ ì½˜í…ì¸  í†µí•©
        if page['text_content']:
            all_text += page['text_content'] + " "
        
        # í—¤ë”© í†µí•©
        all_headings.extend(page['headings'])
        
        # ë¬¸ë‹¨ í†µí•©
        all_paragraphs.extend(page['paragraphs'])
        
        # ë¦¬ìŠ¤íŠ¸ í†µí•©
        all_lists.extend(page['lists'])
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    merged_content['text_content'] = all_text.strip()
    merged_content['headings'] = all_headings
    merged_content['paragraphs'] = list(set(all_paragraphs))  # ì¤‘ë³µ ì œê±°
    merged_content['lists'] = all_lists
    
    # í†µê³„ ì¬ê³„ì‚°
    merged_content['word_count'] = len(merged_content['text_content'].split())
    merged_content['sentence_count'] = len(re.split(r'[.!?]+', merged_content['text_content']))
    merged_content['paragraph_count'] = len(merged_content['paragraphs'])
    
    return merged_content

def analyze_content_readability(text_content):
    """ì½˜í…ì¸  ê°€ë…ì„± ë¶„ì„"""
    try:
        # ê°„ë‹¨í•œ ê°€ë…ì„± ì§€ìˆ˜ ê³„ì‚°
        sentences = re.split(r'[.!?]+', text_content)
        words = text_content.split()
        
        # í‰ê·  ë¬¸ì¥ ê¸¸ì´
        avg_sentence_length = len(words) / len(sentences) if sentences else 0
        
        # ê¸´ ë¬¸ì¥ ë¹„ìœ¨ (20ë‹¨ì–´ ì´ìƒ)
        long_sentences = sum(1 for s in sentences if len(s.split()) > 20)
        long_sentence_ratio = long_sentences / len(sentences) if sentences else 0
        
        # ê°€ë…ì„± ë“±ê¸‰
        if avg_sentence_length < 10:
            readability_grade = "ë§¤ìš° ì‰¬ì›€"
        elif avg_sentence_length < 15:
            readability_grade = "ì‰¬ì›€"
        elif avg_sentence_length < 20:
            readability_grade = "ë³´í†µ"
        elif avg_sentence_length < 25:
            readability_grade = "ì–´ë ¤ì›€"
        else:
            readability_grade = "ë§¤ìš° ì–´ë ¤ì›€"
        
        return {
            'avg_sentence_length': round(avg_sentence_length, 2),
            'long_sentence_ratio': round(long_sentence_ratio * 100, 2),
            'readability_grade': readability_grade,
            'total_sentences': len(sentences),
            'total_words': len(words)
        }
        
    except Exception as e:
        st.warning(f"ê°€ë…ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def get_ai_response(prompt, model_name="gpt-4o-mini", system_prompt="", enable_thinking=False):
    """AI ì‘ë‹µ ìƒì„± í•¨ìˆ˜"""
    try:
        if model_name.startswith("gpt"):
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = openai.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=0.7,
                max_tokens=3000
            )
            
            return {
                'content': response.choices[0].message.content,
                'success': True,
                'error': None
            }
            
        elif model_name.startswith("claude"):
            response = anthropic_client.messages.create(
                model=model_name,
                max_tokens=3000,
                temperature=0.7,
                system=system_prompt,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                'content': response.content[0].text,
                'success': True,
                'error': None
            }
            
    except Exception as e:
        return {
            'content': f"AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}",
            'success': False,
            'error': str(e)
        }

def analyze_with_business_agent(args):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ê°œë³„ ì—ì´ì „íŠ¸ ë¶„ì„ í•¨ìˆ˜"""
    agent_key, agent_info, content_data, model_name, url, enable_thinking = args
    
    try:
        # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” mermaid ì°¨íŠ¸ ê°€ì´ë“œ
        mermaid_guides = {
            "business_model_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìº”ë²„ìŠ¤
- quadrantChart: ìˆ˜ìµ ëª¨ë¸ ë§¤íŠ¸ë¦­ìŠ¤
- timeline: ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ì§„í™”
""",
            "product_service_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ì œí’ˆ/ì„œë¹„ìŠ¤ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¡°
- pie ì°¨íŠ¸: ì œí’ˆ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬
- timeline: ì œí’ˆ ê°œë°œ ë° ì¶œì‹œ ë¡œë“œë§µ
- quadrantChart: ì œí’ˆ ê°€ê²©-ê¸°ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤
""",
            "market_position_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ
- quadrantChart: ê²½ìŸ ìš°ìœ„ ë§¤íŠ¸ë¦­ìŠ¤
- timeline: ì‹œì¥ ì§„í™” ê³¼ì •
""",
            "technology_stack_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ê¸°ìˆ  ìŠ¤íƒ ì•„í‚¤í…ì²˜
- pie ì°¨íŠ¸: ê¸°ìˆ  íˆ¬ì ë¹„ì¤‘
- timeline: ê¸°ìˆ  ë°œì „ ë¡œë“œë§µ
""",
            "growth_strategy_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ì„±ì¥ ì „ëµ ë¡œë“œë§µ
- quadrantChart: ì„±ì¥ ê¸°íšŒ ë§¤íŠ¸ë¦­ìŠ¤
- timeline: ì„±ì¥ ë‹¨ê³„ë³„ ê³„íš
""",
            "financial_health_analyzer": """
**Mermaid ì°¨íŠ¸ ìš”ì²­:**
- flowchart: ì¬ë¬´ ìƒíƒœ ë¶„ì„
- pie ì°¨íŠ¸: ìˆ˜ìµ êµ¬ì¡° ë¶„ì„
- timeline: ì¬ë¬´ ì„±ê³¼ ì¶”ì´
"""
        }
        
        # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
        if agent_key == "product_service_analyzer":
            agent_prompt = f"""
{agent_info['system_prompt']}

ë‹¤ìŒì€ {url} ì›¹ì‚¬ì´íŠ¸ì˜ ì½˜í…ì¸  ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìˆ˜:** {len(content_data.get('scraped_pages', []))}ê°œ
**ì´ ë‹¨ì–´ ìˆ˜:** {content_data.get('word_count', 0):,}ê°œ

**ì½˜í…ì¸  (ì „ì²´ í˜ì´ì§€ í†µí•©):**
{content_data.get('text_content', 'N/A')}

**ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ ìš”ì²­ì‚¬í•­:**

1. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜**
   - ì£¼ìš” ì œí’ˆ ë¼ì¸ì—… (ì¹´í…Œê³ ë¦¬ë³„)
   - ì„œë¹„ìŠ¤ ìœ í˜• (B2B, B2C, SaaS ë“±)
   - ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¡°

2. **ì œí’ˆ ìƒì„¸ ë¶„ì„**
   - ê° ì œí’ˆì˜ êµ¬ì²´ì ì¸ ì´ë¦„ê³¼ ì„¤ëª…
   - ì œí’ˆë³„ ì£¼ìš” ê¸°ëŠ¥ê³¼ íŠ¹ì§•
   - ì œí’ˆë³„ ê°€ê²© ì •ì±… (ê°€ëŠ¥í•œ ê²½ìš°)
   - ì œí’ˆë³„ íƒ€ê²Ÿ ê³ ê°

3. **ì„œë¹„ìŠ¤ ë¶„ì„**
   - ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ ìœ í˜•
   - ì„œë¹„ìŠ¤ íŠ¹ì§•ê³¼ í˜œíƒ
   - ì„œë¹„ìŠ¤ ê°€ê²© ì •ì±…
   - ì„œë¹„ìŠ¤ ì´ìš© ë°©ë²•

4. **ì œí’ˆ/ì„œë¹„ìŠ¤ ì°¨ë³„í™” ìš”ì†Œ**
   - ê²½ìŸì‚¬ ëŒ€ë¹„ ì¥ì 
   - ë…ìì  ê¸°ìˆ ì´ë‚˜ íŠ¹í—ˆ
   - í’ˆì§ˆ ë³´ì¦ ë° ì¸ì¦
   - ê³ ê° ì§€ì› ì„œë¹„ìŠ¤

5. **ì œí’ˆ ê°œë°œ í˜„í™©**
   - ì‹ ì œí’ˆ ê°œë°œ ê³„íš
   - ì œí’ˆ ì—…ë°ì´íŠ¸ ë° ê°œì„ 
   - ì œí’ˆ ë¼ì´í”„ì‚¬ì´í´ ë‹¨ê³„

6. **ì œí’ˆ ê´€ë ¨ ë°ì´í„°**
   - ì œí’ˆ ìˆ˜ëŸ‰ ë° ì¢…ë¥˜
   - ì œí’ˆë³„ ì¸ê¸°ë„ë‚˜ ì¸ì§€ë„
   - ì œí’ˆ ê´€ë ¨ ê³ ê° í”¼ë“œë°±

**ì¤‘ìš”**: ì›¹ì‚¬ì´íŠ¸ì˜ ëª¨ë“  í˜ì´ì§€ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëˆ„ë½ëœ ì œí’ˆì´ë‚˜ ì„œë¹„ìŠ¤ê°€ ì—†ë„ë¡ í•´ì£¼ì„¸ìš”.

{mermaid_guides.get(agent_key, "")}

**ì¤‘ìš”**: ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ Mermaid ì°¨íŠ¸ë¥¼ 1-2ê°œ í¬í•¨í•´ì£¼ì„¸ìš”. ì°¨íŠ¸ëŠ” ```mermaid ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¶„ì„ ë‚´ìš©ê³¼ ì˜ ì—°ê³„ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        else:
            agent_prompt = f"""
{agent_info['system_prompt']}

ë‹¤ìŒì€ {url} ì›¹ì‚¬ì´íŠ¸ì˜ ì½˜í…ì¸  ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

{json.dumps(content_data, ensure_ascii=False, indent=2)}

ë‹¹ì‹ ì˜ ì „ë¬¸ ë¶„ì•¼ì¸ {agent_info['description']} ê´€ì ì—ì„œ ì´ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆì„ ë¶„ì„í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ìš”ì²­ì‚¬í•­:**
1. ì£¼ìš” ë¹„ì¦ˆë‹ˆìŠ¤ ë°œê²¬ì‚¬í•­ (3-5ê°œ)
2. ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì˜ ê°•ì ê³¼ ìš°ìˆ˜í•œ ì 
3. ê°œì„ ì´ í•„ìš”í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì—­
4. êµ¬ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê°œì„  ê¶Œê³ ì‚¬í•­
5. ë¹„ì¦ˆë‹ˆìŠ¤ ì ì¬ë ¥ í‰ê°€ (1-10ì , ì´ìœ  í¬í•¨)

{mermaid_guides.get(agent_key, "")}

**ì¤‘ìš”**: ë¶„ì„ ë‚´ìš©ì— ì ì ˆí•œ Mermaid ì°¨íŠ¸ë¥¼ 1-2ê°œ í¬í•¨í•´ì£¼ì„¸ìš”. ì°¨íŠ¸ëŠ” ```mermaid ì½”ë“œë¸”ë¡ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¶„ì„ ë‚´ìš©ê³¼ ì˜ ì—°ê³„ë˜ë„ë¡ í•´ì£¼ì„¸ìš”.

êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
        
        # AI ì‘ë‹µ ìƒì„±
        response = get_ai_response(
            prompt=agent_prompt,
            model_name=model_name,
            system_prompt=agent_info['system_prompt'],
            enable_thinking=enable_thinking
        )
        
        return {
            'agent_key': agent_key,
            'agent_name': agent_info['name'],
            'agent_emoji': agent_info['emoji'],
            'analysis': response['content'],
            'success': response['success'],
            'error': response.get('error', None)
        }
        
    except Exception as e:
        return {
            'agent_key': agent_key,
            'agent_name': agent_info['name'], 
            'agent_emoji': agent_info['emoji'],
            'analysis': f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            'success': False,
            'error': str(e)
        }

def run_business_multi_agent_analysis(url, content_data, selected_agents, model_name, enable_thinking=False):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰"""
    
    # ì§„í–‰ ìƒí™© í‘œì‹œìš© ì»¨í…Œì´ë„ˆ
    progress_container = st.container()
    
    with progress_container:
        st.info("ğŸš€ **ë¹„ì¦ˆë‹ˆìŠ¤ ë©€í‹° ì—ì´ì „íŠ¸ ë¶„ì„ ì‹œì‘**")
        
        # ì—ì´ì „íŠ¸ë³„ ìƒíƒœ í‘œì‹œ
        agent_status = {}
        agent_progress = {}
        
        cols = st.columns(len(selected_agents))
        for i, agent_key in enumerate(selected_agents):
            with cols[i]:
                agent_info = BUSINESS_ANALYSIS_AGENTS[agent_key]
                agent_status[agent_key] = st.empty()
                agent_progress[agent_key] = st.progress(0)
                
                agent_status[agent_key].info(f"{agent_info['emoji']} {agent_info['name']}\nëŒ€ê¸° ì¤‘...")
        
        # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰
        st.info("âš¡ **ë³‘ë ¬ ë¶„ì„ ì‹¤í–‰ ì¤‘...**")
        
        # ë¶„ì„ ì¸ì ì¤€ë¹„
        analysis_args = []
        for agent_key in selected_agents:
            agent_info = BUSINESS_ANALYSIS_AGENTS[agent_key]
            args = (agent_key, agent_info, content_data, model_name, url, enable_thinking)
            analysis_args.append(args)
        
        # ë³‘ë ¬ ì‹¤í–‰
        agent_analyses = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=len(selected_agents)) as executor:
            # ëª¨ë“  ì—ì´ì „íŠ¸ ì‘ì—… ì œì¶œ
            future_to_agent = {
                executor.submit(analyze_with_business_agent, args): args[0] 
                for args in analysis_args
            }
            
            # ì™„ë£Œëœ ì‘ì—… ì²˜ë¦¬
            completed = 0
            for future in concurrent.futures.as_completed(future_to_agent):
                agent_key = future_to_agent[future]
                
                try:
                    result = future.result()
                    agent_analyses.append(result)
                    
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    completed += 1
                    progress = completed / len(selected_agents)
                    
                    agent_progress[agent_key].progress(1.0)
                    
                    if result['success']:
                        agent_status[agent_key].success(f"{result['agent_emoji']} {result['agent_name']}\nâœ… ë¶„ì„ ì™„ë£Œ")
                    else:
                        agent_status[agent_key].error(f"{result['agent_emoji']} {result['agent_name']}\nâŒ ë¶„ì„ ì‹¤íŒ¨")
                    
                except Exception as e:
                    st.error(f"ì—ì´ì „íŠ¸ {agent_key} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        st.success("âœ… **ëª¨ë“  ì—ì´ì „íŠ¸ ë¶„ì„ ì™„ë£Œ**")
        
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸
        st.info("ğŸ“Š **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...**")
        business_analysis = synthesize_business_analysis(url, agent_analyses, content_data, model_name)
        
        if business_analysis['success']:
            st.success("âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ì™„ë£Œ**")
        else:
            st.error("âŒ **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ì‹¤íŒ¨**")
    
    return agent_analyses, business_analysis

def synthesize_business_analysis(url, agent_analyses, content_data, model_name="gpt-4o-mini"):
    """ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ê°€ ëª¨ë“  ì—ì´ì „íŠ¸ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¦¬í¬íŠ¸ ì œì‹œ"""
    
    # ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ì •ë¦¬
    agent_summaries = []
    for analysis in agent_analyses:
        if analysis['success']:
            agent_summaries.append(f"""
**{analysis['agent_name']} ë¶„ì„:**
{analysis['analysis']}
""")
    
    business_prompt = f"""
ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì „ë¬¸ê°€ë“¤ì´ {url} íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆì— ëŒ€í•´ ë¶„ì„í•œ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

**ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë°ì´í„°:**
{json.dumps(content_data, ensure_ascii=False, indent=2)}

**ì „ë¬¸ê°€ ë¶„ì„ ê²°ê³¼:**
{''.join(agent_summaries)}

**ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìš”ì²­ì‚¬í•­:**
1. **Executive Summary** (ì‹¤í–‰ ìš”ì•½)
2. **í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë°œê²¬ì‚¬í•­** (ê° ì „ë¬¸ê°€ ì˜ê²¬ì˜ ê³µí†µì ê³¼ ì°¨ì´ì )
3. **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ í‰ê°€** (ì „ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì˜ ê°•ì ê³¼ ì•½ì )
4. **ì œí’ˆ/ì„œë¹„ìŠ¤ ë¶„ì„** (í•µì‹¬ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ íŠ¹ì§•)
5. **ì‹œì¥ í¬ì§€ì…”ë‹ ë¶„ì„** (ì‹œì¥ì—ì„œì˜ ìœ„ì¹˜ì™€ ê²½ìŸ ìƒí™©)
6. **ì„±ì¥ ì „ëµ ì œì–¸** (êµ¬ì²´ì ì¸ ì„±ì¥ ë°©í–¥)
7. **íˆ¬ì ê°€ì¹˜ í‰ê°€** (1-10ì , ìƒì„¸ ì´ìœ )

**Mermaid ì°¨íŠ¸ í•„ìˆ˜ í¬í•¨:**
- **ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìº”ë²„ìŠ¤**: flowchartë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ êµ¬ì„±ìš”ì†Œ ì‹œê°í™”
- **ì œí’ˆ í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì„**: pie ì°¨íŠ¸ë¡œ ì œí’ˆë³„ ë§¤ì¶œ ë¹„ì¤‘ í‘œì‹œ
- **ì‹œì¥ í¬ì§€ì…”ë‹ ë§µ**: quadrantChartë¡œ ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜ í‘œì‹œ
- **ì„±ì¥ ë¡œë“œë§µ**: timelineìœ¼ë¡œ ë‹¨ê³„ë³„ ì„±ì¥ ê³„íš í‘œì‹œ

**ì¤‘ìš”**: ë°˜ë“œì‹œ 2-3ê°œì˜ Mermaid ì°¨íŠ¸ë¥¼ ```mermaid ì½”ë“œë¸”ë¡ìœ¼ë¡œ í¬í•¨í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ ì œì‹œí•´ì£¼ì„¸ìš”.

íˆ¬ììì™€ ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ì„ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = get_ai_response(
            prompt=business_prompt,
            model_name=model_name,
            system_prompt="ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ì œí’ˆ ë¶„ì„ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.",
            enable_thinking=False
        )
        
        return {
            'content': response['content'],
            'success': response['success'],
            'error': response.get('error', None)
        }
        
    except Exception as e:
        return {
            'content': f"ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            'success': False,
            'error': str(e)
        }

def save_website_analysis(url, website_data, agent_analyses, cto_analysis, analysis_date):
    """ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()

        # 1. website_analyses í…Œì´ë¸”ì— ê¸°ë³¸ ì •ë³´ ì €ì¥
        cursor.execute('''
            INSERT INTO website_analyses 
            (url, analysis_date, title, meta_description, status_code, response_time)
            VALUES (%s, %s, %s, %s, %s, %s)
        ''', (
            url,
            analysis_date,
            website_data.get('title', ''),
            website_data.get('meta_description', ''),
            website_data.get('status_code', 0),
            website_data.get('response_time', 0)
        ))
        analysis_id = cursor.lastrowid

        # 2. website_content_data í…Œì´ë¸”ì— ì½˜í…ì¸  ë°ì´í„° ì €ì¥
        cursor.execute('''
            INSERT INTO website_content_data
            (analysis_id, headings_count, links_count, images_count, text_content_length)
            VALUES (%s, %s, %s, %s, %s)
        ''', (
            analysis_id,
            len(website_data.get('headings', [])),
            len(website_data.get('links', [])),
            len(website_data.get('images', [])),
            len(website_data.get('text_content', ''))
        ))

        # 3. website_agent_analyses í…Œì´ë¸”ì— AI ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ì €ì¥
        for agent_analysis in agent_analyses:
            cursor.execute('''
                INSERT INTO website_agent_analyses
                (analysis_id, agent_type, analysis_content)
                VALUES (%s, %s, %s)
            ''', (
                analysis_id,
                agent_analysis['agent_key'],
                agent_analysis['analysis']
            ))

        # 4. website_cto_analysis í…Œì´ë¸”ì— CTO ì¢…í•© ë¶„ì„ ì €ì¥
        cursor.execute('''
            INSERT INTO website_cto_analysis
            (analysis_id, cto_analysis_content)
            VALUES (%s, %s)
        ''', (
            analysis_id,
            cto_analysis['content']
        ))

        conn.commit()
        cursor.close()
        conn.close()
        return True, analysis_id

    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return False, None

def create_website_analysis_tables():
    """ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()

        # 1. ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ê¸°ë³¸ ì •ë³´ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_analyses (
                analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                url VARCHAR(500) NOT NULL,
                analysis_date DATETIME DEFAULT CURRENT_TIMESTAMP,
                title VARCHAR(500),
                meta_description TEXT,
                status_code INT,
                response_time DECIMAL(10,3),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # 2. ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_content_data (
                content_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                headings_count INT DEFAULT 0,
                links_count INT DEFAULT 0,
                images_count INT DEFAULT 0,
                text_content_length INT DEFAULT 0,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        # 3. ì›¹ì‚¬ì´íŠ¸ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_agent_analyses (
                agent_analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                agent_type VARCHAR(100),
                analysis_content LONGTEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        # 4. CTO ì¢…í•© ë¶„ì„ í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS website_cto_analysis (
                cto_analysis_id INT AUTO_INCREMENT PRIMARY KEY,
                analysis_id INT,
                cto_analysis_content LONGTEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (analysis_id) REFERENCES website_analyses(analysis_id) ON DELETE CASCADE
            )
        ''')

        conn.commit()
        cursor.close()
        conn.close()
        return True

    except mysql.connector.Error as err:
        st.error(f"í…Œì´ë¸” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return False

def get_saved_website_analyses():
    """ì €ì¥ëœ ì›¹ì‚¬ì´íŠ¸ ë¶„ì„ ëª©ë¡ ì¡°íšŒ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True)
        
        cursor.execute('''
            SELECT 
                wa.analysis_id,
                wa.url,
                wa.title,
                wa.analysis_date,
                wa.status_code,
                wa.response_time,
                wcd.headings_count,
                wcd.links_count,
                wcd.images_count
            FROM website_analyses wa
            LEFT JOIN website_content_data wcd ON wa.analysis_id = wcd.analysis_id
            ORDER BY wa.analysis_date DESC
        ''')
        
        results = cursor.fetchall()
        return results
    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
        return []
    finally:
        if 'conn' in locals():
            cursor.close()
            conn.close()

def get_analysis_detail(analysis_id):
    """íŠ¹ì • ë¶„ì„ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor(dictionary=True)
        # 1. ê¸°ë³¸ ì •ë³´
        cursor.execute('''
            SELECT * FROM website_analyses WHERE analysis_id = %s
        ''', (analysis_id,))
        base = cursor.fetchone()
        # 2. ì½˜í…ì¸  ë°ì´í„°
        cursor.execute('''
            SELECT * FROM website_content_data WHERE analysis_id = %s
        ''', (analysis_id,))
        content = cursor.fetchone()
        # 3. ì—ì´ì „íŠ¸ ë¶„ì„
        cursor.execute('''
            SELECT agent_type, analysis_content FROM website_agent_analyses WHERE analysis_id = %s
        ''', (analysis_id,))
        agents = cursor.fetchall()
        # 4. ì¢…í•© ë¦¬í¬íŠ¸
        cursor.execute('''
            SELECT cto_analysis_content FROM website_cto_analysis WHERE analysis_id = %s
        ''', (analysis_id,))
        cto = cursor.fetchone()
        cursor.close()
        conn.close()
        return {
            'base': base,
            'content': content,
            'agents': agents,
            'cto': cto['cto_analysis_content'] if cto else None
        }
    except Exception as e:
        st.error(f"ìƒì„¸ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return None

def delete_analysis(analysis_id):
    """íŠ¹ì • ë¶„ì„ ì‚­ì œ"""
    try:
        conn = mysql.connector.connect(**db_config)
        cursor = conn.cursor()
        
        # ì‚­ì œ ì „ í™•ì¸
        cursor.execute('SELECT COUNT(*) FROM website_analyses WHERE analysis_id = %s', (analysis_id,))
        before_count = cursor.fetchone()[0]
        
        if before_count == 0:
            st.error(f"ë¶„ì„ ID {analysis_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # ì‚­ì œ ì‹¤í–‰
        cursor.execute('DELETE FROM website_analyses WHERE analysis_id = %s', (analysis_id,))
        deleted_rows = cursor.rowcount
        
        if deleted_rows == 0:
            st.error("ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        conn.commit()
        cursor.close()
        conn.close()
        
        st.success(f"âœ… ë¶„ì„ ID {analysis_id} ì‚­ì œ ì™„ë£Œ (ì‚­ì œëœ í–‰: {deleted_rows})")
        return True
        
    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {err}")
        return False
    except Exception as e:
        st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def main():
    # ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
    create_website_analysis_tables()
    
    # íƒ­ ìƒì„±
    tab1, tab2 = st.tabs(["ìƒˆ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„", "ì €ì¥ëœ ë¶„ì„ ì¡°íšŒ"])
    
    with tab1:
        st.header("ğŸ¢ ì›¹ì‚¬ì´íŠ¸ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ë¶„ì„")
        
        # URL ì…ë ¥
        url = st.text_input("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://example.com")
        
        if url:
            # ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìŠ¤í¬ë˜í•‘
            with st.spinner("ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                content_data = scrape_website_content(url)
                
                if content_data:
                    st.success("âœ… ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # ê¸°ë³¸ ì •ë³´ í‘œì‹œ
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ì œëª©", content_data['title'][:50] + "..." if len(content_data['title']) > 50 else content_data['title'])
                        st.metric("ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€", f"{len(content_data['scraped_pages'])}ê°œ")
                        st.metric("ë‹¨ì–´ ìˆ˜", f"{content_data['word_count']:,}")
                    with col2:
                        st.metric("ë¬¸ì¥ ìˆ˜", f"{content_data['sentence_count']:,}")
                        st.metric("ë¬¸ë‹¨ ìˆ˜", f"{content_data['paragraph_count']:,}")
                        st.metric("í—¤ë”© ìˆ˜", len(content_data['headings']))
                    with col3:
                        st.metric("ë¦¬ìŠ¤íŠ¸ ìˆ˜", len(content_data['lists']))
                        st.metric("ì‘ë‹µ ì‹œê°„", f"{content_data['response_time']:.2f}ì´ˆ")
                        st.metric("ìƒíƒœ ì½”ë“œ", content_data['status_code'])
                    with col4:
                        st.metric("ì´ í˜ì´ì§€", content_data['total_pages'])
                        st.metric("í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´", f"{content_data['word_count'] // len(content_data['scraped_pages']):,}")
                        st.metric("ë°ì´í„° í’ˆì§ˆ", "ğŸŸ¢ ìš°ìˆ˜" if content_data['word_count'] > 1000 else "ğŸŸ¡ ë³´í†µ" if content_data['word_count'] > 500 else "ğŸ”´ ë¶€ì¡±")
                    
                    # ê°€ë…ì„± ë¶„ì„
                    readability_data = analyze_content_readability(content_data['text_content'])
                    if readability_data:
                        st.subheader("ğŸ“– ê°€ë…ì„± ë¶„ì„")
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´", f"{readability_data['avg_sentence_length']:.1f}ë‹¨ì–´")
                        with col2:
                            st.metric("ê¸´ ë¬¸ì¥ ë¹„ìœ¨", f"{readability_data['long_sentence_ratio']}%")
                        with col3:
                            st.metric("ê°€ë…ì„± ë“±ê¸‰", readability_data['readability_grade'])
                    
                    # ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°
                    with st.expander("ğŸ“‹ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        st.write("**ë©”íƒ€ ì„¤ëª…:**")
                        st.write(content_data['meta_description'] if content_data['meta_description'] else "ë©”íƒ€ ì„¤ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
                        
                        # ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ëª©ë¡
                        st.write("**ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ëª©ë¡:**")
                        for i, page in enumerate(content_data['scraped_pages'][:10]):  # ì²˜ìŒ 10ê°œë§Œ
                            st.write(f"{i+1}. {page['url']} - {page['title'][:50]}...")
                        
                        if len(content_data['scraped_pages']) > 10:
                            st.write(f"... ì™¸ {len(content_data['scraped_pages']) - 10}ê°œ í˜ì´ì§€")
                        
                        st.write("**ì£¼ìš” í—¤ë”© (ì „ì²´ í˜ì´ì§€ í†µí•©):**")
                        for heading in content_data['headings'][:15]:  # ì²˜ìŒ 15ê°œë§Œ
                            st.write(f"{'  ' * (heading['level'] - 1)}â€¢ {heading['text']}")
                        
                        st.write("**ì½˜í…ì¸  ìƒ˜í”Œ (ì „ì²´ í˜ì´ì§€ í†µí•©, ì²˜ìŒ 1000ì):**")
                        st.write(content_data['text_content'][:1000] + "..." if len(content_data['text_content']) > 1000 else content_data['text_content'])
                        
                        # ì œí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
                        product_keywords = ['ì œí’ˆ', 'ì„œë¹„ìŠ¤', 'ì†”ë£¨ì…˜', 'ìƒí’ˆ', 'ê¸°ëŠ¥', 'íŠ¹ì§•', 'ê°€ê²©', 'êµ¬ë§¤', 'ì£¼ë¬¸', 'ë¬¸ì˜']
                        highlighted_content = content_data['text_content'][:2000]
                        for keyword in product_keywords:
                            if keyword in highlighted_content:
                                st.write(f"**ğŸ” ë°œê²¬ëœ í‚¤ì›Œë“œ:** {keyword}")
                        
                        st.write("**ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë¶„ì„:**")
                        st.write(f"- ì´ ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€: {len(content_data['scraped_pages'])}ê°œ")
                        st.write(f"- í‰ê·  í˜ì´ì§€ë‹¹ ë‹¨ì–´ ìˆ˜: {content_data['word_count'] // len(content_data['scraped_pages']):,}ê°œ")
                        st.write(f"- ì œí’ˆ/ì„œë¹„ìŠ¤ ê´€ë ¨ í—¤ë”©: {len([h for h in content_data['headings'] if any(kw in h['text'] for kw in product_keywords)])}ê°œ")
                    
                    # AI ì—ì´ì „íŠ¸ ì„¤ì •
                    st.header("ğŸ¤– AI ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì „ë¬¸ê°€")
                    st.subheader("í™œì„±í™”í•  ì „ë¬¸ê°€")
                    
                    default_agents = ["business_model_analyzer", "product_service_analyzer", "market_position_analyzer"]
                    selected_agents = []
                    
                    cols = st.columns(3)
                    for i, (agent_key, agent_info) in enumerate(BUSINESS_ANALYSIS_AGENTS.items()):
                        with cols[i % 3]:
                            is_selected = st.checkbox(
                                f"{agent_info['emoji']} **{agent_info['name']}**",
                                value=(agent_key in default_agents),
                                help=agent_info['description'],
                                key=f"business_agent_{agent_key}"
                            )
                            if is_selected:
                                selected_agents.append(agent_key)
                    
                    if len(selected_agents) == 0:
                        st.warning("âš ï¸ ìµœì†Œ 1ëª…ì˜ ì „ë¬¸ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
                    else:
                        st.success(f"âœ… {len(selected_agents)}ëª…ì˜ ì „ë¬¸ê°€ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
                        
                        # ëª¨ë¸ ì„ íƒ
                        model_name = st.selectbox(
                            "ì‚¬ìš©í•  AI ëª¨ë¸",
                            ["gpt-4o-mini", "gpt-4", "claude-3-5-sonnet-20241022"],
                            index=0
                        )
                        
                        # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
                        if st.button("ğŸš€ ë¹„ì¦ˆë‹ˆìŠ¤ ì¢…í•© ë¶„ì„ ì‹¤í–‰"):
                            with st.spinner("AI ì „ë¬¸ê°€ë“¤ì´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                                # ê°€ë…ì„± ë°ì´í„° ì¶”ê°€
                                if readability_data:
                                    content_data['readability'] = readability_data
                                
                                # ë©€í‹°ì—ì´ì „íŠ¸ ë¶„ì„ ì‹¤í–‰
                                agent_analyses, business_analysis = run_business_multi_agent_analysis(
                                    url, 
                                    content_data, 
                                    selected_agents, 
                                    model_name
                                )
                                
                                # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                                st.success("âœ… **ë¹„ì¦ˆë‹ˆìŠ¤ ì¢…í•© ë¶„ì„ ì™„ë£Œ**")
                                
                                # ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                                st.markdown("## ğŸ“‹ ì „ë¬¸ê°€ë³„ ë¶„ì„ ê²°ê³¼")
                                
                                for analysis in agent_analyses:
                                    if analysis['success']:
                                        with st.expander(f"{analysis['agent_emoji']} {analysis['agent_name']} ë¶„ì„", expanded=False):
                                            st.markdown(analysis['analysis'])
                                    else:
                                        st.error(f"âŒ {analysis['agent_emoji']} {analysis['agent_name']}: {analysis['error']}")
                                
                                # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸ í‘œì‹œ
                                st.markdown("## ğŸ“Š ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¢…í•© ë¦¬í¬íŠ¸")
                                
                                if business_analysis['success']:
                                    st.markdown(business_analysis['content'])
                                else:
                                    st.error(f"ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {business_analysis['error']}")
                                
                                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                                analysis_date = datetime.now()
                                save_success, analysis_id = save_website_analysis(
                                    url, content_data, agent_analyses, business_analysis, analysis_date
                                )
                                
                                if save_success:
                                    st.success(f"âœ… ë¶„ì„ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ID: {analysis_id})")
                                else:
                                    st.warning("âš ï¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.error("âŒ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸  ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ë¶„ì„í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with tab2:
        st.header("ì €ì¥ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¡°íšŒ")
        
        # ì €ì¥ëœ ë¶„ì„ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        saved_analyses = get_saved_website_analyses()
        
        if saved_analyses:
            for analysis in saved_analyses:
                with st.expander(f"ğŸ“„ {analysis['url']} - {analysis['analysis_date']}", expanded=False):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.write(f"**ì œëª©:** {analysis['title']}")
                        st.write(f"**ìƒíƒœ ì½”ë“œ:** {analysis['status_code']}")
                        st.write(f"**ì‘ë‹µ ì‹œê°„:** {analysis['response_time']:.2f}ì´ˆ")
                        st.write(f"**í—¤ë”© ìˆ˜:** {analysis['headings_count']}")
                        st.write(f"**ë§í¬ ìˆ˜:** {analysis['links_count']}")
                        
                    with col2:
                        detail_btn = st.button("ğŸ“‹ ìƒì„¸ë³´ê¸°", key=f"detail_{analysis['analysis_id']}")
                        if detail_btn:
                            detail = get_analysis_detail(analysis['analysis_id'])
                            if detail:
                                st.markdown("---")
                                st.subheader("ê¸°ë³¸ ì •ë³´")
                                st.write(detail['base'])
                                st.subheader("ì½˜í…ì¸  ë°ì´í„°")
                                st.write(detail['content'])
                                st.subheader("AI ì—ì´ì „íŠ¸ë³„ ë¶„ì„ ê²°ê³¼")
                                for agent in detail['agents']:
                                    st.markdown(f"**{agent['agent_type']}**")
                                    st.markdown(agent['analysis_content'])
                                st.subheader("ì¢…í•© ë¦¬í¬íŠ¸")
                                st.markdown(detail['cto'] if detail['cto'] else "(ì—†ìŒ)")
                        # ì‚­ì œ ê¸°ëŠ¥ - íšŒì˜ë¡ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •
                        st.markdown("### âš ï¸ ë¶„ì„ ê´€ë¦¬")
                        delete_button_key = f"delete_button_{analysis['analysis_id']}"
                        
                        # ì‚­ì œ í™•ì¸ì„ ìœ„í•œ ì²´í¬ë°•ìŠ¤
                        confirm_delete = st.checkbox(f"ì‚­ì œ í™•ì¸", key=f"confirm_{analysis['analysis_id']}")
                        
                        if confirm_delete:
                            if st.button("ğŸ—‘ï¸ ë¶„ì„ ì‚­ì œ", key=delete_button_key, type="primary", use_container_width=True):
                                if delete_analysis(analysis['analysis_id']):
                                    st.success("âœ… ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    st.rerun()
                                else:
                                    st.error("âŒ ë¶„ì„ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.button("ğŸ—‘ï¸ ë¶„ì„ ì‚­ì œ", key=delete_button_key, disabled=True, use_container_width=True)
                            st.caption("ì‚­ì œí•˜ë ¤ë©´ ë¨¼ì € 'ì‚­ì œ í™•ì¸' ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
        else:
            st.info("ì €ì¥ëœ ì½˜í…ì¸  ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 