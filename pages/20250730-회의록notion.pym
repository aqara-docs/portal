import streamlit as st
import requests
import json
import pandas as pd
from datetime import datetime
import os
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv
import openai

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Notion ë°ì´í„° ì½ê¸°",
    page_icon="ğŸ“",
    layout="wide"
)

# ì‚¬ì´ë“œë°” ì„¤ì •
st.sidebar.title("âš™ï¸ ì„¤ì •")

# Notion API ì„¤ì • (.envì—ì„œ ì½ì–´ì˜¤ê¸°)
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
NOTION_DB_URL = os.getenv("NOTION_DB_URL")

class LLMClient:
    """ë‹¤ì–‘í•œ LLM í´ë¼ì´ì–¸íŠ¸ë¥¼ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM í´ë¼ì´ì–¸íŠ¸ë“¤ì„ ì„¤ì •"""
        # OpenAI í´ë¼ì´ì–¸íŠ¸ (ê¸°ë³¸)
        openai_key = os.getenv('OPENAI_API_KEY')
        if openai_key:
            try:
                self.clients['openai'] = openai.OpenAI(api_key=openai_key)
                self.models['openai'] = [
                    'gpt-4o-mini',
                    'gpt-4o',
                    'gpt-4-turbo',
                    'gpt-4',
                    'gpt-3.5-turbo'
                ]
            except Exception as e:
                pass
        
        # Ollama í´ë¼ì´ì–¸íŠ¸ (ë¡œì»¬ LLM) - ì„ íƒì 
        try:
            import requests
            # Ollama ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ (ì§§ì€ íƒ€ì„ì•„ì›ƒ)
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                self.clients['ollama'] = requests
                self.models['ollama'] = [
                    'mistral:latest',
                    'llama3.1:latest',
                    'llama3.1:8b',
                    'phi4:latest',
                    'llama2:latest',
                    'gemma2:latest',
                    'gemma:latest',
                    'llama3.2:latest',
                    'deepseek-r1:14b',
                    'nomic-embed-text:latest'
                ]
        except Exception as e:
            # Ollama ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë¬´ì‹œ
            pass
        
        # Perplexity í´ë¼ì´ì–¸íŠ¸
        perplexity_key = os.getenv('PERPLEXITY_API_KEY')
        if perplexity_key:
            try:
                self.clients['perplexity'] = openai.OpenAI(
                    api_key=perplexity_key,
                    base_url="https://api.perplexity.ai"
                )
                self.models['perplexity'] = [
                    "sonar-pro",
                    "sonar-small-chat"
                ]
            except Exception as e:
                pass
        
        # Anthropic í´ë¼ì´ì–¸íŠ¸ (Claude)
        anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        if anthropic_api_key:
            try:
                from langchain_anthropic import ChatAnthropic
                self.clients['anthropic'] = ChatAnthropic(
                    model="claude-3-5-sonnet-20241022",
                    anthropic_api_key=anthropic_api_key,
                    temperature=0.1,
                    max_tokens=4000
                )
                self.models['anthropic'] = [
                    'claude-3-7-sonnet-latest',
                    'claude-3-5-sonnet-20241022',
                    'claude-3-5-haiku-20241022',
                    'claude-3-5-sonnet-20241022',
                    'claude-3-haiku-20240307'
                ]
            except Exception as e:
                pass
        
        # Google í´ë¼ì´ì–¸íŠ¸ (Gemini)
        google_api_key = os.getenv('GOOGLE_API_KEY')
        if google_api_key:
            try:
                import google.generativeai as genai
                genai.configure(api_key=google_api_key)
                self.clients['google'] = genai
                self.models['google'] = [
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-1.0-pro'
                ]
            except Exception as e:
                pass
    
    def get_available_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡ ë°˜í™˜"""
        return list(self.clients.keys())
    
    def get_models_for_provider(self, provider):
        """íŠ¹ì • ì œê³µìì˜ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
        return self.models.get(provider, [])
    
    def generate_response(self, provider, model, messages, temperature=0.7, max_tokens=2000):
        """ì„ íƒëœ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            if provider not in self.clients:
                return None, f"í´ë¼ì´ì–¸íŠ¸ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ì œê³µì: {provider}"
            
            if provider == 'ollama':
                return self._generate_ollama_response(model, messages, temperature, max_tokens)
            elif provider == 'openai':
                return self._generate_openai_response(model, messages, temperature, max_tokens)
            elif provider == 'perplexity':
                return self._generate_perplexity_response(model, messages, temperature, max_tokens)
            elif provider == 'anthropic':
                return self._generate_anthropic_response(model, messages, temperature, max_tokens)
            elif provider == 'google':
                return self._generate_google_response(model, messages, temperature, max_tokens)
            else:
                return None, f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}"
        except Exception as e:
            return None, f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_ollama_response(self, model, messages, temperature, max_tokens):
        """Ollama ì‘ë‹µ ìƒì„±"""
        try:
            # Ollama API í˜•ì‹ì— ë§ê²Œ ë©”ì‹œì§€ ë³€í™˜
            ollama_messages = []
            for msg in messages:
                if msg['role'] == 'system':
                    continue
                elif msg['role'] == 'user':
                    ollama_messages.append({
                        'role': 'user',
                        'content': msg['content']
                    })
                elif msg['role'] == 'assistant':
                    ollama_messages.append({
                        'role': 'assistant',
                        'content': msg['content']
                    })
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš©ì ë©”ì‹œì§€ì— í¬í•¨
            system_content = ""
            for msg in messages:
                if msg['role'] == 'system':
                    system_content = msg['content']
                    break
            
            if system_content and ollama_messages:
                ollama_messages[0]['content'] = f"{system_content}\n\n{ollama_messages[0]['content']}"
            
            # Ollama API í˜¸ì¶œ
            response = self.clients['ollama'].post(
                "http://localhost:11434/api/chat",
                json={
                    "model": model,
                    "messages": ollama_messages,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['message']['content'], None
            else:
                return None, f"Ollama API ì˜¤ë¥˜: {response.status_code}"
                
        except Exception as e:
            return None, f"Ollama ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_openai_response(self, model, messages, temperature, max_tokens):
        """OpenAI ì‘ë‹µ ìƒì„±"""
        try:
            response = self.clients['openai'].chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content, None
        except Exception as e:
            return None, f"OpenAI ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_perplexity_response(self, model, messages, temperature, max_tokens):
        """Perplexity ì‘ë‹µ ìƒì„±"""
        try:
            response = self.clients['perplexity'].chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return response.choices[0].message.content, None
        except Exception as e:
            return None, f"Perplexity ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_anthropic_response(self, model, messages, temperature, max_tokens):
        """Anthropic ì‘ë‹µ ìƒì„±"""
        try:
            # LangChain ChatAnthropicì„ ì‚¬ìš©
            response = self.clients['anthropic'].invoke(messages)
            return response.content, None
        except Exception as e:
            return None, f"Anthropic ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_google_response(self, model, messages, temperature, max_tokens):
        """Google ì‘ë‹µ ìƒì„±"""
        try:
            # Google Gemini API ì‚¬ìš©
            model_obj = self.clients['google'].GenerativeModel(model)
            # ë©”ì‹œì§€ë¥¼ Gemini í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            prompt = ""
            for msg in messages:
                if msg['role'] == 'user':
                    prompt += f"User: {msg['content']}\n"
                elif msg['role'] == 'assistant':
                    prompt += f"Assistant: {msg['content']}\n"
                elif msg['role'] == 'system':
                    prompt = f"System: {msg['content']}\n" + prompt
            
            response = model_obj.generate_content(prompt)
            return response.text, None
        except Exception as e:
            return None, f"Google ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}"

# Database ID ì¶”ì¶œ í•¨ìˆ˜
def extract_database_id(url: str) -> str:
    """Notion URLì—ì„œ Database IDë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    import re
    
    # ë‹¤ì–‘í•œ URL íŒ¨í„´ì—ì„œ Database ID ì¶”ì¶œ
    patterns = [
        r'notion\.so/workspace/([a-zA-Z0-9]{32})',
        r'notion\.so/([a-zA-Z0-9]{32})',
        r'notion\.so/workspace/([a-zA-Z0-9]{32})\?',
        r'notion\.so/([a-zA-Z0-9]{32})\?'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    
    return None

# Database ID ì¶”ì¶œ
DATABASE_ID = None
if NOTION_DB_URL:
    DATABASE_ID = extract_database_id(NOTION_DB_URL)
else:
    pass

# ìˆ˜ë™ Database ID ì…ë ¥ (ë°±ì—…ìš©)
MANUAL_DATABASE_ID = st.sidebar.text_input(
    "Database ID (ìˆ˜ë™ ì…ë ¥)",
    help="ìë™ ì¶”ì¶œì´ ì‹¤íŒ¨í•œ ê²½ìš° ìˆ˜ë™ìœ¼ë¡œ Database IDë¥¼ ì…ë ¥í•˜ì„¸ìš”."
)

# ìµœì¢… Database ID ê²°ì •
FINAL_DATABASE_ID = DATABASE_ID or MANUAL_DATABASE_ID

if NOTION_API_KEY:
    pass
else:
    pass

# ë©”ì¸ í•¨ìˆ˜ë“¤
def test_notion_connection(api_key: str) -> Dict:
    """Notion API ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    url = "https://api.notion.com/v1/users/me"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return {"success": True, "data": response.json()}
    except requests.exceptions.RequestException as e:
        return {"success": False, "error": str(e), "status_code": response.status_code if 'response' in locals() else None}

def get_notion_database(database_id: str, api_key: str, start_date: str = None, end_date: str = None) -> Dict:
    """Notion ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/databases/{database_id}/query"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    # Notion APIì—ì„œëŠ” created_timeì„ ì†ì„± í•„í„°ë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ
    # ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¨ í›„ í´ë¼ì´ì–¸íŠ¸ì—ì„œ í•„í„°ë§
    try:
        response = requests.post(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"API ìš”ì²­ ì˜¤ë¥˜: {e}")
        if hasattr(e, 'response') and e.response is not None:
            st.error(f"ìƒíƒœ ì½”ë“œ: {e.response.status_code}")
            try:
                error_detail = e.response.json()
                st.error(f"ì˜¤ë¥˜ ìƒì„¸: {error_detail}")
            except:
                st.error(f"ì‘ë‹µ ë‚´ìš©: {e.response.text}")
        return None

def parse_notion_properties(properties: Dict) -> Dict:
    """Notion ì†ì„±ë“¤ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
    parsed = {}
    
    for key, value in properties.items():
        prop_type = value.get("type")
        
        if prop_type == "title":
            title_content = value.get("title", [])
            if title_content:
                parsed[key] = title_content[0].get("plain_text", "")
            else:
                parsed[key] = ""
                
        elif prop_type == "rich_text":
            rich_text_content = value.get("rich_text", [])
            if rich_text_content:
                parsed[key] = rich_text_content[0].get("plain_text", "")
            else:
                parsed[key] = ""
                
        elif prop_type == "number":
            parsed[key] = value.get("number")
            
        elif prop_type == "select":
            select_value = value.get("select")
            if select_value:
                parsed[key] = select_value.get("name", "")
            else:
                parsed[key] = ""
                
        elif prop_type == "multi_select":
            multi_select_values = value.get("multi_select", [])
            parsed[key] = ", ".join([item.get("name", "") for item in multi_select_values])
            
        elif prop_type == "date":
            date_value = value.get("date")
            if date_value:
                parsed[key] = date_value.get("start", "")
            else:
                parsed[key] = ""
                
        elif prop_type == "checkbox":
            parsed[key] = value.get("checkbox", False)
            
        elif prop_type == "url":
            parsed[key] = value.get("url", "")
            
        elif prop_type == "email":
            parsed[key] = value.get("email", "")
            
        elif prop_type == "phone_number":
            parsed[key] = value.get("phone_number", "")
            
        else:
            parsed[key] = str(value)
    
    return parsed

def convert_to_dataframe(notion_data: Dict, start_date: str = None, end_date: str = None) -> pd.DataFrame:
    """Notion ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  ë‚ ì§œ í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤."""
    if not notion_data or "results" not in notion_data:
        return pd.DataFrame()
    
    rows = []
    for page in notion_data["results"]:
        properties = page.get("properties", {})
        parsed_properties = parse_notion_properties(properties)
        
        # í˜ì´ì§€ IDì™€ ìƒì„±/ìˆ˜ì • ì‹œê°„ ì¶”ê°€
        parsed_properties["page_id"] = page.get("id", "")
        parsed_properties["created_time"] = page.get("created_time", "")
        parsed_properties["last_edited_time"] = page.get("last_edited_time", "")
        
        rows.append(parsed_properties)
    
    df = pd.DataFrame(rows)
    
    # ë‚ ì§œ í•„í„°ë§ ì ìš©
    if not df.empty and (start_date or end_date):
        try:
            # created_timeì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  í•œêµ­ ì‹œê°„ëŒ€(KST)ë¡œ ë³€í™˜
            df['created_time_dt'] = pd.to_datetime(df['created_time']).dt.tz_convert('Asia/Seoul')
        except:
            # ì‹œê°„ëŒ€ ë³€í™˜ì— ì‹¤íŒ¨í•˜ë©´ UTCë¡œ ì²˜ë¦¬í•˜ê³  9ì‹œê°„ ì¶”ê°€ (KST = UTC+9)
            df['created_time_dt'] = pd.to_datetime(df['created_time']) + pd.Timedelta(hours=9)
        
        # ë‚ ì§œë§Œ ë¹„êµí•˜ë„ë¡ ì‹œê°„ ì •ë³´ ì œê±° (ë” ì•ˆì „í•œ ë°©ë²•)
        df['created_time_date'] = df['created_time_dt'].dt.date
        
        if start_date and end_date:
            start_dt = pd.to_datetime(start_date).date()
            end_dt = pd.to_datetime(end_date).date()
            mask = (df['created_time_date'] >= start_dt) & (df['created_time_date'] <= end_dt)
        elif start_date:
            start_dt = pd.to_datetime(start_date).date()
            mask = df['created_time_date'] >= start_dt
        elif end_date:
            end_dt = pd.to_datetime(end_date).date()
            mask = df['created_time_date'] <= end_dt
        else:
            mask = pd.Series([True] * len(df), index=df.index)
        
        df = df[mask]
        
        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        if 'created_time_dt' in df.columns:
            df = df.drop('created_time_dt', axis=1)
        if 'created_time_date' in df.columns:
            df = df.drop('created_time_date', axis=1)
    
    return df

def export_data(df: pd.DataFrame, format_type: str):
    """ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    if df.empty:
        return
    
    if format_type == "CSV":
        csv = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=f"notion_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    elif format_type == "Excel":
        output = pd.ExcelWriter('temp.xlsx', engine='openpyxl')
        df.to_excel(output, index=False, sheet_name='Notion Data')
        output.close()
        
        with open('temp.xlsx', 'rb') as f:
            excel_data = f.read()
        
        st.download_button(
            label="Excel ë‹¤ìš´ë¡œë“œ",
            data=excel_data,
            file_name=f"notion_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if os.path.exists('temp.xlsx'):
            os.remove('temp.xlsx')

def get_notion_page_content(page_id: str, api_key: str) -> Dict:
    """Notion í˜ì´ì§€ì˜ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/pages/{page_id}"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return None

def get_notion_page_blocks(page_id: str, api_key: str) -> List[Dict]:
    """Notion í˜ì´ì§€ì˜ ë¸”ë¡ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Notion-Version": "2022-06-28",
        "Content-Type": "application/json"
    }
    
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.json().get("results", [])
    except requests.exceptions.RequestException as e:
        st.error(f"ë¸”ë¡ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return []

def parse_notion_blocks(blocks: List[Dict], api_key: str = None) -> str:
    """Notion ë¸”ë¡ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    content = []
    
    for block in blocks:
        block_type = block.get("type")
        
        if block_type == "paragraph":
            rich_text = block.get("paragraph", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(text)
                    
        elif block_type == "heading_1":
            rich_text = block.get("heading_1", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"# {text}")
                    
        elif block_type == "heading_2":
            rich_text = block.get("heading_2", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"## {text}")
                    
        elif block_type == "heading_3":
            rich_text = block.get("heading_3", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"### {text}")
                    
        elif block_type == "bulleted_list_item":
            rich_text = block.get("bulleted_list_item", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"â€¢ {text}")
                    
        elif block_type == "numbered_list_item":
            rich_text = block.get("numbered_list_item", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"1. {text}")
                    
        elif block_type == "quote":
            rich_text = block.get("quote", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"> {text}")
                    
        elif block_type == "code":
            rich_text = block.get("code", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"```\n{text}\n```")
                    
        elif block_type == "divider":
            content.append("---")
            
        elif block_type == "table_of_contents":
            content.append("[ëª©ì°¨]")
            
        elif block_type == "callout":
            rich_text = block.get("callout", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    icon = block.get("callout", {}).get("icon", {}).get("emoji", "ğŸ’¡")
                    content.append(f"{icon} {text}")
                    
        elif block_type == "toggle":
            rich_text = block.get("toggle", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    content.append(f"â–¶ {text}")
                    
        elif block_type == "to_do":
            rich_text = block.get("to_do", {}).get("rich_text", [])
            if rich_text:
                text = "".join([rt.get("plain_text", "") for rt in rich_text])
                if text.strip():
                    checked = block.get("to_do", {}).get("checked", False)
                    checkbox = "â˜‘" if checked else "â˜"
                    content.append(f"{checkbox} {text}")
                    
        elif block_type == "synced_block":
            # ë™ê¸°í™”ëœ ë¸”ë¡ì€ í•˜ìœ„ ë¸”ë¡ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì²˜ë¦¬
            children = block.get("synced_block", {}).get("children", [])
            if children:
                child_content = parse_notion_blocks(children, api_key)
                if child_content.strip():
                    content.append(child_content)
                    
        elif block_type == "child_database":
            # í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë§í¬ë¡œ í‘œì‹œ
            database_id = block.get("child_database", {}).get("id", "")
            if database_id:
                content.append(f"[ğŸ“Š í•˜ìœ„ ë°ì´í„°ë² ì´ìŠ¤](https://notion.so/{database_id})")
                
        elif block_type == "child_page":
            # í•˜ìœ„ í˜ì´ì§€ì˜ ì‹¤ì œ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ í‘œì‹œ
            page_id = block.get("child_page", {}).get("id", "")
            if page_id and api_key:
                # í•˜ìœ„ í˜ì´ì§€ì˜ ì œëª© ê°€ì ¸ì˜¤ê¸°
                page_info = get_notion_page_content(page_id, api_key)
                if page_info:
                    properties = page_info.get("properties", {})
                    title = ""
                    
                    # ì œëª© ì†ì„± ì°¾ê¸°
                    for prop_name, prop_value in properties.items():
                        if prop_value.get("type") == "title":
                            title_content = prop_value.get("title", [])
                            if title_content:
                                title = title_content[0].get("plain_text", "")
                                break
                    
                    if title:
                        content.append(f"## ğŸ“„ {title}")
                        
                        # í•˜ìœ„ í˜ì´ì§€ì˜ ë¸”ë¡ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                        child_blocks = get_notion_page_blocks(page_id, api_key)
                        if child_blocks:
                            child_content = parse_notion_blocks(child_blocks, api_key)
                            if child_content.strip():
                                content.append(child_content)
                            else:
                                content.append("*[ë‚´ìš© ì—†ìŒ]*")
                        else:
                            content.append("*[ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ]*")
                    else:
                        content.append(f"[ğŸ“„ í•˜ìœ„ í˜ì´ì§€](https://notion.so/{page_id})")
                else:
                    content.append(f"[ğŸ“„ í•˜ìœ„ í˜ì´ì§€](https://notion.so/{page_id})")
            else:
                content.append(f"[ğŸ“„ í•˜ìœ„ í˜ì´ì§€](https://notion.so/{page_id})")
                
        elif block_type == "embed":
            # ì„ë² ë“œëœ ì½˜í…ì¸ 
            url = block.get("embed", {}).get("url", "")
            if url:
                content.append(f"[ğŸ”— ì„ë² ë“œ ë§í¬]({url})")
                
        elif block_type == "image":
            # ì´ë¯¸ì§€
            url = block.get("image", {}).get("file", {}).get("url", "")
            if url:
                content.append(f"![ì´ë¯¸ì§€]({url})")
                
        elif block_type == "video":
            # ë¹„ë””ì˜¤
            url = block.get("video", {}).get("file", {}).get("url", "")
            if url:
                content.append(f"ğŸ¥ [ë¹„ë””ì˜¤]({url})")
                
        elif block_type == "file":
            # íŒŒì¼
            url = block.get("file", {}).get("file", {}).get("url", "")
            if url:
                content.append(f"ğŸ“ [íŒŒì¼]({url})")
                
        elif block_type == "pdf":
            # PDF
            url = block.get("pdf", {}).get("file", {}).get("url", "")
            if url:
                content.append(f"ğŸ“„ [PDF]({url})")
                
        elif block_type == "bookmark":
            # ë¶ë§ˆí¬
            url = block.get("bookmark", {}).get("url", "")
            if url:
                content.append(f"ğŸ”– [ë¶ë§ˆí¬]({url})")
                
        elif block_type == "equation":
            # ìˆ˜ì‹
            expression = block.get("equation", {}).get("expression", "")
            if expression:
                content.append(f"$${expression}$$")
                
        elif block_type == "table":
            # í…Œì´ë¸” (ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ)
            content.append("[ğŸ“Š í…Œì´ë¸”]")
            
        elif block_type == "column_list":
            # ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            content.append("[ğŸ“‹ ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ]")
            
        elif block_type == "column":
            # ì»¬ëŸ¼
            content.append("[ğŸ“‹ ì»¬ëŸ¼]")
            
        elif block_type == "template":
            # í…œí”Œë¦¿
            content.append("[ğŸ“‹ í…œí”Œë¦¿]")
            
        elif block_type == "link_preview":
            # ë§í¬ ë¯¸ë¦¬ë³´ê¸°
            url = block.get("link_preview", {}).get("url", "")
            if url:
                content.append(f"ğŸ”— [ë§í¬ ë¯¸ë¦¬ë³´ê¸°]({url})")
                
        elif block_type == "unsupported":
            # ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸”ë¡
            content.append("[âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ë¸”ë¡]")
            
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ë¸”ë¡ íƒ€ì…
            content.append(f"[â“ ì•Œ ìˆ˜ ì—†ëŠ” ë¸”ë¡: {block_type}]")
    
    return "\n\n".join(content)

def get_meeting_content_for_search(df: pd.DataFrame, api_key: str) -> pd.DataFrame:
    """ê²€ìƒ‰ì„ ìœ„í•´ íšŒì˜ë¡ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ë°ì´í„°í”„ë ˆì„ì— ì¶”ê°€í•©ë‹ˆë‹¤."""
    if df.empty:
        return df
    
    # íšŒì˜ë¡ í•­ëª©ë“¤ë§Œ í•„í„°ë§ (ì´ë¦„ì´ ìˆëŠ” í•­ëª©ë“¤)
    meeting_items = df[df['ì´ë¦„'].notna() & (df['ì´ë¦„'] != '')]
    
    if meeting_items.empty:
        return df
    
    # ë‚´ìš© ì»¬ëŸ¼ ì¶”ê°€
    df_with_content = df.copy()
    df_with_content['íšŒì˜_ë‚´ìš©'] = ""
    
    # ê° íšŒì˜ë¡ì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ì¶”ê°€
    for idx, row in meeting_items.iterrows():
        page_id = row['page_id']
        try:
            blocks = get_notion_page_blocks(page_id, api_key)
            if blocks:
                content = parse_notion_blocks(blocks, api_key)
                df_with_content.at[idx, 'íšŒì˜_ë‚´ìš©'] = content
        except Exception as e:
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
            continue
    
    return df_with_content

def display_meeting_details(df: pd.DataFrame, api_key: str):
    """íšŒì˜ë¡ ìƒì„¸ ë‚´ìš©ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    if df.empty:
        return
    
    # íšŒì˜ë¡ í•­ëª©ë“¤ë§Œ í•„í„°ë§ (ì´ë¦„ì´ ìˆëŠ” í•­ëª©ë“¤)
    meeting_items = df[df['ì´ë¦„'].notna() & (df['ì´ë¦„'] != '')]
    
    if meeting_items.empty:
        return
    
    st.subheader("ğŸ“‹ íšŒì˜ë¡ ìƒì„¸ ë‚´ìš©")
    
    # íšŒì˜ë¡ ì„ íƒ
    selected_meeting = st.selectbox(
        "íšŒì˜ë¡ ì„ íƒ",
        options=meeting_items['ì´ë¦„'].tolist(),
        index=0,
        help="ë¶„ì„í•  íšŒì˜ë¡ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    if selected_meeting:
        # ì„ íƒëœ íšŒì˜ë¡ì˜ í˜ì´ì§€ ID ì°¾ê¸°
        selected_row = meeting_items[meeting_items['ì´ë¦„'] == selected_meeting].iloc[0]
        page_id = selected_row['page_id']
        
        pass
        
        # í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        with st.spinner("íšŒì˜ë¡ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            blocks = get_notion_page_blocks(page_id, api_key)
            
            if blocks:
                # ë¸”ë¡ íƒ€ì… ë¶„ì„
                block_types = [block.get("type", "unknown") for block in blocks]
                type_counts = {}
                for block_type in block_types:
                    type_counts[block_type] = type_counts.get(block_type, 0) + 1
                
                content = parse_notion_blocks(blocks, api_key)
                
                if content.strip():
                    st.markdown("---")
                    st.markdown("### ğŸ“ íšŒì˜ë¡ ë‚´ìš©")
                    st.markdown(content)
                    
                    # LLM ë¶„ì„ ê¸°ëŠ¥
                    st.markdown("---")
                    st.subheader("ğŸ¤– LLM ë¶„ì„")
                    
                    # ë¶„ì„ ë²”ìœ„ ì„ íƒ
                    analysis_scope = st.radio(
                        "ë¶„ì„ ë²”ìœ„ ì„ íƒ",
                        ["í˜„ì¬ ì„ íƒëœ íšŒì˜ë¡", "ì „ì²´ íšŒì˜ë¡ (ê¸°ê°„ë³„ ê°€ì ¸ì˜¨ ëª¨ë“  íšŒì˜ë¡)"],
                        help="ë¶„ì„í•  íšŒì˜ë¡ì˜ ë²”ìœ„ë¥¼ ì„ íƒí•˜ì„¸ìš”"
                    )
                    
                    # LLM ì„ íƒ
                    llm_client = LLMClient()
                    available_providers = llm_client.get_available_providers()
                    
                    if available_providers:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            selected_provider = st.selectbox(
                                "ğŸ¤– LLM ì œê³µì ì„ íƒ",
                                available_providers,
                                help="ë¶„ì„ì— ì‚¬ìš©í•  LLM ì œê³µìë¥¼ ì„ íƒí•˜ì„¸ìš”"
                            )
                        
                        with col2:
                            available_models = llm_client.get_models_for_provider(selected_provider)
                            if available_models:
                                selected_model = st.selectbox(
                                    "ğŸ“‹ ëª¨ë¸ ì„ íƒ",
                                    available_models,
                                    help="ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”"
                                )
                            else:
                                selected_model = None
                                st.warning(f"{selected_provider}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        selected_provider = "openai"
                        selected_model = "gpt-4o-mini"
                    
                    # ì°¸ê³  í”„ë¡¬í”„íŠ¸ ì…ë ¥
                    reference_prompt = st.text_area(
                        "ğŸ“ LLM ë¶„ì„ ì‹œ ì°¸ê³ í•  í”„ë¡¬í”„íŠ¸",
                        placeholder="ì˜ˆì‹œ:\n- íŠ¹ì • í‚¤ì›Œë“œë‚˜ ì£¼ì œì— ì§‘ì¤‘í•´ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”\n- íŠ¹ì • ì°¸ì„ìì˜ ë°œì–¸ì„ ì¤‘ì ì ìœ¼ë¡œ ì‚´í´ë³´ì„¸ìš”\n- íŠ¹ì • ê¸°ê°„ì˜ ì§„í–‰ ìƒí™©ì„ ì¶”ì í•´ì£¼ì„¸ìš”\n- íŠ¹ì • í”„ë¡œì íŠ¸ë‚˜ ì´ìŠˆì— ëŒ€í•œ ë…¼ì˜ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                        help="LLMì´ ë¶„ì„í•  ë•Œ ë°˜ë“œì‹œ ì°¸ê³ í•  ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”. ë¹„ì›Œë‘ë©´ ì¼ë°˜ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
                        height=100
                    )
                    
                    # ë¶„ì„ ì˜µì…˜
                    analysis_type = st.selectbox(
                        "ë¶„ì„ ìœ í˜• ì„ íƒ",
                        [
                            "íšŒì˜ë¡ ìš”ì•½",
                            "ì£¼ìš” ë…¼ì˜ ì‚¬í•­ ì¶”ì¶œ",
                            "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ",
                            "ê²°ì • ì‚¬í•­ ì •ë¦¬",
                            "ì°¸ì„ìë³„ ì—­í•  ë¶„ì„",
                            "ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸"
                        ],
                        help="ì›í•˜ëŠ” ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”"
                    )
                    
                    if st.button("ğŸ” LLM ë¶„ì„ ì‹œì‘", type="primary"):
                        with st.spinner("LLMì´ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
                            if analysis_scope == "í˜„ì¬ ì„ íƒëœ íšŒì˜ë¡":
                                # í˜„ì¬ ì„ íƒëœ íšŒì˜ë¡ë§Œ ë¶„ì„
                                analysis_result = analyze_with_llm(content, analysis_type, False, reference_prompt, selected_provider, selected_model)
                                file_name = f"{selected_meeting}_ë¶„ì„ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                            else:
                                # ì „ì²´ íšŒì˜ë¡ ë¶„ì„
                                all_contents = get_all_meeting_contents(df, api_key)
                                if all_contents:
                                    analysis_result = analyze_with_llm(all_contents, analysis_type, True, reference_prompt, selected_provider, selected_model)
                                    file_name = f"ì „ì²´íšŒì˜ë¡_ë¶„ì„ê²°ê³¼_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
                                else:
                                    analysis_result = "âŒ ë¶„ì„í•  íšŒì˜ë¡ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                    file_name = ""
                            
                            if analysis_result and not analysis_result.startswith("âŒ"):
                                st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
                                st.markdown(analysis_result)
                                
                                # ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
                                if file_name:
                                    st.download_button(
                                        label="ğŸ“„ ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                                        data=analysis_result,
                                        file_name=file_name,
                                        mime="text/markdown"
                                    )
                            else:
                                st.error(analysis_result)
                    
                    # ì›ë³¸ JSON ë°ì´í„° (ë””ë²„ê¹…ìš©)
                    with st.expander("ğŸ”§ ì›ë³¸ ë°ì´í„° ë³´ê¸°"):
                        st.json(blocks)
                        
                    # ë¸”ë¡ íƒ€ì…ë³„ ìƒì„¸ ì •ë³´
                    with st.expander("ğŸ” ë¸”ë¡ íƒ€ì…ë³„ ìƒì„¸ ì •ë³´"):
                        for i, block in enumerate(blocks):
                            block_type = block.get("type", "unknown")
                            st.write(f"**ë¸”ë¡ {i+1}**: {block_type}")
                            if block_type in ["paragraph", "heading_1", "heading_2", "heading_3"]:
                                rich_text = block.get(block_type, {}).get("rich_text", [])
                                if rich_text:
                                    text = "".join([rt.get("plain_text", "") for rt in rich_text])
                                    st.write(f"  ë‚´ìš©: {text}")
                            st.write("---")
                else:
                    # ë””ë²„ê¹…ì„ ìœ„í•´ ì›ë³¸ ë°ì´í„° í‘œì‹œ
                    with st.expander("ğŸ”§ ë””ë²„ê¹…: ì›ë³¸ ë¸”ë¡ ë°ì´í„°"):
                        st.json(blocks)
            else:
                # í˜ì´ì§€ ì •ë³´ í™•ì¸
                page_info = get_notion_page_content(page_id, api_key)
                if page_info:
                    with st.expander("ğŸ”§ í˜ì´ì§€ ì •ë³´"):
                        st.json(page_info)

def analyze_with_llm(content: str, analysis_type: str, is_multiple_meetings: bool = False, reference_prompt: str = "", provider: str = "openai", model: str = None) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ íšŒì˜ë¡ì„ ë¶„ì„í•©ë‹ˆë‹¤."""
    try:
        # LLMClient ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        llm_client = LLMClient()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì í™•ì¸
        available_providers = llm_client.get_available_providers()
        if not available_providers:
            return "âŒ ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
        
        # ì œê³µìê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ì‚¬ìš©
        if provider not in available_providers:
            provider = available_providers[0]
        
        # ëª¨ë¸ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        if not model:
            available_models = llm_client.get_models_for_provider(provider)
            if available_models:
                model = available_models[0]
            else:
                return f"âŒ {provider}ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if is_multiple_meetings:
            prompts = {
                "íšŒì˜ë¡ ìš”ì•½": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”. ê° íšŒì˜ì˜ ì£¼ìš” ë‚´ìš©ê³¼ ì „ì²´ì ì¸ íë¦„ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì£¼ìš” ë…¼ì˜ ì‚¬í•­ ì¶”ì¶œ": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì—ì„œ ì£¼ìš” ë…¼ì˜ ì‚¬í•­ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê³µí†µ ì£¼ì œì™€ ê° íšŒì˜ë³„ íŠ¹ì´ì‚¬í•­ì„ êµ¬ë¶„í•´ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì—ì„œ ì•¡ì…˜ ì•„ì´í…œë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ë‹´ë‹¹ìì™€ ê¸°í•œì´ ìˆë‹¤ë©´ í•¨ê»˜ í‘œì‹œí•˜ê³ , ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ì£¼ì„¸ìš”.",
                "ê²°ì • ì‚¬í•­ ì •ë¦¬": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì—ì„œ ê²°ì •ëœ ì‚¬í•­ë“¤ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”. ê° ê²°ì • ì‚¬í•­ì˜ ë°°ê²½ê³¼ ì§„í–‰ ìƒí™©ì„ í¬í•¨í•´ì£¼ì„¸ìš”.",
                "ì°¸ì„ìë³„ ì—­í•  ë¶„ì„": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì—ì„œ ì°¸ì„ìë“¤ì˜ ì—­í• ê³¼ ê¸°ì—¬ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. ê°ìì˜ ì±…ì„ê³¼ ì—­í•  ë³€í™”ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸": "ë‹¤ìŒ ì—¬ëŸ¬ íšŒì˜ë¡ë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. ì „ì²´ì ì¸ ìš”ì•½, ì£¼ìš” ë…¼ì˜ ì‚¬í•­, ì•¡ì…˜ ì•„ì´í…œ, ê²°ì • ì‚¬í•­, ì°¸ì„ì ì—­í• ì„ ëª¨ë‘ í¬í•¨í•œ ì¢…í•© ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
            }
        else:
            prompts = {
                "íšŒì˜ë¡ ìš”ì•½": "ë‹¤ìŒ íšŒì˜ë¡ì„ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. ì£¼ìš” ë‚´ìš©ì„ 3-4ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì£¼ìš” ë…¼ì˜ ì‚¬í•­ ì¶”ì¶œ": "ë‹¤ìŒ íšŒì˜ë¡ì—ì„œ ì£¼ìš” ë…¼ì˜ ì‚¬í•­ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ê° í•­ëª©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•´ì„œ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ": "ë‹¤ìŒ íšŒì˜ë¡ì—ì„œ ì•¡ì…˜ ì•„ì´í…œ(í•´ì•¼ í•  ì¼)ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ë‹´ë‹¹ìì™€ ê¸°í•œì´ ìˆë‹¤ë©´ í•¨ê»˜ í‘œì‹œí•´ì£¼ì„¸ìš”.",
                "ê²°ì • ì‚¬í•­ ì •ë¦¬": "ë‹¤ìŒ íšŒì˜ë¡ì—ì„œ ê²°ì •ëœ ì‚¬í•­ë“¤ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”. ê° ê²°ì • ì‚¬í•­ì˜ ë°°ê²½ê³¼ ì´ìœ ë„ í¬í•¨í•´ì£¼ì„¸ìš”.",
                "ì°¸ì„ìë³„ ì—­í•  ë¶„ì„": "ë‹¤ìŒ íšŒì˜ë¡ì—ì„œ ì°¸ì„ìë“¤ì˜ ì—­í• ê³¼ ê¸°ì—¬ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”. ê°ìì˜ ì±…ì„ê³¼ ì—­í• ì„ ì •ë¦¬í•´ì£¼ì„¸ìš”.",
                "ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸": "ë‹¤ìŒ íšŒì˜ë¡ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”. ìš”ì•½, ì£¼ìš” ë…¼ì˜ ì‚¬í•­, ì•¡ì…˜ ì•„ì´í…œ, ê²°ì • ì‚¬í•­, ì°¸ì„ì ì—­í• ì„ ëª¨ë‘ í¬í•¨í•œ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."
            }
        
        prompt = prompts.get(analysis_type, "ë‹¤ìŒ íšŒì˜ë¡ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.")
        
        # ì°¸ê³  í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if reference_prompt.strip():
            prompt += f"\n\n[ì°¸ê³ ì‚¬í•­]\n{reference_prompt}\n\nìœ„ ì°¸ê³ ì‚¬í•­ì„ ë°˜ë“œì‹œ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”."
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ íšŒì˜ë¡ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ íšŒì˜ë¡ì„ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ëª…í™•í•˜ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”."},
            {"role": "user", "content": f"{prompt}\n\níšŒì˜ë¡ ë‚´ìš©:\n{content}"}
        ]
        
        # LLM í˜¸ì¶œ
        response, error = llm_client.generate_response(
            provider=provider,
            model=model,
            messages=messages,
            temperature=0.3,
            max_tokens=3000 if is_multiple_meetings else 2000
        )
        
        if error:
            return f"âŒ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}"
        
        return response
        
    except Exception as e:
        return f"âŒ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

def get_all_meeting_contents(df: pd.DataFrame, api_key: str) -> str:
    """ëª¨ë“  íšŒì˜ë¡ì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹©ë‹ˆë‹¤."""
    if df.empty:
        return ""
    
    # íšŒì˜ë¡ í•­ëª©ë“¤ë§Œ í•„í„°ë§ (ì´ë¦„ì´ ìˆëŠ” í•­ëª©ë“¤)
    meeting_items = df[df['ì´ë¦„'].notna() & (df['ì´ë¦„'] != '')]
    
    if meeting_items.empty:
        return ""
    
    all_contents = []
    
    for idx, row in meeting_items.iterrows():
        page_id = row['page_id']
        meeting_name = row['ì´ë¦„']
        created_date = row['created_time'][:10] if row['created_time'] else "ë‚ ì§œ ì—†ìŒ"
        
        try:
            blocks = get_notion_page_blocks(page_id, api_key)
            if blocks:
                content = parse_notion_blocks(blocks, api_key)
                if content.strip():
                    all_contents.append(f"=== {meeting_name} ({created_date}) ===\n{content}\n")
        except Exception as e:
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰
            continue
    
    return "\n\n".join(all_contents)

# ë©”ì¸ ì•±
def main():
    st.title("ğŸ“ Notion ë°ì´í„° ì½ê¸°")
    st.markdown("---")
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'notion_data' not in st.session_state:
        st.session_state.notion_data = None
    if 'selected_meeting' not in st.session_state:
        st.session_state.selected_meeting = None
    if 'meeting_content' not in st.session_state:
        st.session_state.meeting_content = None
    
    # API í‚¤ì™€ ë°ì´í„°ë² ì´ìŠ¤ ID í™•ì¸
    if not NOTION_API_KEY:
        return
    
    # API ì—°ê²° í…ŒìŠ¤íŠ¸
    if st.button("ğŸ”— API ì—°ê²° í…ŒìŠ¤íŠ¸", type="secondary"):
        with st.spinner("API ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì¤‘..."):
            test_result = test_notion_connection(NOTION_API_KEY)
            
            if test_result["success"]:
                st.json(test_result["data"])
            else:
                pass
    
    if not FINAL_DATABASE_ID:
        return
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì„¤ì •
    st.subheader("ğŸ“… ê¸°ê°„ë³„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
    
    # ë‚ ì§œ ì„ íƒ ì˜µì…˜
    date_filter_option = st.selectbox(
        "ë‚ ì§œ í•„í„° ì˜µì…˜",
        ["ì „ì²´ ê¸°ê°„", "íŠ¹ì • ê¸°ê°„ ì„ íƒ", "ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼", "ìµœê·¼ 90ì¼", "ì´ë²ˆ ë‹¬", "ì§€ë‚œ ë‹¬"],
        help="ê°€ì ¸ì˜¬ ë°ì´í„°ì˜ ê¸°ê°„ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    
    start_date = None
    end_date = None
    
    if date_filter_option == "íŠ¹ì • ê¸°ê°„ ì„ íƒ":
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ì‹œì‘ì¼", value=None)
        with col2:
            end_date = st.date_input("ì¢…ë£Œì¼", value=None)
    elif date_filter_option == "ìµœê·¼ 7ì¼":
        from datetime import datetime, timedelta
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=7)
    elif date_filter_option == "ìµœê·¼ 30ì¼":
        from datetime import datetime, timedelta
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=30)
    elif date_filter_option == "ìµœê·¼ 90ì¼":
        from datetime import datetime, timedelta
        end_date = datetime.now().date()
        start_date = end_date - timedelta(days=90)
    elif date_filter_option == "ì´ë²ˆ ë‹¬":
        from datetime import datetime
        now = datetime.now()
        start_date = now.replace(day=1).date()
        end_date = now.date()
    elif date_filter_option == "ì§€ë‚œ ë‹¬":
        from datetime import datetime, timedelta
        now = datetime.now()
        last_month = now.replace(day=1) - timedelta(days=1)
        start_date = last_month.replace(day=1).date()
        end_date = last_month.date()
    
    # ì„ íƒëœ ê¸°ê°„ í‘œì‹œ
    if start_date and end_date:
        st.info(f"ğŸ“… ì„ íƒëœ ê¸°ê°„: {start_date} ~ {end_date} (í•œêµ­ ì‹œê°„ ê¸°ì¤€)")
    elif start_date:
        st.info(f"ğŸ“… ì‹œì‘ì¼: {start_date} ì´í›„ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)")
    elif end_date:
        st.info(f"ğŸ“… ì¢…ë£Œì¼: {end_date} ì´ì „ (í•œêµ­ ì‹œê°„ ê¸°ì¤€)")
    else:
        st.info("ğŸ“… ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤")
    
    # ì‹œê°„ëŒ€ ì •ë³´ í‘œì‹œ
    st.info("â° Notionì˜ created_timeì€ UTC ì‹œê°„ì´ë©°, í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í•„í„°ë§í•©ë‹ˆë‹¤.")
    
    # ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë²„íŠ¼
    if st.button("ğŸ”„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°", type="primary"):
        with st.spinner("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            # ë‚ ì§œë¥¼ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            start_date_str = start_date.isoformat() if start_date else None
            end_date_str = end_date.isoformat() if end_date else None
            
            # ë‚ ì§œ í•„í„° ì •ë³´ í‘œì‹œ
            if start_date or end_date:
                if start_date and end_date:
                    st.info(f"ğŸ“… {start_date} ~ {end_date} ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤")
                elif start_date:
                    st.info(f"ğŸ“… {start_date} ì´í›„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤")
                elif end_date:
                    st.info(f"ğŸ“… {end_date} ì´ì „ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤")
            else:
                st.info("ğŸ“… ì „ì²´ ê¸°ê°„ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤")
            
            notion_data = get_notion_database(FINAL_DATABASE_ID, NOTION_API_KEY, start_date_str, end_date_str)
            
            if notion_data:
                df = convert_to_dataframe(notion_data, start_date_str, end_date_str)
                
                if not df.empty:
                    st.session_state.notion_data = df
                    
                    # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                    st.dataframe(df, use_container_width=True)
                    
                    # íšŒì˜ë¡ ìƒì„¸ ë‚´ìš© í‘œì‹œ
                    display_meeting_details(df, NOTION_API_KEY)
                    
                    # ë°ì´í„° í†µê³„
                    st.subheader("ğŸ“ˆ ë°ì´í„° í†µê³„")
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("ì´ í•­ëª© ìˆ˜", len(df))
                    
                    with col2:
                        st.metric("ì»¬ëŸ¼ ìˆ˜", len(df.columns))
                    
                    with col3:
                        st.metric("ë°ì´í„° í¬ê¸°", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
                    
                    # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
                    st.subheader("ğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
                    export_format = st.selectbox(
                        "ë‚´ë³´ë‚¼ í˜•ì‹ ì„ íƒ",
                        ["CSV", "Excel"]
                    )
                    
                    export_data(df, export_format)
                    
                    # ë°ì´í„° ê²€ìƒ‰ ë° í•„í„°ë§
                    st.subheader("ğŸ” ë°ì´í„° ê²€ìƒ‰")
                    
                    # ê²€ìƒ‰ ì˜µì…˜
                    search_option = st.selectbox(
                        "ê²€ìƒ‰ ë²”ìœ„ ì„ íƒ",
                        ["ì œëª©ë§Œ ê²€ìƒ‰", "ì œëª© + íšŒì˜ ë‚´ìš© ê²€ìƒ‰"],
                        help="íšŒì˜ ë‚´ìš©ê¹Œì§€ ê²€ìƒ‰í•˜ë ¤ë©´ 'ì œëª© + íšŒì˜ ë‚´ìš© ê²€ìƒ‰'ì„ ì„ íƒí•˜ì„¸ìš”"
                    )
                    
                    search_term = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
                    
                    if search_term:
                        if search_option == "ì œëª©ë§Œ ê²€ìƒ‰":
                            # ê¸°ì¡´ ë°©ì‹: ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œë§Œ ê²€ìƒ‰
                            mask = pd.DataFrame([df[col].astype(str).str.contains(search_term, case=False, na=False) 
                                              for col in df.select_dtypes(include=['object']).columns]).any()
                            filtered_df = df[mask]
                        else:
                            # íšŒì˜ ë‚´ìš©ê¹Œì§€ ê²€ìƒ‰
                            with st.spinner("íšŒì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ê²€ìƒ‰ ì¤‘..."):
                                df_with_content = get_meeting_content_for_search(df, NOTION_API_KEY)
                                
                                # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                                title_mask = df_with_content['ì´ë¦„'].astype(str).str.contains(search_term, case=False, na=False)
                                content_mask = df_with_content['íšŒì˜_ë‚´ìš©'].astype(str).str.contains(search_term, case=False, na=False)
                                
                                # ì œëª© ë˜ëŠ” ë‚´ìš©ì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ í•­ëª©ë“¤
                                combined_mask = title_mask | content_mask
                                filtered_df = df_with_content[combined_mask]
                                
                                # ê²€ìƒ‰ ê²°ê³¼ì— ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
                                if not filtered_df.empty:
                                    st.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(filtered_df)}ê°œ í•­ëª©")
                                    
                                    # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë” ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
                                    for idx, row in filtered_df.iterrows():
                                        with st.expander(f"ğŸ“„ {row['ì´ë¦„']}"):
                                            st.write(f"**ìƒì„±ì¼:** {row['created_time'][:10]}")
                                            if 'íšŒì˜_ë‚´ìš©' in row and row['íšŒì˜_ë‚´ìš©']:
                                                # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ë¶€ë¶„ì„ ê°•ì¡°
                                                content = row['íšŒì˜_ë‚´ìš©']
                                                highlighted_content = content.replace(
                                                    search_term, f"**{search_term}**"
                                                )
                                                st.markdown("**íšŒì˜ ë‚´ìš©:**")
                                                st.markdown(highlighted_content[:500] + "..." if len(highlighted_content) > 500 else highlighted_content)
                                            else:
                                                st.write("íšŒì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        
                        if not filtered_df.empty:
                            st.dataframe(filtered_df, use_container_width=True)
                        else:
                            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                else:
                    pass
            else:
                pass
    
    # ì„¸ì…˜ì— ì €ì¥ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    elif st.session_state.notion_data is not None:
        df = st.session_state.notion_data
        
        # ìƒˆë¡œìš´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ì˜µì…˜
        st.subheader("ğŸ”„ ìƒˆë¡œìš´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°")
        if st.button("ğŸ“… ê¸°ê°„ë³„ ë°ì´í„° ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°", type="secondary"):
            st.session_state.notion_data = None
            st.rerun()
        
        # ê¸°ì¡´ ë°ì´í„° í•„í„°ë§
        st.subheader("ğŸ“… ê¸°ì¡´ ë°ì´í„° í•„í„°ë§")
        
        # ë‚ ì§œ í•„í„° ì˜µì…˜
        filter_option = st.selectbox(
            "í•„í„° ì˜µì…˜",
            ["ì „ì²´ ë°ì´í„°", "ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§"],
            help="ê¸°ì¡´ ë°ì´í„°ë¥¼ ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
        )
        
        if filter_option == "ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§":
            col1, col2 = st.columns(2)
            with col1:
                filter_start_date = st.date_input("í•„í„° ì‹œì‘ì¼", value=None)
            with col2:
                filter_end_date = st.date_input("í•„í„° ì¢…ë£Œì¼", value=None)
            
            if filter_start_date or filter_end_date:
                try:
                    # created_timeì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  í•œêµ­ ì‹œê°„ëŒ€(KST)ë¡œ ë³€í™˜
                    df['created_time_dt'] = pd.to_datetime(df['created_time']).dt.tz_convert('Asia/Seoul')
                except:
                    # ì‹œê°„ëŒ€ ë³€í™˜ì— ì‹¤íŒ¨í•˜ë©´ UTCë¡œ ì²˜ë¦¬í•˜ê³  9ì‹œê°„ ì¶”ê°€ (KST = UTC+9)
                    df['created_time_dt'] = pd.to_datetime(df['created_time']) + pd.Timedelta(hours=9)
                
                if filter_start_date and filter_end_date:
                    # ë‚ ì§œë§Œ ë¹„êµí•˜ë„ë¡ ì‹œê°„ ì •ë³´ ì œê±°
                    mask = (df['created_time_dt'].dt.date >= filter_start_date) & (df['created_time_dt'].dt.date <= filter_end_date)
                elif filter_start_date:
                    mask = df['created_time_dt'].dt.date >= filter_start_date
                elif filter_end_date:
                    mask = df['created_time_dt'].dt.date <= filter_end_date
                else:
                    mask = pd.Series([True] * len(df), index=df.index)
                
                filtered_df = df[mask]
                st.info(f"ğŸ“Š í•„í„°ë§ ê²°ê³¼: {len(filtered_df)}ê°œ í•­ëª© (ì „ì²´ {len(df)}ê°œ ì¤‘)")
                df = filtered_df
            else:
                st.info("ğŸ“Š ì „ì²´ ë°ì´í„° í‘œì‹œ")
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.subheader("ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(df, use_container_width=True)
        
        # íšŒì˜ë¡ ìƒì„¸ ë‚´ìš© í‘œì‹œ
        display_meeting_details(df, NOTION_API_KEY)
        
        # ë°ì´í„° í†µê³„
        st.subheader("ğŸ“ˆ ë°ì´í„° í†µê³„")
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ì´ í•­ëª© ìˆ˜", len(df))
        
        with col2:
            st.metric("ì»¬ëŸ¼ ìˆ˜", len(df.columns))
        
        with col3:
            st.metric("ë°ì´í„° í¬ê¸°", f"{df.memory_usage(deep=True).sum() / 1024:.1f} KB")
        
        # ë°ì´í„° ë‚´ë³´ë‚´ê¸°
        st.subheader("ğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")
        export_format = st.selectbox(
            "ë‚´ë³´ë‚¼ í˜•ì‹ ì„ íƒ",
            ["CSV", "Excel"]
        )
        
        export_data(df, export_format)
        
        # ë°ì´í„° ê²€ìƒ‰ ë° í•„í„°ë§
        st.subheader("ğŸ” ë°ì´í„° ê²€ìƒ‰")
        
        # ê²€ìƒ‰ ì˜µì…˜
        search_option = st.selectbox(
            "ê²€ìƒ‰ ë²”ìœ„ ì„ íƒ",
            ["ì œëª©ë§Œ ê²€ìƒ‰", "ì œëª© + íšŒì˜ ë‚´ìš© ê²€ìƒ‰"],
            help="íšŒì˜ ë‚´ìš©ê¹Œì§€ ê²€ìƒ‰í•˜ë ¤ë©´ 'ì œëª© + íšŒì˜ ë‚´ìš© ê²€ìƒ‰'ì„ ì„ íƒí•˜ì„¸ìš”"
        )
        
        search_term = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
        
        if search_term:
            if search_option == "ì œëª©ë§Œ ê²€ìƒ‰":
                # ê¸°ì¡´ ë°©ì‹: ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œë§Œ ê²€ìƒ‰
                mask = pd.DataFrame([df[col].astype(str).str.contains(search_term, case=False, na=False) 
                                  for col in df.select_dtypes(include=['object']).columns]).any()
                filtered_df = df[mask]
            else:
                # íšŒì˜ ë‚´ìš©ê¹Œì§€ ê²€ìƒ‰
                with st.spinner("íšŒì˜ ë‚´ìš©ì„ ê°€ì ¸ì™€ì„œ ê²€ìƒ‰ ì¤‘..."):
                    df_with_content = get_meeting_content_for_search(df, NOTION_API_KEY)
                    
                    # ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                    title_mask = df_with_content['ì´ë¦„'].astype(str).str.contains(search_term, case=False, na=False)
                    content_mask = df_with_content['íšŒì˜_ë‚´ìš©'].astype(str).str.contains(search_term, case=False, na=False)
                    
                    # ì œëª© ë˜ëŠ” ë‚´ìš©ì— ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ í•­ëª©ë“¤
                    combined_mask = title_mask | content_mask
                    filtered_df = df_with_content[combined_mask]
                    
                    # ê²€ìƒ‰ ê²°ê³¼ì— ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° ì¶”ê°€
                    if not filtered_df.empty:
                        st.info(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(filtered_df)}ê°œ í•­ëª©")
                        
                        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë” ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
                        for idx, row in filtered_df.iterrows():
                            with st.expander(f"ğŸ“„ {row['ì´ë¦„']}"):
                                st.write(f"**ìƒì„±ì¼:** {row['created_time'][:10]}")
                                if 'íšŒì˜_ë‚´ìš©' in row and row['íšŒì˜_ë‚´ìš©']:
                                    # ê²€ìƒ‰ì–´ê°€ í¬í•¨ëœ ë¶€ë¶„ì„ ê°•ì¡°
                                    content = row['íšŒì˜_ë‚´ìš©']
                                    highlighted_content = content.replace(
                                        search_term, f"**{search_term}**"
                                    )
                                    st.markdown("**íšŒì˜ ë‚´ìš©:**")
                                    st.markdown(highlighted_content[:500] + "..." if len(highlighted_content) > 500 else highlighted_content)
                                else:
                                    st.write("íšŒì˜ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            if not filtered_df.empty:
                st.dataframe(filtered_df, use_container_width=True)
            else:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì•± ì‹¤í–‰
if __name__ == "__main__":
    main()
