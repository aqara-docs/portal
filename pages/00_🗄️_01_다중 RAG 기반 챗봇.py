import streamlit as st
import os
from datetime import datetime
import time
import random
from dotenv import load_dotenv
from openai import OpenAI
import anthropic
from langchain_anthropic import ChatAnthropic
import json
import asyncio
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# RAG ê´€ë ¨ import ì¶”ê°€
import mysql.connector
import pandas as pd
from sqlalchemy import create_engine
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser
import nltk
import logging
from pathlib import Path
import hashlib

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ import
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import UnstructuredFileLoader
from langchain.schema import Document

# ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜ import
from chunk_processor import process_chunked_rag

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ¤– ë‹¤ì¤‘ RAG ê¸°ë°˜ ì±—ë´‡ ğŸ—„ï¸",
    page_icon="ğŸ¤–",
    layout="wide"
)

st.title("ğŸ¤– ë‹¤ì¤‘ RAG ê¸°ë°˜ ì±—ë´‡ ğŸ—„ï¸")

# ì¸ì¦ ê¸°ëŠ¥ (ê°„ë‹¨í•œ ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸)
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False
admin_pw = os.getenv('ADMIN_PASSWORD')
if not admin_pw:
    st.error('í™˜ê²½ë³€ìˆ˜(ADMIN_PASSWORD)ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.')
    st.stop()
if not st.session_state.authenticated:
    password = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")
    if password == admin_pw:
        st.session_state.authenticated = True
        st.rerun()
    else:
        if password:
            st.error("ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤")
        st.stop()

st.markdown("RAG ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ - MySQL DB, ì›¹ì‚¬ì´íŠ¸, ë¬¸ì„œ ë¶„ì„ ì§€ì›")

# í‘œì¤€í™”ëœ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í•¨ìˆ˜
def connect_to_db():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
    try:
        conn = mysql.connector.connect(
            host=os.getenv('SQL_HOST'),
            user=os.getenv('SQL_USER'),
            password=os.getenv('SQL_PASSWORD'),
            database=os.getenv('SQL_DATABASE_NEWBIZ'),
            charset='utf8mb4',
            collation='utf8mb4_unicode_ci'
        )
        return conn
    except mysql.connector.Error as err:
        st.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {err}")
        return None

def create_rag_agent_tables():
    """RAG ì—ì´ì „íŠ¸ìš© í…Œì´ë¸” ìƒì„±"""
    try:
        connection = connect_to_db()
        if not connection:
            return False
        
        cursor = connection.cursor()
        
        # 1. ë©”ì¸ ëŒ€í™” ì„¸ì…˜ í…Œì´ë¸”
        rag_conversations_table = """
        CREATE TABLE IF NOT EXISTS rag_conversations (
            id INT AUTO_INCREMENT PRIMARY KEY,
            session_title VARCHAR(255) NOT NULL COMMENT 'ëŒ€í™” ì„¸ì…˜ ì œëª©',
            user_query TEXT NOT NULL COMMENT 'ì‚¬ìš©ì ì§ˆë¬¸',
            assistant_response LONGTEXT COMMENT 'AI ì‘ë‹µ',
            model_name VARCHAR(100) NOT NULL COMMENT 'ì‚¬ìš©ëœ AI ëª¨ë¸',
            has_reasoning BOOLEAN DEFAULT FALSE COMMENT 'reasoning í¬í•¨ ì—¬ë¶€',
            reasoning_content LONGTEXT COMMENT 'reasoning ê³¼ì •',
            rag_sources_used INT DEFAULT 0 COMMENT 'ì‚¬ìš©ëœ RAG ì†ŒìŠ¤ ìˆ˜',
            execution_time_seconds INT COMMENT 'ì‘ë‹µ ìƒì„± ì‹œê°„(ì´ˆ)',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
            tags VARCHAR(500) COMMENT 'ê²€ìƒ‰ìš© íƒœê·¸',
            notes TEXT COMMENT 'ì¶”ê°€ ë©”ëª¨',
            INDEX idx_created_at (created_at),
            INDEX idx_model_name (model_name),
            INDEX idx_has_reasoning (has_reasoning),
            FULLTEXT idx_search (session_title, user_query, tags, notes)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='RAG ì—ì´ì „íŠ¸ ëŒ€í™” ì„¸ì…˜';
        """
        cursor.execute(rag_conversations_table)
        
        # 2. RAG ì†ŒìŠ¤ ì •ë³´ í…Œì´ë¸”
        rag_sources_table = """
        CREATE TABLE IF NOT EXISTS rag_conversation_sources (
            id INT AUTO_INCREMENT PRIMARY KEY,
            conversation_id INT NOT NULL COMMENT 'ëŒ€í™” ì„¸ì…˜ ID',
            source_type ENUM('mysql', 'website', 'files') NOT NULL COMMENT 'RAG ì†ŒìŠ¤ íƒ€ì…',
            source_name VARCHAR(255) NOT NULL COMMENT 'ì†ŒìŠ¤ ì´ë¦„',
            source_description TEXT COMMENT 'ì†ŒìŠ¤ ì„¤ëª…',
            source_details JSON COMMENT 'ì†ŒìŠ¤ ìƒì„¸ ì •ë³´',
            data_size INT COMMENT 'ë°ì´í„° í¬ê¸°',
            content_preview TEXT COMMENT 'ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (conversation_id) REFERENCES rag_conversations(id) ON DELETE CASCADE,
            INDEX idx_conversation_id (conversation_id),
            INDEX idx_source_type (source_type),
            INDEX idx_source_name (source_name)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT='RAG ì†ŒìŠ¤ ì •ë³´';
        """
        cursor.execute(rag_sources_table)
        
        connection.commit()
        cursor.close()
        connection.close()
        return True
        
    except Exception as e:
        st.error(f"í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return False

def get_mysql_tables():
    """MySQL í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""
    try:
        connection = connect_to_db()
        if not connection:
            return []
        
        cursor = connection.cursor()
        cursor.execute("SHOW TABLES")
        tables = [table[0] for table in cursor.fetchall()]
        
        cursor.close()
        connection.close()
        return tables
    except Exception as e:
        st.error(f"í…Œì´ë¸” ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

def load_mysql_data(selected_tables):
    """ì„ íƒëœ MySQL í…Œì´ë¸” ë°ì´í„° ë¡œë“œ"""
    mysql_data = {}
    
    try:
        connection = connect_to_db()
        if not connection:
            return mysql_data
        
        for table in selected_tables:
            try:
                query = f"SELECT * FROM {table} LIMIT 1000"
                df = pd.read_sql(query, connection)
                mysql_data[table] = df
                st.success(f"âœ… {table} í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ ({len(df)}í–‰)")
            except Exception as e:
                st.error(f"âŒ {table} í…Œì´ë¸” ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        
        connection.close()
        return mysql_data
        
    except Exception as e:
        st.error(f"MySQL ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        return mysql_data

def scrape_website_simple(url, max_pages=5):
    """ê°„ë‹¨í•œ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ - ê°œì„ ëœ ë²„ì „"""
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        visited_urls = set()
        scraped_data = []
        urls_to_visit = [url]
        
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        successfully_scraped = 0  # ì‹¤ì œë¡œ ìŠ¤í¬ë˜í•‘ëœ í˜ì´ì§€ ìˆ˜
        
        while urls_to_visit and successfully_scraped < max_pages:
            current_url = urls_to_visit.pop(0)
            
            if current_url in visited_urls:
                continue
                
            try:
                response = session.get(current_url, timeout=10)
                response.raise_for_status()
                
                visited_urls.add(current_url)
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # í˜ì´ì§€ ì œëª© ì¶”ì¶œ
                title = soup.find('title')
                title_text = title.get_text().strip() if title else f"í˜ì´ì§€ {successfully_scraped + 1}"
                
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for script in soup(["script", "style", "nav", "header", "footer"]):
                    script.decompose()
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                content = ' '.join(chunk for chunk in chunks if chunk)
                
                # ì½˜í…ì¸ ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ê¸¸ì´ ì œí•œ ì™„í™”)
                if len(content) > 50:  # 50ìë¡œ ê¸°ì¤€ ë‚®ì¶¤
                    scraped_data.append({
                        'url': current_url,
                        'title': title_text,
                        'content': content[:2000]  # ì²˜ìŒ 2000ìë§Œ
                    })
                    
                    successfully_scraped += 1
                    st.success(f"âœ… í˜ì´ì§€ {successfully_scraped}/{max_pages} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {title_text}")
                
                # ë” ë§ì€ ë§í¬ ì°¾ê¸° (ì œí•œ ì™„í™”)
                base_domain = urlparse(url).netloc
                
                # ë§í¬ íƒìƒ‰ì„ ë” ì ê·¹ì ìœ¼ë¡œ
                for link in soup.find_all("a", href=True)[:20]:  # 20ê°œë¡œ ì¦ê°€
                    absolute_link = urljoin(current_url, link['href'])
                    parsed_link = urlparse(absolute_link)
                    
                    # ì¡°ê±´ ì™„í™”: ê°™ì€ ë„ë©”ì¸ + ê¸°ë³¸ì ì¸ í•„í„°ë§
                    if (parsed_link.netloc == base_domain and 
                        absolute_link not in visited_urls and 
                        absolute_link not in urls_to_visit and
                        not any(x in absolute_link.lower() for x in ['#', 'javascript:', 'mailto:', 'tel:']) and
                        len(urls_to_visit) < 50):  # ëŒ€ê¸° í í¬ê¸° ì œí•œ
                        urls_to_visit.append(absolute_link)
                
                time.sleep(0.5)  # ì§€ì—°ì‹œê°„ ë‹¨ì¶• (1ì´ˆ â†’ 0.5ì´ˆ)
                
            except Exception as e:
                st.warning(f"âš ï¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({current_url}): {str(e)}")
                continue
        
        # ê²°ê³¼ ë©”ì‹œì§€ ê°œì„ 
        if scraped_data:
            if successfully_scraped == max_pages:
                st.success(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! {successfully_scraped}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
            else:
                st.info(f"ğŸ“„ {successfully_scraped}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ëª©í‘œ: {max_pages}ê°œ)")
                if successfully_scraped < max_pages:
                    st.warning(f"ğŸ’¡ {max_pages - successfully_scraped}ê°œ í˜ì´ì§€ ë¶€ì¡± - ì‚¬ì´íŠ¸ì— ì¶”ê°€ ë§í¬ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.error("âŒ í¬ë¡¤ë§ëœ í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        return scraped_data
        
    except Exception as e:
        st.error(f"âŒ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}")
        return []

def process_files(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬"""
    files_data = []
    
    for file in files:
        try:
            if file.type == "text/plain":
                content = str(file.read(), "utf-8")
            elif file.type == "application/pdf":
                # PDF ì²˜ë¦¬ (ê°„ë‹¨í•œ ì˜ˆì‹œ)
                content = "PDF íŒŒì¼ - ì „ë¬¸ ì²˜ë¦¬ í•„ìš”"
            else:
                content = str(file.read(), "utf-8", errors='ignore')
            
            files_data.append({
                'name': file.name,
                'size': len(content),
                'content': content
            })
            
            st.success(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file.name}")
            
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ ({file.name}): {str(e)}")
    
    return files_data 

def create_rag_context(mysql_data=None, website_data=None, files_data=None, model_name=None):
    """RAG ì»¨í…ìŠ¤íŠ¸ ìƒì„± - ëª¨ë¸ë³„ í¬ê¸° ì œí•œ ì ìš©"""
    context_parts = []
    rag_sources_used = []
    
    # ëª¨ë¸ë³„ í¬ê¸° ì œí•œ ì„¤ì •
    if model_name and model_name.startswith('claude'):
        MAX_CONTEXT_SIZE = 7000000  # 7MB
        MYSQL_SAMPLE_ROWS = 1
        CONTENT_PREVIEW_SIZE = 200
        apply_size_limit = True
        limit_type = "Claude 7MB"
    elif model_name and model_name.startswith('o1'):
        MAX_CONTEXT_SIZE = 800000   # 800KB
        MYSQL_SAMPLE_ROWS = 2
        CONTENT_PREVIEW_SIZE = 300
        apply_size_limit = True
        limit_type = "o1 800KB"
    elif model_name and (model_name.startswith('gpt-4') or model_name.startswith('gpt-3')):
        MAX_CONTEXT_SIZE = 3000000  # 3MB
        MYSQL_SAMPLE_ROWS = 3
        CONTENT_PREVIEW_SIZE = 500
        apply_size_limit = True
        limit_type = "GPT 3MB"
    else:
        MAX_CONTEXT_SIZE = 1000000  # 1MB
        MYSQL_SAMPLE_ROWS = 2
        CONTENT_PREVIEW_SIZE = 300
        apply_size_limit = True
        limit_type = "ê¸°ë³¸ 1MB"
    
    current_size = 0
    
    # MySQL ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
    if mysql_data and current_size < MAX_CONTEXT_SIZE:
        mysql_context = "=== MySQL ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ ===\n"
        mysql_tables = []
        total_mysql_rows = 0
        
        for table_name, df in mysql_data.items():
            table_info = f"\n[{table_name}] í…Œì´ë¸”:\n"
            table_info += f"- í–‰ ìˆ˜: {len(df):,}ê°œ\n"
            table_info += f"- ì»¬ëŸ¼: {', '.join(df.columns.tolist())}\n"
            
            mysql_tables.append(table_name)
            total_mysql_rows += len(df)
            
            if len(df) > 0:
                table_info += "- ìƒ˜í”Œ ë°ì´í„°:\n"
                sample_data = df.head(MYSQL_SAMPLE_ROWS).to_string(index=False, max_cols=5 if apply_size_limit else None)
                if apply_size_limit and len(sample_data) > 500:
                    sample_data = sample_data[:500] + "..."
                table_info += sample_data + "\n"
            
            if current_size + len(table_info) > MAX_CONTEXT_SIZE:
                mysql_context += f"\n[í…Œì´ë¸” {table_name} ìƒëµ - {limit_type} í¬ê¸° ì œí•œ ì´ˆê³¼]\n"
                break
            
            mysql_context += table_info
            current_size += len(table_info)
        
        context_parts.append(mysql_context)
        rag_sources_used.append({
            'type': 'mysql',
            'name': 'MySQL ë°ì´í„°ë² ì´ìŠ¤',
            'details': f"{len(mysql_tables)}ê°œ í…Œì´ë¸” ({total_mysql_rows:,}í–‰)",
            'tables': mysql_tables
        })
    
    # ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
    if website_data and current_size < MAX_CONTEXT_SIZE:
        website_context = "=== ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì •ë³´ ===\n"
        website_urls = []
        
        for i, page_data in enumerate(website_data[:3]):
            page_info = f"\n[í˜ì´ì§€ {i+1}] {page_data['title']}\n"
            page_info += f"URL: {page_data['url']}\n"
            
            content_preview = page_data['content'][:CONTENT_PREVIEW_SIZE] + "..."
            page_info += f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}\n"
            
            if apply_size_limit and current_size + len(page_info) > MAX_CONTEXT_SIZE:
                website_context += f"\n[í˜ì´ì§€ {i+1} ìƒëµ - í¬ê¸° ì œí•œ ì´ˆê³¼]\n"
                break
            
            website_context += page_info
            current_size += len(page_info)
            
            website_urls.append({
                'title': page_data['title'],
                'url': page_data['url']
            })
        
        context_parts.append(website_context)
        rag_sources_used.append({
            'type': 'website',
            'name': 'ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§',
            'details': f"{len(website_data)}ê°œ í˜ì´ì§€",
            'urls': website_urls
        })
    
    # íŒŒì¼ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
    if files_data and current_size < MAX_CONTEXT_SIZE:
        files_context = "=== ì—…ë¡œë“œëœ ë¬¸ì„œ ì •ë³´ ===\n"
        file_list = []
        total_file_size = 0
        
        for file_data in files_data:
            file_info = f"\n[ë¬¸ì„œ] {file_data['name']}\n"
            file_info += f"í¬ê¸°: {file_data['size']:,}ì\n"
            
            content_preview = file_data['content'][:CONTENT_PREVIEW_SIZE] + "..."
            file_info += f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content_preview}\n"
            
            if apply_size_limit and current_size + len(file_info) > MAX_CONTEXT_SIZE:
                files_context += f"\n[íŒŒì¼ {file_data['name']} ìƒëµ - í¬ê¸° ì œí•œ ì´ˆê³¼]\n"
                break
            
            files_context += file_info
            current_size += len(file_info)
            
            file_list.append(file_data['name'])
            total_file_size += file_data['size']
        
        context_parts.append(files_context)
        rag_sources_used.append({
            'type': 'files',
            'name': 'ì—…ë¡œë“œ ë¬¸ì„œ',
            'details': f"{len(file_list)}ê°œ íŒŒì¼ ({total_file_size:,}ì)",
            'files': file_list
        })
    
    context_text = "\n\n".join(context_parts)
    
    final_size = len(context_text)
    if apply_size_limit:
        if final_size > MAX_CONTEXT_SIZE:
            st.warning(f"âš ï¸ RAG ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({final_size:,}ì > {MAX_CONTEXT_SIZE:,}ì). ì¼ë¶€ ë°ì´í„°ê°€ ì œì™¸ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            context_text = context_text[:MAX_CONTEXT_SIZE] + "\n\n[ì½˜í…ì¸ ê°€ í¬ê¸° ì œí•œìœ¼ë¡œ ì¸í•´ ì˜ë ¸ìŠµë‹ˆë‹¤]"
        elif final_size > MAX_CONTEXT_SIZE * 0.8:
            st.info(f"ğŸ’¡ RAG ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {final_size:,}ì (ì œí•œ: {MAX_CONTEXT_SIZE:,}ì)")
    else:
        if final_size > 1000000:
            st.info(f"ğŸ’¡ RAG ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: {final_size:,}ì (OpenAI ëª¨ë¸ - í¬ê¸° ì œí•œ ì—†ìŒ)")
    
    return context_text, rag_sources_used

def get_ai_response(prompt, model_name, system_prompt="", enable_thinking=False, thinking_budget=4000):
    """AI ëª¨ë¸ë¡œë¶€í„° ì‘ë‹µì„ ë°›ëŠ” í•¨ìˆ˜ (reasoning ê³¼ì • í‘œì‹œ ì§€ì›)"""
    try:
        if model_name.startswith('claude'):
            # Reasoning ëª¨ë¸ì¸ì§€ í™•ì¸ (ì°¸ì¡° íŒŒì¼ ê¸°ì¤€)
            is_reasoning_model = (
                model_name == 'claude-3-7-sonnet-latest' or
                model_name == 'claude-3-5-sonnet-latest'
            )
            
            if is_reasoning_model and enable_thinking:
                # Extended Thinking ì‚¬ìš© (ì§ì ‘ Anthropic í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©)
                import anthropic
                direct_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))
                
                messages = [{"role": "user", "content": prompt}]
                
                response = direct_client.messages.create(
                    model=model_name,
                    max_tokens=8192,
                    system=system_prompt if system_prompt else None,
                    thinking={
                        "type": "enabled",
                        "budget_tokens": thinking_budget
                    },
                    messages=messages
                )
                
                thinking_content = ""
                final_content = ""
                
                for block in response.content:
                    if hasattr(block, 'type'):
                        if block.type == "thinking":
                            thinking_content = block.thinking
                        elif block.type == "text":
                            final_content = block.text
                        elif block.type == "redacted_thinking":
                            thinking_content += "\n[ì¼ë¶€ ì‚¬ê³  ê³¼ì •ì´ ë³´ì•ˆìƒ ì•”í˜¸í™”ë˜ì—ˆìŠµë‹ˆë‹¤]"
                
                return {
                    "content": final_content,
                    "thinking": thinking_content,
                    "has_thinking": True
                }
            else:
                # ì¼ë°˜ ëª¨ë“œ (LangChain ì‚¬ìš©)
                from langchain_anthropic import ChatAnthropic
                client = ChatAnthropic(
                    model=model_name, 
                    api_key=os.getenv('ANTHROPIC_API_KEY'), 
                    temperature=0.7, 
                    max_tokens=8192
                )
                
                if system_prompt:
                    full_prompt = f"System: {system_prompt}\n\nUser: {prompt}"
                    response = client.invoke(full_prompt)
                else:
                    response = client.invoke(prompt)
                
                content = response.content if hasattr(response, 'content') else str(response)
                return {
                    "content": content,
                    "thinking": "",
                    "has_thinking": False
                }
                
        else:
            # OpenAI ëª¨ë¸ë“¤
            openai_key = os.getenv('OPENAI_API_KEY')
            if not openai_key:
                st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
                return {
                    "content": "OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "thinking": "",
                    "has_thinking": False
                }
            
            from openai import OpenAI
            openai_client = OpenAI(api_key=openai_key)
            
            is_o1_model = model_name.startswith('o1')
            
            if is_o1_model:
                response = openai_client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "user", "content": f"{system_prompt}\n\n{prompt}" if system_prompt else prompt}
                    ],
                    max_completion_tokens=8192
                )
                
                content = response.choices[0].message.content
                
                return {
                    "content": content,
                    "thinking": "ğŸ§  ì´ ëª¨ë¸ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë³µì¡í•œ reasoning ê³¼ì •ì„ ê±°ì³ ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
                    "has_thinking": True
                }
            else:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                response = openai_client.chat.completions.create(
                    model=model_name,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=8192
                )
                
                content = response.choices[0].message.content
                return {
                    "content": content,
                    "thinking": "",
                    "has_thinking": False
                }
            
    except Exception as e:
        st.error(f"âŒ AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {
            "content": f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
            "thinking": "",
            "has_thinking": False
        } 

def save_conversation(session_title, user_query, assistant_response, model_name, has_reasoning=False, reasoning_content="", rag_sources_used=None, execution_time_seconds=None):
    """ëŒ€í™” ë‚´ìš©ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    try:
        connection = connect_to_db()
        if not connection:
            return False
            
        cursor = connection.cursor()
        
        rag_source_count = len(rag_sources_used) if rag_sources_used else 0
        
        # ë©”ì¸ ëŒ€í™” ì €ì¥
        insert_conversation = """
        INSERT INTO rag_conversations 
        (session_title, user_query, assistant_response, model_name, has_reasoning, reasoning_content, rag_sources_used, execution_time_seconds)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
        """
        
        cursor.execute(insert_conversation, (
            session_title,
            user_query,
            assistant_response,
            model_name,
            has_reasoning,
            reasoning_content,
            rag_source_count,
            execution_time_seconds
        ))
        
        conversation_id = cursor.lastrowid
        
        # RAG ì†ŒìŠ¤ ì •ë³´ ì €ì¥
        if rag_sources_used:
            insert_source = """
            INSERT INTO rag_conversation_sources
            (conversation_id, source_type, source_name, source_description, source_details)
            VALUES (%s, %s, %s, %s, %s)
            """
            
            for source in rag_sources_used:
                cursor.execute(insert_source, (
                    conversation_id,
                    source['type'],
                    source['name'],
                    source['details'],
                    json.dumps(source, ensure_ascii=False)
                ))
        
        connection.commit()
        cursor.close()
        connection.close()
        
        return True
        
    except Exception as e:
        st.error(f"ëŒ€í™” ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        return False

def get_conversation_history(limit=20, search_term=None, model_filter=None):
    """ì €ì¥ëœ ëŒ€í™” ì´ë ¥ ì¡°íšŒ"""
    try:
        connection = connect_to_db()
        if not connection:
            return []
            
        cursor = connection.cursor(dictionary=True)
        
        base_query = """
        SELECT id, session_title, user_query, assistant_response, model_name, 
               has_reasoning, reasoning_content, rag_sources_used, 
               execution_time_seconds, created_at
        FROM rag_conversations
        """
        
        conditions = []
        params = []
        
        if search_term:
            conditions.append("(session_title LIKE %s OR user_query LIKE %s)")
            params.extend([f'%{search_term}%', f'%{search_term}%'])
        
        if model_filter and model_filter != "ì „ì²´":
            conditions.append("model_name LIKE %s")
            params.append(f'%{model_filter}%')
        
        if conditions:
            base_query += " WHERE " + " AND ".join(conditions)
        
        base_query += " ORDER BY created_at DESC LIMIT %s"
        params.append(limit)
        
        cursor.execute(base_query, params)
        conversations = cursor.fetchall()
        
        cursor.close()
        connection.close()
        
        return conversations
        
    except Exception as e:
        st.error(f"ëŒ€í™” ì´ë ¥ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return []

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'mysql_data' not in st.session_state:
    st.session_state.mysql_data = {}
if 'website_data' not in st.session_state:
    st.session_state.website_data = []
if 'files_data' not in st.session_state:
    st.session_state.files_data = []
if 'enable_sidebar_reasoning' not in st.session_state:
    st.session_state.enable_sidebar_reasoning = False
if 'rag_sources_used' not in st.session_state:
    st.session_state.rag_sources_used = []
if 'current_conversation' not in st.session_state:
    st.session_state.current_conversation = None

# í…Œì´ë¸” ìƒì„± ë²„íŠ¼
if st.button("ğŸ› ï¸ RAG ì—ì´ì „íŠ¸ í…Œì´ë¸” ìƒì„±/ê²€ì¦"):
    if create_rag_agent_tables():
        st.success("âœ… RAG ì—ì´ì „íŠ¸ í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±/ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        st.error("âŒ í…Œì´ë¸” ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

# ë©”ì¸ íƒ­ ì¸í„°í˜ì´ìŠ¤
tab1, tab2, tab3 = st.tabs([
    "ğŸ¤– AI ì±—ë´‡",
    "ğŸ“Š RAG ë°ì´í„° ê´€ë¦¬",
    "ğŸ“ ëŒ€í™” ì´ë ¥ ì¡°íšŒ"
])

# Tab 1: AI ì±—ë´‡
with tab1:
    st.header("ğŸ¤– RAG ê¸°ë°˜ AI ì±—ë´‡")
    
    # ëª¨ë¸ ì„ íƒ
    col1, col2 = st.columns([3, 2])
    
    with col1:
        model_options = [
            "claude-3-7-sonnet-latest",
            "claude-3-5-sonnet-latest",
            "claude-3-5-sonnet-20241022",
            "claude-3-5-haiku-20241022",
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229", 
            "claude-3-haiku-20240307",
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4-turbo",
            "gpt-3.5-turbo",
            "o1-preview",
            "o1-mini"
        ]
        
        selected_model = st.selectbox(
            "ğŸ¯ AI ëª¨ë¸ ì„ íƒ",
            model_options,
            index=0,
            help="Claude-3-7-sonnet-latest, Claude-3-5-sonnet-latestì™€ o1 ëª¨ë¸ë“¤ì€ Extended Thinking(Reasoning)ì„ ì§€ì›í•©ë‹ˆë‹¤."
        )
    
    with col2:
        session_title = st.text_input(
            "ğŸ“ ì„¸ì…˜ ì œëª© (ì„ íƒì‚¬í•­)",
            placeholder="ì˜ˆ: ë§¤ì¶œ ë¶„ì„ ì§ˆë¬¸",
            help="ëŒ€í™”ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•œ ì œëª©ì…ë‹ˆë‹¤."
        )
    
    # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
    user_query = st.text_area(
        "â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
        height=100,
        placeholder="ì˜ˆ: ìµœê·¼ 3ê°œì›” ë§¤ì¶œ ë™í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”."
    )
    
    # ì§ˆë¬¸ ë²„íŠ¼ê³¼ RAG ë°ì´í„° ìš”ì•½
    col1, col2 = st.columns([2, 3])
    
    with col1:
        submit_button = st.button("ğŸš€ ì§ˆë¬¸í•˜ê¸°", type="primary", use_container_width=True)
    
    with col2:
        # í˜„ì¬ ë¡œë“œëœ RAG ë°ì´í„° ìš”ì•½ í‘œì‹œ
        rag_summary = []
        if st.session_state.mysql_data:
            total_rows = sum(len(df) for df in st.session_state.mysql_data.values())
            rag_summary.append(f"ğŸ—„ï¸ MySQL: {len(st.session_state.mysql_data)}ê°œ í…Œì´ë¸” ({total_rows:,}í–‰)")
        
        if st.session_state.website_data:
            rag_summary.append(f"ğŸŒ ì›¹ì‚¬ì´íŠ¸: {len(st.session_state.website_data)}ê°œ í˜ì´ì§€")
        
        if st.session_state.files_data:
            total_size = sum(f['size'] for f in st.session_state.files_data)
            rag_summary.append(f"ğŸ“„ ë¬¸ì„œ: {len(st.session_state.files_data)}ê°œ ({total_size:,}ì)")
        
        if rag_summary:
            st.info("ğŸ“‹ **í˜„ì¬ RAG ë°ì´í„°**: " + " | ".join(rag_summary))
        else:
            st.warning("âš ï¸ RAG ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 'ğŸ“Š RAG ë°ì´í„° ê´€ë¦¬' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì„¸ìš”.")
    
    # AI ì‘ë‹µ ì²˜ë¦¬
    if submit_button and user_query.strip():
        if not any([st.session_state.mysql_data, st.session_state.website_data, st.session_state.files_data]):
            st.error("âŒ RAG ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤! ë¨¼ì € 'ğŸ“Š RAG ë°ì´í„° ê´€ë¦¬' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
        else:
            start_time = time.time()
            
            with st.spinner(f"ğŸ¤– {selected_model}ì´ RAG ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                
                # ì²­í¬ ì²˜ë¦¬ê°€ í•„ìš”í•œì§€ í™•ì¸ (ì²­í¬ ì²˜ë¦¬ í•¨ìˆ˜ì—ì„œ íŒë‹¨)
                chunked_result = process_chunked_rag(
                    user_query=user_query,
                    mysql_data=st.session_state.mysql_data if st.session_state.mysql_data else None,
                    website_data=st.session_state.website_data if st.session_state.website_data else None,
                    files_data=st.session_state.files_data if st.session_state.files_data else None,
                    model_name=selected_model,
                    get_ai_response_func=get_ai_response
                )
                
                if chunked_result:
                    # ì²­í¬ ì²˜ë¦¬ ê²°ê³¼
                    response = chunked_result
                    rag_context = "ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬"
                    rag_sources_used = response['rag_sources_used']
                else:
                    # ì¼ë°˜ RAG ì²˜ë¦¬
                    rag_context, rag_sources_used = create_rag_context(
                        mysql_data=st.session_state.mysql_data if st.session_state.mysql_data else None,
                        website_data=st.session_state.website_data if st.session_state.website_data else None,
                        files_data=st.session_state.files_data if st.session_state.files_data else None,
                        model_name=selected_model
                    )
                    
                    # RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                    system_prompt = "ë‹¹ì‹ ì€ RAG(Retrieval-Augmented Generation) ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”."
                    
                    if rag_context.strip():
                        prompt = f"""
ë‹¤ìŒì€ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ìˆ˜ì§‘ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ì…ë‹ˆë‹¤:

{rag_context}

ì§ˆë¬¸: {user_query}

ìœ„ì˜ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”. 
- ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
- ê´€ë ¨ ë°ì´í„°ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
- ë‹µë³€ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ì†ŒìŠ¤ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”
"""
                    else:
                        prompt = f"ì§ˆë¬¸: {user_query}\n\nì»¨í…ìŠ¤íŠ¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                    
                    # AI ì‘ë‹µ ìƒì„±
                    response = get_ai_response(
                        prompt=prompt,
                        model_name=selected_model,
                        system_prompt=system_prompt,
                        enable_thinking=st.session_state.get('enable_sidebar_reasoning', False)
                    )
            
            end_time = time.time()
            execution_time = int(end_time - start_time)
            
            # ì‘ë‹µ í‘œì‹œ
            st.success(f"âœ… **ì‘ë‹µ ì™„ë£Œ** (ì†Œìš”ì‹œê°„: {execution_time}ì´ˆ)")
            
            # ì²­í¬ ì²˜ë¦¬ì¸ì§€ í™•ì¸í•˜ì—¬ ë‹¤ë¥¸ í˜•íƒœë¡œ í‘œì‹œ
            if chunked_result:
                # ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
                st.markdown("### ğŸ¤– AI ì‘ë‹µ")
                st.markdown(response['content'])
                
                # ì²­í¬ë³„ ìƒì„¸ ê²°ê³¼ í‘œì‹œ
                if 'chunk_responses' in response:
                    with st.expander(f"ğŸ“Š ì²­í¬ë³„ ì²˜ë¦¬ ê²°ê³¼ ({response['chunk_count']}ê°œ ì²­í¬)", expanded=False):
                        for chunk_resp in response['chunk_responses']:
                            st.markdown(f"**ì²­í¬ {chunk_resp['chunk_id']} ê²°ê³¼:**")
                            st.markdown(chunk_resp['content'])
                            st.markdown("---")
            else:
                # ì¼ë°˜ ì²˜ë¦¬ ê²°ê³¼ í‘œì‹œ
                st.markdown("### ğŸ¤– AI ì‘ë‹µ")
                st.markdown(response['content'])
            
            # RAG ì†ŒìŠ¤ ì •ë³´ í‘œì‹œ
            if rag_sources_used:
                with st.expander(f"ğŸ“š ì‚¬ìš©ëœ RAG ì†ŒìŠ¤ ({len(rag_sources_used)}ê°œ)", expanded=False):
                    for i, source in enumerate(rag_sources_used):
                        st.markdown(f"**{i+1}. {source['name']}**")
                        st.markdown(f"- íƒ€ì…: {source['type']}")
                        st.markdown(f"- ìƒì„¸: {source['details']}")
                        if i < len(rag_sources_used) - 1:
                            st.markdown("---")
            
            # Reasoning ê³¼ì • í‘œì‹œ (ì‚¬ì´ë“œë°” ì„¤ì •ì— ë”°ë¼)
            if response.get('has_thinking', False) and response.get('thinking', '').strip():
                if st.session_state.get('enable_sidebar_reasoning', False):
                    # ì‚¬ì´ë“œë°”ì— í‘œì‹œëŠ” ë‚˜ì¤‘ì— ì²˜ë¦¬
                    pass
                else:
                    # ë©”ì¸ ì˜ì—­ì— í‘œì‹œ
                    st.markdown("### ğŸ§  AI ì‚¬ê³  ê³¼ì • (Reasoning)")
                    reasoning_content = response['thinking']
                    reasoning_html = f"""
                    <details open>
                        <summary style="cursor: pointer; font-weight: bold; color: #1f77b4; font-size: 16px;">
                            ğŸ§  Reasoning ê³¼ì • ë³´ê¸°/ìˆ¨ê¸°ê¸°
                        </summary>
                        <div style="margin-top: 15px; padding: 15px; background-color: #f8f9fa; border-radius: 8px; border-left: 4px solid #1f77b4; color: #000000; font-family: 'Courier New', monospace; white-space: pre-wrap; max-height: 500px; overflow-y: auto;">
{reasoning_content}
                        </div>
                    </details>
                    """
                    st.markdown(reasoning_html, unsafe_allow_html=True)
            
            # ëŒ€í™” ì €ì¥
            final_session_title = session_title.strip() if session_title.strip() else f"ì§ˆë¬¸_{datetime.now().strftime('%m%d_%H%M')}"
            
            save_success = save_conversation(
                session_title=final_session_title,
                user_query=user_query,
                assistant_response=response['content'],
                model_name=selected_model,
                has_reasoning=response.get('has_thinking', False),
                reasoning_content=response.get('thinking', ''),
                rag_sources_used=rag_sources_used,
                execution_time_seconds=execution_time
            )
            
            if save_success:
                st.success("ğŸ’¾ ëŒ€í™”ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                
                # í˜„ì¬ ëŒ€í™” ì„¸ì…˜ì— ì €ì¥
                st.session_state.current_conversation = {
                    'title': final_session_title,
                    'query': user_query,
                    'response': response['content'],
                    'model': selected_model,
                    'has_thinking': response.get('has_thinking', False),
                    'thinking': response.get('thinking', ''),
                    'rag_sources': rag_sources_used,
                    'execution_time': execution_time,
                    'chunked': bool(chunked_result)
                }
                
                # ì‚¬ì´ë“œë°” reasoning ì •ë³´ ì—…ë°ì´íŠ¸
                st.session_state.rag_sources_used = rag_sources_used
            else:
                st.warning("âš ï¸ ëŒ€í™” ì €ì¥ì— ì‹¤íŒ¨í–ˆì§€ë§Œ ì‘ë‹µì€ ì •ìƒì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    elif submit_button:
        st.warning("â“ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!") 

# Tab 2: RAG ë°ì´í„° ê´€ë¦¬
with tab2:
    st.header("ğŸ“Š RAG ë°ì´í„° ê´€ë¦¬")
    
    # MySQL ë°ì´í„° ì„¹ì…˜
    st.subheader("ğŸ—„ï¸ MySQL ë°ì´í„°ë² ì´ìŠ¤")
    
    if st.button("ğŸ”„ í…Œì´ë¸” ëª©ë¡ ìƒˆë¡œê³ ì¹¨"):
        st.rerun()
    
    # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
    available_tables = get_mysql_tables()
    
    if available_tables:
        selected_tables = st.multiselect(
            "ğŸ“‹ ë¶„ì„í•  í…Œì´ë¸” ì„ íƒ",
            available_tables,
            default=list(st.session_state.mysql_data.keys()) if st.session_state.mysql_data else None,
            help="Ctrl/Cmdë¥¼ ëˆ„ë¥´ê³  í´ë¦­í•˜ì—¬ ì—¬ëŸ¬ í…Œì´ë¸”ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“¥ ì„ íƒëœ í…Œì´ë¸” ë¡œë“œ", type="primary"):
                if selected_tables:
                    with st.spinner("MySQL ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
                        loaded_data = load_mysql_data(selected_tables)
                        st.session_state.mysql_data = loaded_data
                        
                        if loaded_data:
                            total_rows = sum(len(df) for df in loaded_data.values())
                            st.success(f"âœ… {len(loaded_data)}ê°œ í…Œì´ë¸” ë¡œë“œ ì™„ë£Œ! (ì´ {total_rows:,}í–‰)")
                        else:
                            st.error("âŒ í…Œì´ë¸” ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("í…Œì´ë¸”ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        
        with col2:
            if st.button("ğŸ—‘ï¸ MySQL ë°ì´í„° ì´ˆê¸°í™”"):
                st.session_state.mysql_data = {}
                st.success("âœ… MySQL ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # í˜„ì¬ ë¡œë“œëœ í…Œì´ë¸” ì •ë³´ í‘œì‹œ
        if st.session_state.mysql_data:
            st.markdown("#### ğŸ“Š ë¡œë“œëœ í…Œì´ë¸” ì •ë³´")
            for table_name, df in st.session_state.mysql_data.items():
                with st.expander(f"ğŸ“‹ {table_name} ({len(df):,}í–‰)", expanded=False):
                    st.markdown(f"**ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}")
                    st.dataframe(df.head(3), use_container_width=True)
    else:
        st.warning("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    st.divider()
    
    # ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì„¹ì…˜
    st.subheader("ğŸŒ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§")
    
    col1, col2 = st.columns([3, 1])
    with col1:
        website_url = st.text_input(
            "ğŸ”— í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URL",
            placeholder="ì˜ˆ: example.com ë˜ëŠ” https://example.com",
            help="ë„ë©”ì¸ëª…ë§Œ ì…ë ¥í•´ë„ ë©ë‹ˆë‹¤ (http:// ìë™ ì¶”ê°€)"
        )
    
    with col2:
        max_pages = st.number_input(
            "ğŸ“„ ìµœëŒ€ í˜ì´ì§€ ìˆ˜",
            min_value=1,
            max_value=10,
            value=3,
            help="ë„ˆë¬´ ë§ì€ í˜ì´ì§€ëŠ” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤"
        )
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸš€ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘", type="primary"):
            if website_url.strip():
                with st.spinner(f"ğŸ•·ï¸ {website_url} í¬ë¡¤ë§ ì¤‘... (ìµœëŒ€ {max_pages}í˜ì´ì§€)"):
                    scraped_data = scrape_website_simple(website_url.strip(), max_pages)
                    
                    if scraped_data:
                        # ê¸°ì¡´ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ì— ì¶”ê°€
                        st.session_state.website_data.extend(scraped_data)
                        st.success(f"âœ… {len(scraped_data)}ê°œ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
                    else:
                        st.error("âŒ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ì›¹ì‚¬ì´íŠ¸ ë°ì´í„° ì´ˆê¸°í™”"):
            st.session_state.website_data = []
            st.success("âœ… ì›¹ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í˜„ì¬ í¬ë¡¤ë§ëœ ì›¹ì‚¬ì´íŠ¸ ì •ë³´ í‘œì‹œ
    if st.session_state.website_data:
        st.markdown("#### ğŸŒ í¬ë¡¤ë§ëœ ì›¹ì‚¬ì´íŠ¸ ì •ë³´")
        for i, page in enumerate(st.session_state.website_data):
            with st.expander(f"ğŸ“„ í˜ì´ì§€ {i+1}: {page['title']}", expanded=False):
                st.markdown(f"**URL**: {page['url']}")
                st.markdown(f"**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°**: {page['content'][:200]}...")
    
    st.divider()
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜  
    st.subheader("ğŸ“„ ë¬¸ì„œ íŒŒì¼ ì—…ë¡œë“œ")
    
    uploaded_files = st.file_uploader(
        "ğŸ“‚ ë¶„ì„í•  ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        accept_multiple_files=True,
        type=['txt', 'csv', 'json', 'md'],
        help="ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ“¥ íŒŒì¼ ì²˜ë¦¬", type="primary"):
            if uploaded_files:
                with st.spinner("ğŸ“„ íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
                    processed_files = process_files(uploaded_files)
                    
                    if processed_files:
                        # ê¸°ì¡´ íŒŒì¼ ë°ì´í„°ì— ì¶”ê°€
                        st.session_state.files_data.extend(processed_files)
                        total_size = sum(f['size'] for f in processed_files)
                        st.success(f"âœ… {len(processed_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! (ì´ {total_size:,}ì)")
                    else:
                        st.error("âŒ íŒŒì¼ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.warning("íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    with col2:
        if st.button("ğŸ—‘ï¸ ë¬¸ì„œ ë°ì´í„° ì´ˆê¸°í™”"):
            st.session_state.files_data = []
            st.success("âœ… ë¬¸ì„œ ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
    if st.session_state.files_data:
        st.markdown("#### ğŸ“š ì—…ë¡œë“œëœ ë¬¸ì„œ ì •ë³´")
        for i, file_data in enumerate(st.session_state.files_data):
            with st.expander(f"ğŸ“„ {file_data['name']} ({file_data['size']:,}ì)", expanded=False):
                st.markdown(f"**ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°**: {file_data['content'][:200]}...")
    
    st.divider()
    
    # ì „ì²´ RAG ë°ì´í„° ìš”ì•½
    st.subheader("ğŸ“Š ì „ì²´ RAG ë°ì´í„° ìš”ì•½")
    
    total_sources = 0
    summary_parts = []
    
    if st.session_state.mysql_data:
        table_count = len(st.session_state.mysql_data)
        row_count = sum(len(df) for df in st.session_state.mysql_data.values())
        summary_parts.append(f"ğŸ—„ï¸ **MySQL**: {table_count}ê°œ í…Œì´ë¸”, {row_count:,}í–‰")
        total_sources += table_count
    
    if st.session_state.website_data:
        page_count = len(st.session_state.website_data)
        summary_parts.append(f"ğŸŒ **ì›¹ì‚¬ì´íŠ¸**: {page_count}ê°œ í˜ì´ì§€")
        total_sources += page_count
    
    if st.session_state.files_data:
        file_count = len(st.session_state.files_data)
        total_chars = sum(f['size'] for f in st.session_state.files_data)
        summary_parts.append(f"ğŸ“„ **ë¬¸ì„œ**: {file_count}ê°œ íŒŒì¼, {total_chars:,}ì")
        total_sources += file_count
    
    if summary_parts:
        st.success(f"âœ… **ì´ {total_sources}ê°œ RAG ì†ŒìŠ¤ ì¤€ë¹„ë¨**")
        for part in summary_parts:
            st.markdown(f"- {part}")
        
        # ì˜ˆìƒ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° í‘œì‹œ
        test_context, _ = create_rag_context(
            mysql_data=st.session_state.mysql_data if st.session_state.mysql_data else None,
            website_data=st.session_state.website_data if st.session_state.website_data else None,
            files_data=st.session_state.files_data if st.session_state.files_data else None,
            model_name="claude-3-5-sonnet-20241022"  # í…ŒìŠ¤íŠ¸ìš©
        )
        
        context_size = len(test_context)
        if context_size > 0:
            st.info(f"ğŸ’¡ **ì˜ˆìƒ RAG ì»¨í…ìŠ¤íŠ¸ í¬ê¸°**: {context_size:,}ì")
            
            # ëª¨ë¸ë³„ í¬ê¸° ì œí•œ ì •ë³´
            size_info = []
            if context_size > 7000000:
                size_info.append("âš ï¸ Claude ëª¨ë¸ ì œí•œ (7MB) ì´ˆê³¼")
            if context_size > 3000000:
                size_info.append("âš ï¸ GPT ëª¨ë¸ ì œí•œ (3MB) ì´ˆê³¼")
            if context_size > 800000:
                size_info.append("âš ï¸ o1 ëª¨ë¸ ì œí•œ (800KB) ì´ˆê³¼")
            
            if size_info:
                st.warning("í¬ê¸° ì œí•œì„ ì´ˆê³¼í•œ ëª¨ë¸ë“¤:")
                for info in size_info:
                    st.markdown(f"- {info}")
                st.info("ğŸ’¡ í° ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ RAG ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ì„¹ì…˜ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.") 

# Tab 3: ëŒ€í™” ì´ë ¥ ì¡°íšŒ
with tab3:
    st.header("ğŸ“ ëŒ€í™” ì´ë ¥ ì¡°íšŒ")
    
    # ê²€ìƒ‰ ë° í•„í„° ì˜µì…˜
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        search_term = st.text_input(
            "ğŸ” ê²€ìƒ‰ì–´",
            placeholder="ì œëª© ë˜ëŠ” ì§ˆë¬¸ ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰",
            help="ëŒ€í™” ì œëª©ì´ë‚˜ ì§ˆë¬¸ ë‚´ìš©ì„ ê²€ìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    
    with col2:
        model_filter = st.selectbox(
            "ğŸ¯ ëª¨ë¸ í•„í„°",
            ["ì „ì²´", "claude", "gpt", "o1"],
            help="íŠ¹ì • AI ëª¨ë¸ë¡œ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    
    with col3:
        limit = st.number_input(
            "ğŸ“Š ì¡°íšŒ ê°œìˆ˜",
            min_value=5,
            max_value=100,
            value=20,
            step=5
        )
    
    # ëŒ€í™” ì´ë ¥ ì¡°íšŒ
    conversations = get_conversation_history(
        limit=limit,
        search_term=search_term if search_term.strip() else None,
        model_filter=model_filter
    )
    
    if conversations:
        st.success(f"âœ… {len(conversations)}ê°œì˜ ëŒ€í™”ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        for conv in conversations:
            with st.expander(
                f"ğŸ’¬ {conv['session_title']} | {conv['model_name']} | {conv['created_at'].strftime('%Y-%m-%d %H:%M')}",
                expanded=False
            ):
                # ì§ˆë¬¸ í‘œì‹œ
                st.markdown("**â“ ì§ˆë¬¸:**")
                st.markdown(conv['user_query'])
                
                # ì‘ë‹µ í‘œì‹œ
                st.markdown("**ğŸ¤– AI ì‘ë‹µ:**")
                st.markdown(conv['assistant_response'])
                
                # ë©”íƒ€ë°ì´í„° í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown(f"**ğŸ¯ ëª¨ë¸**: {conv['model_name']}")
                    if conv['execution_time_seconds']:
                        st.markdown(f"**â±ï¸ ì†Œìš”ì‹œê°„**: {conv['execution_time_seconds']}ì´ˆ")
                
                with col2:
                    if conv['rag_sources_used']:
                        st.markdown(f"**ğŸ“š RAG ì†ŒìŠ¤**: {conv['rag_sources_used']}ê°œ")
                    
                    if conv['has_reasoning']:
                        st.markdown("**ğŸ§  Reasoning**: âœ…")
                
                with col3:
                    st.markdown(f"**ğŸ“… ìƒì„±ì¼**: {conv['created_at'].strftime('%Y-%m-%d %H:%M:%S')}")
                
                # Reasoning ë‚´ìš© í‘œì‹œ (ìˆëŠ” ê²½ìš°)
                if conv['has_reasoning'] and conv['reasoning_content']:
                    st.markdown("**ğŸ§  AI ì‚¬ê³  ê³¼ì •:**")
                    with st.container():
                        # details íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ‘íŒ í˜•íƒœë¡œ í‘œì‹œ
                        reasoning_content = conv['reasoning_content']
                        reasoning_html = f"""
                        <details>
                            <summary style="cursor: pointer; font-weight: bold; color: #1f77b4;">
                                ğŸ§  Reasoning ë‚´ìš© ë³´ê¸°
                            </summary>
                            <div style="margin-top: 10px; padding: 10px; background-color: #f0f2f6; border-radius: 5px; max-height: 400px; overflow-y: auto; color: #000000;">
                                {reasoning_content.replace('\n', '<br>')}
                            </div>
                        </details>
                        """
                        st.markdown(reasoning_html, unsafe_allow_html=True)
                
                st.markdown("---")
    
    else:
        if search_term or model_filter != "ì „ì²´":
            st.warning("ğŸ” ê²€ìƒ‰ ì¡°ê±´ì— ë§ëŠ” ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ’¬ ì•„ì§ ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤. AI ì±—ë´‡ íƒ­ì—ì„œ ëŒ€í™”ë¥¼ ì‹œì‘í•´ë³´ì„¸ìš”!")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # Reasoning í‘œì‹œ ì„¤ì •
    st.subheader("ğŸ§  AI ì‚¬ê³  ê³¼ì • (Reasoning)")
    
    enable_reasoning = st.checkbox(
        "ğŸ”„ Reasoning ê³¼ì • í‘œì‹œ",
        value=st.session_state.get('enable_sidebar_reasoning', False),
        help="Claude Reasoning ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •ì„ ì‚¬ì´ë“œë°”ì— í‘œì‹œí•©ë‹ˆë‹¤."
    )
    st.session_state.enable_sidebar_reasoning = enable_reasoning
    
    if enable_reasoning:
        thinking_budget = st.slider(
            "ğŸ’­ Thinking í† í° ì˜ˆì‚°",
            min_value=1000,
            max_value=8000,
            value=4000,
            step=500,
            help="Reasoning ê³¼ì •ì— ì‚¬ìš©í•  í† í° ìˆ˜ì…ë‹ˆë‹¤."
        )
        st.session_state.thinking_budget = thinking_budget
        
        # í˜„ì¬ ëŒ€í™”ì˜ Reasoning í‘œì‹œ
        if (st.session_state.get('current_conversation') and 
            st.session_state.current_conversation.get('has_thinking') and
            st.session_state.current_conversation.get('thinking')):
            
            st.markdown("### ğŸ§  ìµœì‹  AI ì‚¬ê³  ê³¼ì •")
            with st.container():
                # details íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ‘íŒ í˜•íƒœë¡œ í‘œì‹œ
                reasoning_content = st.session_state.current_conversation['thinking']
                reasoning_html = f"""
                <details>
                    <summary style="cursor: pointer; font-weight: bold; color: #1f77b4;">
                        ğŸ§  Reasoning ë‚´ìš© ë³´ê¸°
                    </summary>
                    <div style="margin-top: 10px; padding: 10px; background-color: #f0f2f6; border-radius: 5px; max-height: 400px; overflow-y: auto; color: #000000;">
                        {reasoning_content.replace('\n', '<br>')}
                    </div>
                </details>
                """
                st.markdown(reasoning_html, unsafe_allow_html=True)
    
    st.divider()
    
    # RAG ì†ŒìŠ¤ ì •ë³´
    st.subheader("ğŸ“š í˜„ì¬ RAG ì†ŒìŠ¤")
    
    if st.session_state.get('rag_sources_used'):
        for i, source in enumerate(st.session_state.rag_sources_used):
            st.markdown(f"**{i+1}. {source['name']}**")
            st.markdown(f"- íƒ€ì…: {source['type']}")
            st.markdown(f"- ìƒì„¸: {source['details']}")
            if i < len(st.session_state.rag_sources_used) - 1:
                st.markdown("---")
    else:
        st.info("RAG ì†ŒìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    st.divider()
    
    # ìŠ¤ë§ˆíŠ¸ RAG ì²˜ë¦¬ ì •ë³´
    st.subheader("ğŸš€ ìŠ¤ë§ˆíŠ¸ RAG ì²˜ë¦¬")
    
    # í˜„ì¬ ëŒ€í™”ê°€ ì²­í¬ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if (st.session_state.get('current_conversation') and 
        st.session_state.current_conversation.get('chunked')):
        st.success("âœ… ì²­í¬ ê¸°ë°˜ ì²˜ë¦¬ ì™„ë£Œ")
        st.info("ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ’¡ ì¼ë°˜ RAG ì²˜ë¦¬ ëª¨ë“œ")
    
    # ëª¨ë¸ë³„ ì²˜ë¦¬ ë°©ì‹ ì•ˆë‚´
    st.markdown("**ğŸ“– ëª¨ë¸ë³„ ìµœì í™” ì •ë³´**")
    model_info_html = """
    <details>
        <summary style="cursor: pointer; font-weight: bold; color: #1f77b4;">
            ëª¨ë¸ë³„ ì œí•œì‚¬í•­ ë³´ê¸°
        </summary>
        <div style="margin-top: 10px; padding: 10px; background-color: #f0f2f6; border-radius: 5px; color: #000000;">
            <strong>Claude ëª¨ë¸</strong>: 7MB ì œí•œ, 1í–‰ ìƒ˜í”Œ<br><br>
            <strong>GPT ëª¨ë¸</strong>: 3MB ì œí•œ, 3í–‰ ìƒ˜í”Œ<br><br>
            <strong>o1 ëª¨ë¸</strong>: 800KB ì œí•œ, 2í–‰ ìƒ˜í”Œ<br><br>
            í° ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬ë©ë‹ˆë‹¤.
        </div>
    </details>
    """
    st.markdown(model_info_html, unsafe_allow_html=True)
    
    st.divider()
    
    # ì‹œìŠ¤í…œ ì •ë³´
    st.subheader("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´")
    
    # í˜„ì¬ ë¡œë“œëœ ë°ì´í„° ìš”ì•½
    data_summary = []
    if st.session_state.mysql_data:
        data_summary.append(f"ğŸ—„ï¸ MySQL: {len(st.session_state.mysql_data)}ê°œ")
    if st.session_state.website_data:
        data_summary.append(f"ğŸŒ ì›¹: {len(st.session_state.website_data)}ê°œ")
    if st.session_state.files_data:
        data_summary.append(f"ğŸ“„ ë¬¸ì„œ: {len(st.session_state.files_data)}ê°œ")
    
    if data_summary:
        st.success("**ë¡œë“œëœ ë°ì´í„°:**")
        for summary in data_summary:
            st.markdown(f"- {summary}")
    else:
        st.warning("ë°ì´í„° ì—†ìŒ")
    
    # ë²„ì „ ì •ë³´
    st.markdown("---")
    st.caption("ğŸ¤– RAG ì±—ë´‡ v2.0")
    st.caption("ì²­í¬ ì²˜ë¦¬ ì§€ì› ë²„ì „") 