import streamlit as st
import mysql.connector
import pandas as pd
import plotly.express as px
from datetime import datetime
import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOllama
from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import (
    UnstructuredMarkdownLoader,
    UnstructuredPDFLoader,
    UnstructuredFileLoader,
    DirectoryLoader
)
import tempfile

load_dotenv()

# Set page configuration
st.set_page_config(page_title="Vote ê²°ê³¼", page_icon="ğŸ“Š", layout="wide")

# Page header
st.title("íˆ¬í‘œ ê²°ê³¼")

# MySQL ì—°ê²° ì„¤ì •
def connect_to_db():
    return mysql.connector.connect(
        user=os.getenv('SQL_USER'),
        password=os.getenv('SQL_PASSWORD'),
        host=os.getenv('SQL_HOST'),
        database=os.getenv('SQL_DATABASE_NEWBIZ'),
        charset='utf8mb4',
        collation='utf8mb4_unicode_ci'
    )

def get_all_questions():
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT q.*, 
                   COUNT(DISTINCT r.response_id) as total_votes,
                   COUNT(DISTINCT r.voter_name) as unique_voters
            FROM vote_questions q
            LEFT JOIN vote_responses r ON q.question_id = r.question_id
            GROUP BY q.question_id
            ORDER BY q.created_at DESC
        """)
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_question_results(question_id):
    """íˆ¬í‘œ ê²°ê³¼ì™€ íˆ¬í‘œì ëª©ë¡ ì¡°íšŒ"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        # ì „ì²´ íˆ¬í‘œ ìˆ˜ ì¡°íšŒ
        cursor.execute("""
            SELECT COUNT(*) as total_votes
            FROM vote_responses
            WHERE question_id = %s
        """, (question_id,))
        total_votes = cursor.fetchone()['total_votes']
        
        # ì˜µì…˜ë³„ ê²°ê³¼ ì¡°íšŒ
        cursor.execute("""
            SELECT 
                o.option_id,
                o.option_text,
                COUNT(r.response_id) as vote_count,
                COALESCE(
                    ROUND(COUNT(r.response_id) * 100.0 / NULLIF(%s, 0), 1),
                    0.0
                ) as vote_percentage,
                GROUP_CONCAT(DISTINCT r.reasoning SEPARATOR '\n') as reasonings
            FROM vote_options o
            LEFT JOIN vote_responses r ON o.option_id = r.option_id
            WHERE o.question_id = %s
            GROUP BY o.option_id, o.option_text
            ORDER BY vote_count DESC
        """, (total_votes, question_id))
        results = cursor.fetchall()
        
        # íˆ¬í‘œì ëª©ë¡ ì¡°íšŒ
        cursor.execute("""
            SELECT DISTINCT voter_name
            FROM vote_responses
            WHERE question_id = %s AND voter_name IS NOT NULL
            ORDER BY voter_name
        """, (question_id,))
        voters = cursor.fetchall()
        
        return results, voters
    finally:
        cursor.close()
        conn.close()

def get_available_models():
    """Ollamaì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    return [
        "llama3.3:latest",    # ê°€ì¥ í° ëª¨ë¸ (42GB)
        "deepseek-r1:70b",    # ëŒ€í˜• ëª¨ë¸ (42GB)
        "deepseek-r1:32b",    # ì¤‘í˜• ëª¨ë¸ (19GB)
        "phi4:latest",        # ì¤‘í˜• ëª¨ë¸ (9.1GB)
        "deepseek-r1:14b",    # ì¤‘í˜• ëª¨ë¸ (9GB)
        "gemma2:latest",      # ì†Œí˜• ëª¨ë¸ (5.4GB)
        "llama3.1:latest",    # ì†Œí˜• ëª¨ë¸ (4.9GB)
        "mistral:latest",     # ì†Œí˜• ëª¨ë¸ (4.1GB)
        "llama2:latest"       # ì†Œí˜• ëª¨ë¸ (3.8GB)
    ]

def get_llm_vote(question_id, model_name):
    """LLMì˜ íˆ¬í‘œ ê²°ê³¼ì™€ ì´ìœ  ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    result = None
    
    try:
        cursor.execute("""
            SELECT o.option_text, lr.reasoning
            FROM vote_llm_responses lr
            JOIN vote_options o ON lr.option_id = o.option_id
            WHERE lr.question_id = %s AND lr.llm_model = %s
            ORDER BY lr.voted_at DESC
            LIMIT 1
        """, (question_id, model_name))
        
        # ê²°ê³¼ë¥¼ ì™„ì „íˆ ì½ì–´ì˜´
        result = cursor.fetchone()
        
    except mysql.connector.Error as err:
        st.error(f"LLM íˆ¬í‘œ ê²°ê³¼ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {err}")
    finally:
        # ì»¤ì„œì™€ ì—°ê²° ì •ë¦¬
        cursor.close()
        conn.close()
    
    return result

def load_single_file(file_path):
    """ë‹¨ì¼ íŒŒì¼ ë¡œë“œ"""
    try:
        if file_path.endswith('.md'):
            loader = UnstructuredMarkdownLoader(file_path)
        elif file_path.endswith('.pdf'):
            loader = UnstructuredPDFLoader(file_path)
        else:
            loader = UnstructuredFileLoader(file_path)
        
        return loader.load()
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

def load_files(files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    documents = []
    for file in files:
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:
                tmp_file.write(file.getvalue())
                tmp_file.flush()
                
                # íŒŒì¼ ë¡œë“œ
                docs = load_single_file(tmp_file.name)
                documents.extend(docs)
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                os.unlink(tmp_file.name)
        except Exception as e:
            st.error(f"'{file.name}' íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    return documents

def create_vectorstore(documents):
    """ë¬¸ì„œë¡œë¶€í„° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±"""
    try:
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        vectorstore = FAISS.from_documents(documents, embeddings)
        return vectorstore
    except Exception as e:
        st.error(f"ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def get_relevant_context(vectorstore, question, options):
    """ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ë§¥ ê²€ìƒ‰"""
    if not vectorstore:
        return ""
        
    # ì§ˆë¬¸ê³¼ ëª¨ë“  ì„ íƒì§€ë¥¼ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
    search_text = f"{question}\n{options}"
    
    # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(search_text, k=3)
    
    # ë¬¸ë§¥ ê²°í•©
    context = "\n\n".join([doc.page_content for doc in docs])
    return context

def ask_llm(question, options, model_name, context=""):
    """LLMì—ê²Œ íˆ¬í‘œ ìš”ì²­í•˜ê³  ì‘ë‹µ ë°›ê¸°"""
    llm = ChatOllama(
        model=model_name,
        temperature=0.1,
        format="json"
    )
    
    # ë¬¸ë§¥ì´ ìˆëŠ” ê²½ìš°ì™€ ì—†ëŠ” ê²½ìš°ì˜ í”„ë¡¬í”„íŠ¸ ë¶„ë¦¬
    if context:
        prompt = f"""
        ë‹¹ì‹ ì€ íˆ¬í‘œ ì‹œìŠ¤í…œì˜ ì°¸ì—¬ìì…ë‹ˆë‹¤. 
        ì•„ë˜ ì œê³µëœ ë¬¸ë§¥, ì§ˆë¬¸, ì„ íƒì§€ë¥¼ ì‹ ì¤‘íˆ ë¶„ì„í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•´ì£¼ì„¸ìš”.
        
        ì°¸ê³ í•  ë¬¸ë§¥:
        {context}
        
        ì§ˆë¬¸: {question}
        
        ì„ íƒì§€:
        {options}
        """
    else:
        prompt = f"""
        ë‹¹ì‹ ì€ íˆ¬í‘œ ì‹œìŠ¤í…œì˜ ì°¸ì—¬ìì…ë‹ˆë‹¤. 
        ì•„ë˜ ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ ì‹ ì¤‘íˆ ë¶„ì„í•˜ê³  ê°€ì¥ ì ì ˆí•œ ë‹µì„ ì„ íƒí•´ì£¼ì„¸ìš”.
        
        ì§ˆë¬¸: {question}
        
        ì„ íƒì§€:
        {options}
        """
    
    prompt += """
    ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”:
    {
        "selection": <ì„ íƒí•œ ë²ˆí˜¸>,
        "reasoning": "<ì„ íƒí•œ ì´ìœ ì— ëŒ€í•œ ìƒì„¸ ì„¤ëª…>",
        "reference": "<ì°¸ê³ í•œ ë¬¸ë§¥ ë‚´ìš© ìš”ì•½ ë˜ëŠ” 'ë¬¸ë§¥ ì—†ìŒ'>"
    }
    
    ì£¼ì˜ì‚¬í•­:
    1. selectionì€ ë°˜ë“œì‹œ ìˆ«ìë§Œ ì…ë ¥
    2. reasoningì€ ë…¼ë¦¬ì ì´ê³  êµ¬ì²´ì ì¸ ì´ìœ  ì„¤ëª…
    3. referenceëŠ” ì°¸ê³ í•œ ë¬¸ë§¥ì´ ìˆëŠ” ê²½ìš° ê´€ë ¨ ë‚´ìš© ìš”ì•½
    4. JSON í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œì£¼ì„¸ìš”
    """
    
    response = llm.invoke(prompt)
    return response.content

def parse_llm_response(response_text):
    """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì„ íƒ ë²ˆí˜¸ì™€ ì´ìœ  ì¶”ì¶œ"""
    try:
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì›ë³¸ ì‘ë‹µ ì¶œë ¥
        st.write("ë””ë²„ê·¸ - ì›ë³¸ ì‘ë‹µ:", response_text)
        
        # JSON í˜•ì‹ ì°¾ê¸°
        import re
        import json
        
        # JSON í˜•ì‹ ì°¾ê¸° ì‹œë„
        json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
        if json_match:
            try:
                response_json = json.loads(json_match.group())
                selection = int(response_json['selection'])
                reasoning = response_json['reasoning']
                return selection, reasoning
            except (json.JSONDecodeError, KeyError, ValueError):
                pass
        
        # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
        lines = [line.strip() for line in response_text.split('\n') if line.strip()]
        selection = None
        reasoning = []
        parsing_reason = False
        
        for line in lines:
            if 'ì„ íƒ' in line.lower() or 'selection' in line.lower():
                numbers = [int(s) for s in line.split() if s.isdigit()]
                if numbers:
                    selection = numbers[0]
                    continue
            
            if 'ì´ìœ ' in line.lower() or 'reasoning' in line.lower() or parsing_reason:
                parsing_reason = True
                current_reason = line.replace('ì´ìœ :', '').replace('reasoning:', '').strip()
                if current_reason:
                    reasoning.append(current_reason)
        
        if selection is None:
            raise ValueError("ì„ íƒ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        reasoning_text = ' '.join(reasoning) if reasoning else "ì´ìœ ê°€ ëª…ì‹œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        return selection, reasoning_text
        
    except Exception as e:
        raise ValueError(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {str(e)}\nì›ë³¸ ì‘ë‹µ: {response_text}")

def save_llm_vote(question_id, option_id, model_name, reasoning, weight):
    """LLMì˜ íˆ¬í‘œ ê²°ê³¼ë¥¼ DBì— ì €ì¥"""
    conn = connect_to_db()
    cursor = conn.cursor()
    
    try:
        cursor.execute("""
            INSERT INTO vote_llm_responses 
            (question_id, option_id, llm_model, reasoning, weight)
            VALUES (%s, %s, %s, %s, %s)
        """, (question_id, option_id, model_name, reasoning, weight))
        conn.commit()
    finally:
        cursor.close()
        conn.close()

def get_combined_results(question_id):
    """ì¼ë°˜ íˆ¬í‘œì™€ LLM íˆ¬í‘œ ê²°ê³¼ ëª¨ë‘ ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT 
                o.option_text,
                CAST(COUNT(DISTINCT r.response_id) AS SIGNED) as human_votes,
                CAST(COALESCE(SUM(lr.weight), 0) AS SIGNED) as weighted_llm_votes,
                CAST(
                    COUNT(DISTINCT r.response_id) + COALESCE(SUM(lr.weight), 0)
                    AS SIGNED
                ) as total_votes
            FROM vote_options o
            LEFT JOIN vote_responses r ON o.option_id = r.option_id
            LEFT JOIN vote_llm_responses lr ON o.option_id = lr.option_id
            WHERE o.question_id = %s
            GROUP BY o.option_id, o.option_text
            ORDER BY total_votes DESC
        """, (question_id,))
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_question_options(question_id):
    """ì§ˆë¬¸ì— ëŒ€í•œ ì„ íƒì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT option_id, option_text
            FROM vote_options
            WHERE question_id = %s
            ORDER BY option_id
        """, (question_id,))
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def get_vote_results():
    """íˆ¬í‘œ ê²°ê³¼ ì¡°íšŒ"""
    conn = connect_to_db()
    cursor = conn.cursor(dictionary=True)
    
    try:
        cursor.execute("""
            SELECT 
                v.option_id,
                o.option_text,
                COUNT(v.vote_id) as vote_count,
                COALESCE(
                    (COUNT(v.vote_id) * 100.0 / 
                    NULLIF((SELECT COUNT(*) FROM votes), 0)), 
                    0
                ) as vote_percentage
            FROM vote_options o
            LEFT JOIN votes v ON o.option_id = v.option_id
            GROUP BY o.option_id, o.option_text
            ORDER BY vote_count DESC
        """)
        return cursor.fetchall()
    finally:
        cursor.close()
        conn.close()

def main():
    # ëª¨ë“  íˆ¬í‘œ ë¬¸ì œ ê°€ì ¸ì˜¤ê¸°
    questions = get_all_questions()
    
    if not questions:
        st.info("ë“±ë¡ëœ íˆ¬í‘œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë¬¸ì œ ì„ íƒ
    selected_question = st.selectbox(
        "ê²°ê³¼ë¥¼ ë³¼ íˆ¬í‘œë¥¼ ì„ íƒí•˜ì„¸ìš”",
        questions,
        format_func=lambda x: f"{x['title']} ({x['created_at'].strftime('%Y-%m-%d %H:%M')})"
    )
    
    if selected_question:
        st.write("---")
        st.write(f"## {selected_question['title']}")
        st.write(selected_question['description'])
        
        # íˆ¬í‘œ ìƒíƒœ í‘œì‹œ
        status_color = "ğŸŸ¢" if selected_question['status'] == 'active' else "ğŸ”´"
        st.write(f"ìƒíƒœ: {status_color} {selected_question['status'].upper()}")
        
        # ê¸°ë³¸ í†µê³„
        st.write(f"ì´ íˆ¬í‘œ ìˆ˜: {selected_question['total_votes']}")
        st.write(f"ì°¸ì—¬ì ìˆ˜: {selected_question['unique_voters']}")
        
        # ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        results, voters = get_question_results(selected_question['question_id'])
        
        if results:
            # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            df_results = pd.DataFrame(results)
            
            # ì°¨íŠ¸ ê·¸ë¦¬ê¸°
            col1, col2 = st.columns([2, 1])
            
            with col1:
                st.write("### íˆ¬í‘œ ê²°ê³¼ ì°¨íŠ¸")
                fig = px.bar(
                    df_results,
                    x='option_text',
                    y='vote_count',
                    text='vote_count',
                    title="ì„ íƒì§€ë³„ íˆ¬í‘œ ìˆ˜",
                    labels={'option_text': 'ì„ íƒì§€', 'vote_count': 'íˆ¬í‘œ ìˆ˜'}
                )
                fig.update_traces(textposition='outside')
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.write("### ìƒì„¸ ê²°ê³¼")
                for result in results:
                    st.write(f"#### {result['option_text']}")
                    
                    # ì•ˆì „í•œ ê°’ ì¶”ì¶œ
                    vote_count = result.get('vote_count', 0) or 0
                    vote_percentage = result.get('vote_percentage', 0.0) or 0.0
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.write(f"íˆ¬í‘œ ìˆ˜: {vote_count} ({vote_percentage:.1f}%)")
                    
                    # ì„ íƒ ì´ìœ  í‘œì‹œ
                    if result.get('reasonings'):
                        with st.expander("ğŸ’¬ ì„ íƒ ì´ìœ  ë³´ê¸°"):
                            reasonings = result['reasonings'].split('\n')
                            for reasoning in reasonings:
                                if reasoning.strip():
                                    st.markdown(f"- {reasoning}")
            
            # íˆ¬í‘œì ëª©ë¡ (ìµëª… ì œì™¸)
            if voters:
                with st.expander("íˆ¬í‘œì ëª©ë¡ ë³´ê¸° (ìµëª… ì œì™¸)"):
                    for voter in voters:
                        st.write(f"- {voter['voter_name']}")
            
            # ê´€ë¦¬ì ê¸°ëŠ¥
            if selected_question['status'] == 'active':
                if st.button("íˆ¬í‘œ ì¢…ë£Œí•˜ê¸°"):
                    conn = connect_to_db()
                    cursor = conn.cursor()
                    try:
                        cursor.execute("""
                            UPDATE vote_questions
                            SET status = 'closed'
                            WHERE question_id = %s
                        """, (selected_question['question_id'],))
                        conn.commit()
                        st.success("íˆ¬í‘œê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()
                    except mysql.connector.Error as err:
                        st.error(f"íˆ¬í‘œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {err}")
                    finally:
                        cursor.close()
                        conn.close()

        # LLM íˆ¬í‘œ ì„¹ì…˜
        st.write("---")
        st.write("## ğŸ¤– LLM íˆ¬í‘œ")
        
        col1, col2 = st.columns([1, 2])
        
        with col1:
            selected_model = st.selectbox(
                "LLM ëª¨ë¸ ì„ íƒ",
                get_available_models()
            )
            
            # LLM íˆ¬í‘œ ê°€ì¤‘ì¹˜ ì„¤ì •
            llm_weight = st.slider(
                "LLM íˆ¬í‘œ ê°€ì¤‘ì¹˜",
                min_value=1,
                max_value=10,
                value=1,
                help="LLMì˜ íˆ¬í‘œê°€ ëª‡ ëª…ì˜ íˆ¬í‘œì™€ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§ˆì§€ ì„¤ì •í•©ë‹ˆë‹¤."
            )
            
            # RAG ì‚¬ìš© ì—¬ë¶€ ì„ íƒ
            use_rag = st.checkbox("ë¬¸ì„œ ì°¸ì¡° ì‚¬ìš© (RAG)", 
                                help="ì„ íƒí•œ ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤.")
            
            if use_rag:
                # íŒŒì¼ ì…ë ¥ ë°©ì‹ ì„ íƒ
                input_method = st.radio(
                    "ì°¸ì¡° ë¬¸ì„œ ì…ë ¥ ë°©ì‹",
                    ["íŒŒì¼ ì—…ë¡œë“œ", "ë””ë ‰í† ë¦¬ ê²½ë¡œ"]
                )
                
                context = ""
                if input_method == "íŒŒì¼ ì—…ë¡œë“œ":
                    uploaded_files = st.file_uploader(
                        "ì°¸ì¡°í•  íŒŒì¼ ì„ íƒ (ì—¬ëŸ¬ íŒŒì¼ ê°€ëŠ¥)",
                        accept_multiple_files=True,
                        type=['txt', 'md', 'pdf']
                    )
                    
                    if uploaded_files:
                        with st.spinner("íŒŒì¼ ì²˜ë¦¬ ì¤‘..."):
                            documents = load_files(uploaded_files)
                            if documents:
                                vectorstore = create_vectorstore(documents)
                                
                else:  # ë””ë ‰í† ë¦¬ ê²½ë¡œ
                    doc_directory = st.text_input(
                        "ì°¸ì¡°í•  ë¬¸ì„œ ë””ë ‰í† ë¦¬ ê²½ë¡œ",
                        help="ë§ˆí¬ë‹¤ìš´/í…ìŠ¤íŠ¸/PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬"
                    )
                    
                    if doc_directory and os.path.exists(doc_directory):
                        with st.spinner("ë””ë ‰í† ë¦¬ ì²˜ë¦¬ ì¤‘..."):
                            documents = load_documents(doc_directory)
                            if documents:
                                vectorstore = create_vectorstore(documents)
            
            # LLM íˆ¬í‘œ ë²„íŠ¼
            if st.button("LLM íˆ¬í‘œ ì‹¤í–‰"):
                options = get_question_options(selected_question['question_id'])
                options_text = "\n".join([f"{i+1}. {opt['option_text']}" 
                                        for i, opt in enumerate(options)])
                
                context = ""
                if use_rag and 'vectorstore' in locals():
                    with st.spinner("ê´€ë ¨ ë¬¸ë§¥ ê²€ìƒ‰ ì¤‘..."):
                        context = get_relevant_context(
                            vectorstore,
                            selected_question['description'],
                            options_text
                        )
                        if context:
                            st.write("### ì°¸ì¡°í•œ ë¬¸ë§¥:")
                            st.write(context)
                
                # LLMì—ê²Œ ë¬¼ì–´ë³´ê¸°
                with st.spinner("LLM ì‘ë‹µ ëŒ€ê¸° ì¤‘..."):
                    llm_response = ask_llm(
                        selected_question['description'],
                        options_text,
                        selected_model,
                        context if use_rag else ""
                    )
                
                # ì‘ë‹µ íŒŒì‹± ë° ì €ì¥
                try:
                    selection, reasoning = parse_llm_response(llm_response)
                    
                    if selection < 1 or selection > len(options):
                        st.error(f"LLMì´ ì˜ëª»ëœ ì„ íƒì§€ ë²ˆí˜¸ë¥¼ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤: {selection}")
                        return
                    
                    save_llm_vote(
                        selected_question['question_id'],
                        options[selection - 1]['option_id'],
                        selected_model,
                        reasoning,
                        llm_weight
                    )
                    st.success(f"LLM íˆ¬í‘œê°€ ê°€ì¤‘ì¹˜ {llm_weight}ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
                    
                except ValueError as e:
                    st.error(str(e))
                except Exception as e:
                    st.error(f"LLM ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        with col2:
            # ê¸°ì¡´ LLM íˆ¬í‘œ ê²°ê³¼ í‘œì‹œ
            llm_vote = get_llm_vote(selected_question['question_id'], selected_model)
            if llm_vote:
                st.write("### ğŸ¤– LLM íˆ¬í‘œ ê²°ê³¼")
                st.write(f"**ì„ íƒí•œ í•­ëª©:** {llm_vote['option_text']}")
                st.write("**ì„ íƒ ì´ìœ :**")
                st.write(llm_vote['reasoning'])
        
        # ê²°ê³¼ ë¹„êµ í‘œì‹œ
        st.write("---")
        st.write("## ğŸ“Š í†µí•© ê²°ê³¼ ë¹„êµ")
        
        results = get_combined_results(selected_question['question_id'])
        if results:
            # ë°ì´í„°í”„ë ˆì„ ìƒì„± ì‹œ íƒ€ì… ëª…ì‹œ
            df_results = pd.DataFrame(results).astype({
                'human_votes': 'int64',
                'weighted_llm_votes': 'int64',
                'total_votes': 'int64'
            })
            
            # ì¸ê°„ íˆ¬í‘œ ì°¨íŠ¸
            fig1 = px.bar(
                df_results,
                x='option_text',
                y='human_votes',
                title="ì¸ê°„ íˆ¬í‘œ ê²°ê³¼",
                labels={'option_text': 'ì„ íƒì§€', 'human_votes': 'íˆ¬í‘œ ìˆ˜'}
            )
            st.plotly_chart(fig1, use_container_width=True)
            
            # í†µí•© ê²°ê³¼ ì°¨íŠ¸ (ê°€ì¤‘ì¹˜ ì ìš©)
            df_melted = pd.melt(
                df_results,
                id_vars=['option_text'],
                value_vars=['human_votes', 'weighted_llm_votes']
            )
            
            fig2 = px.bar(
                df_melted,
                x='option_text',
                y='value',
                color='variable',
                title="í†µí•© íˆ¬í‘œ ê²°ê³¼ (ì¸ê°„ + ê°€ì¤‘ì¹˜ ì ìš©ëœ LLM)",
                labels={
                    'option_text': 'ì„ íƒì§€',
                    'value': 'íˆ¬í‘œ ìˆ˜',
                    'variable': 'íˆ¬í‘œì ìœ í˜•'
                },
                barmode='stack'
            )
            
            # ë²”ë¡€ ì´ë¦„ ë³€ê²½
            fig2.update_traces(
                name="ì¸ê°„ íˆ¬í‘œ",
                selector=dict(name="human_votes")
            )
            fig2.update_traces(
                name="LLM íˆ¬í‘œ (ê°€ì¤‘ì¹˜ ì ìš©)",
                selector=dict(name="weighted_llm_votes")
            )
            
            st.plotly_chart(fig2, use_container_width=True)

if __name__ == "__main__":
    main() 